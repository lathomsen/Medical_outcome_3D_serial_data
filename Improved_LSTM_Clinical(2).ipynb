{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11afabd9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import ToTensor\n",
        "import tensorflow as tf\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "\n",
        "from tensorflow import keras \n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import Input, regularizers, layers\n",
        "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, KFold, train_test_split,StratifiedKFold\n",
        "from sklearn.metrics import make_scorer, roc_auc_score,confusion_matrix"
      ],
      "id": "11afabd9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06d5b7e3"
      },
      "outputs": [],
      "source": [
        "#pip install torch\n",
        "#pip install torch.nn \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "06d5b7e3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce788567",
        "outputId": "1f088394-f8c6-4542-e5e6-306001613a41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "id": "ce788567"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ5E7BCO1Zop",
        "outputId": "815fb4ba-ac90-487f-c3ee-44ec159fd8b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "id": "ZQ5E7BCO1Zop"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQbC_PFzNdqe"
      },
      "outputs": [],
      "source": [
        "#calculate confidence interval\n",
        "def get_standard_diviation(values):\n",
        "  mean = np.mean(values)\n",
        "  n = len(values)\n",
        "  s = np.sqrt(np.sum((values-mean)**2)/(n-1))\n",
        "\n",
        "  return s"
      ],
      "id": "JQbC_PFzNdqe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple visit data set"
      ],
      "metadata": {
        "id": "IVqm9dFQYiTY"
      },
      "id": "IVqm9dFQYiTY"
    },
    {
      "cell_type": "code",
      "source": [
        "df_mult= pd.read_csv(\"/content/gdrive/My Drive/Speciale/clindata_multiple_visits.csv\", sep=\",\", header=0)\n",
        "df_mult = df_mult.set_index(\"iD\")\n",
        "df_mult= df_mult.drop([\"Unnamed: 0\"],axis=1)"
      ],
      "metadata": {
        "id": "UTueBUfpYhnS"
      },
      "id": "UTueBUfpYhnS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68f23dd2"
      },
      "source": [
        "# Get data"
      ],
      "id": "68f23dd2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92a3331a"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/gdrive/My Drive/Speciale/clindata_new_25042022\", sep=\",\", header=0)\n",
        "df = df.set_index(\"iD\")\n",
        "df= df.drop([\"Unnamed: 0\"],axis=1)\n",
        "\n",
        "y = df[\"y\"]"
      ],
      "id": "92a3331a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e0d481b"
      },
      "source": [
        "#### Seperate data in train and test"
      ],
      "id": "0e0d481b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTD7J2jnBY8C"
      },
      "outputs": [],
      "source": [
        "TRAIN = pd.read_csv(\"/content/gdrive/My Drive/Speciale/matched_15032022\", sep=\",\", header=0)\n",
        "TRAIN = TRAIN.set_index(\"Unnamed: 0\")\n",
        "TRAIN = TRAIN.index\n",
        "\n",
        "VAL = pd.read_csv(\"/content/gdrive/My Drive/Speciale/validation_15032022\", sep=\",\", header=0)\n",
        "VAL = VAL.set_index(\"Unnamed: 0\")\n",
        "VAL = VAL.index"
      ],
      "id": "aTD7J2jnBY8C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSDGmu2lHWdv",
        "outputId": "f87d1850-86fd-468b-da15-bde41303eefa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All:  (1506, 39)\n",
            "Train (1232, 39)\n",
            "Val (274, 39)\n"
          ]
        }
      ],
      "source": [
        "print(\"All: \", df.shape )\n",
        "\n",
        "df[\"id\"] = [int(i[:7]) for i in df.index]\n",
        "\n",
        "# Select validation or training mode VAL or TRAIN\n",
        "train = df.loc[ df['id'].isin(TRAIN)]\n",
        "val = df.loc[~df['id'].isin(TRAIN)]\n",
        "\n",
        "train = train.drop([\"id\"],axis=1)\n",
        "val = val.drop([\"id\"],axis=1)\n",
        "\n",
        "print(\"Train\", train.shape)\n",
        "print(\"Val\", val.shape)\n",
        "train.shape[0]+val.shape[0]\n",
        "\n",
        "y = train[\"y\"]\n",
        "train= train.drop([\"y\"],axis=1)\n",
        "\n",
        "y_val = val[\"y\"]\n",
        "val= val.drop([\"y\"],axis=1)"
      ],
      "id": "VSDGmu2lHWdv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ7g_NnIHWK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb2d625d-d7d0-46f7-e655-bf803c9d4688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All:  (1114, 64)\n",
            "Train_mult (894, 64)\n",
            "Val_mult (220, 64)\n"
          ]
        }
      ],
      "source": [
        "#mult visits\n",
        "print(\"All: \", df_mult.shape )\n",
        "\n",
        "df_mult[\"id\"] = [int(i[:7]) for i in df_mult.index]\n",
        "\n",
        "# Select validation or training mode VAL or TRAIN\n",
        "train_mult = df_mult.loc[ df_mult['id'].isin(TRAIN)]\n",
        "val_mult = df_mult.loc[~df_mult['id'].isin(TRAIN)]\n",
        "\n",
        "train_mult = train_mult.drop([\"id\"],axis=1)\n",
        "val_mult = val_mult.drop([\"id\"],axis=1)\n",
        "\n",
        "print(\"Train_mult\", train_mult.shape)\n",
        "print(\"Val_mult\", val_mult.shape)\n",
        "train_mult.shape[0]+val_mult.shape[0]\n",
        "\n",
        "y_mult = train_mult[\"y\"]\n",
        "train_mult= train_mult.drop([\"y\"],axis=1)\n",
        "\n",
        "y_mult_val = val_mult[\"y\"]\n",
        "val_mult= val_mult.drop([\"y\"],axis=1)"
      ],
      "id": "MZ7g_NnIHWK3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fheS0bGrHqpM"
      },
      "source": [
        "## Split in visits"
      ],
      "id": "fheS0bGrHqpM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71cdcda0"
      },
      "outputs": [],
      "source": [
        "k = 8\n",
        "n = 10\n",
        "\n",
        "constants = train.iloc[:, :k]\n",
        "# split into vitsits\n",
        "v00 = (train.iloc[:,k:n+k])\n",
        "v00 = pd.concat([constants, v00], axis=1)\n",
        "\n",
        "v01 = (train.iloc[:,k+n:2*n+k])\n",
        "v01 = pd.concat([constants, v01], axis=1)\n",
        "\n",
        "v03 = (train.iloc[:, 2*n+k:])\n",
        "v03 = pd.concat([constants, v03], axis=1)\n",
        "\n",
        "\n",
        "#ALSO FOR VALIDAtION\n",
        "constants_val = val.iloc[:, :k]\n",
        "# split into vitsits\n",
        "v00_val = (val.iloc[:,k:n+k])\n",
        "v00_val = pd.concat([constants_val, v00_val], axis=1)\n",
        "\n",
        "v01_val = (val.iloc[:,k+n:2*n+k])\n",
        "v01_val = pd.concat([constants_val, v01_val], axis=1)\n",
        "\n",
        "v03_val = (val.iloc[:, 2*n+k:])\n",
        "v03_val = pd.concat([constants_val, v03_val], axis=1)"
      ],
      "id": "71cdcda0"
    },
    {
      "cell_type": "code",
      "source": [
        "v00.shape, v01.shape, v03.shape, v00_val.shape, v01_val.shape, v03_val.shape,"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mPi4-0lz7PJ",
        "outputId": "a87da85c-c99a-4345-bc46-476b7af5b3e3"
      },
      "id": "8mPi4-0lz7PJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1232, 18), (1232, 18), (1232, 18), (274, 18), (274, 18), (274, 18))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 8\n",
        "n = int((64-k-1)/5)\n",
        "\n",
        "constants_mult = train_mult.iloc[:, :k]\n",
        "# split into vitsits\n",
        "v00_mult = (train_mult.iloc[:,k:n+k])\n",
        "v00_mult = pd.concat([constants_mult, v00_mult], axis=1)\n",
        "\n",
        "v01_mult = (train_mult.iloc[:,k+n:2*n+k])\n",
        "v01_mult = pd.concat([constants_mult, v01_mult], axis=1)\n",
        "\n",
        "v03_mult = (train_mult.iloc[:, 2*n+k:3*n+k])\n",
        "v03_mult = pd.concat([constants_mult, v03_mult], axis=1)\n",
        "\n",
        "v04_mult = (train_mult.iloc[:, 3*n+k:4*n+k])\n",
        "v04_mult = pd.concat([constants_mult, v04_mult], axis=1)\n",
        "\n",
        "v05_mult = (train_mult.iloc[:, 4*n+k:5*n+k])\n",
        "v05_mult = pd.concat([constants_mult, v05_mult], axis=1)\n",
        "\n",
        "\n",
        "#Validation\n",
        "constants_mult_val = val_mult.iloc[:, :k]\n",
        "# split into vitsits\n",
        "v00_mult_val = (val_mult.iloc[:,k:n+k])\n",
        "v00_mult_val = pd.concat([constants_mult_val, v00_mult_val], axis=1)\n",
        "\n",
        "v01_mult_val = (val_mult.iloc[:,k+n:2*n+k])\n",
        "v01_mult_val = pd.concat([constants_mult_val, v01_mult_val], axis=1)\n",
        "\n",
        "v03_mult_val = (val_mult.iloc[:, 2*n+k:3*n+k])\n",
        "v03_mult_val = pd.concat([constants_mult_val, v03_mult_val], axis=1)\n",
        "\n",
        "v04_mult_val = (val_mult.iloc[:, 3*n+k:4*n+k])\n",
        "v04_mult_val = pd.concat([constants_mult_val, v04_mult_val], axis=1)\n",
        "\n",
        "v05_mult_val = (val_mult.iloc[:, 4*n+k:5*n+k])\n",
        "v05_mult_val = pd.concat([constants_mult_val, v05_mult_val], axis=1)"
      ],
      "metadata": {
        "id": "R-5mzQifdqnb"
      },
      "id": "R-5mzQifdqnb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-fCxyhPEtos",
        "outputId": "1c660235-a3d9-47cd-8905-12f4ae5a2464"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((894, 19),\n",
              " (894, 19),\n",
              " (894, 19),\n",
              " (894, 19),\n",
              " (894, 19),\n",
              " (220, 19),\n",
              " (220, 19),\n",
              " (220, 19),\n",
              " (220, 19),\n",
              " (220, 19))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "v00_mult.shape, v01_mult.shape, v03_mult.shape, v04_mult.shape, v05_mult.shape, v00_mult_val.shape, v01_mult_val.shape, v03_mult_val.shape, v04_mult_val.shape, v05_mult_val.shape"
      ],
      "id": "q-fCxyhPEtos"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7laCNTBeDhRo",
        "outputId": "1b1e95d9-80a6-4b92-c8b9-9710cb98a850"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1232, 3, 18), (1232,), (274, 3, 18), (274,))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "#Combine \n",
        "df_all = np.array([v00,v01,v03])\n",
        "df_all = np.moveaxis(df_all, 0, 1)\n",
        "\n",
        "# and validation\n",
        "df_all_val = np.array([v00_val,v01_val,v03_val])\n",
        "df_all_val = np.moveaxis(df_all_val, 0, 1)\n",
        "\n",
        "df_all.shape, y.shape, df_all_val.shape, y_val.shape"
      ],
      "id": "7laCNTBeDhRo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0z2Jkw3HEWe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdfdd1ad-de9b-4572-c138-d0606aa433f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((894, 5, 19), (894,), (220, 5, 19), (220,))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "#Combine \n",
        "df_all_mult = np.array([v00_mult,v01_mult,v03_mult,v04_mult,v05_mult])\n",
        "df_all_mult = np.moveaxis(df_all_mult, 0, 1)\n",
        "\n",
        "# Validation\n",
        "df_all_mult_val = np.array([v00_mult_val,v01_mult_val,v03_mult_val,v04_mult_val,v05_mult_val])\n",
        "df_all_mult_val = np.moveaxis(df_all_mult_val, 0, 1)\n",
        "\n",
        "df_all_mult.shape, y_mult.shape, df_all_mult_val.shape, y_mult_val.shape"
      ],
      "id": "0z2Jkw3HEWe9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Make data with 3 visits from multiple set\n",
        "df_all_5v3 = df_all_mult[:,:3,:]"
      ],
      "metadata": {
        "id": "RFvy04f066wN"
      },
      "id": "RFvy04f066wN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvhksJDO3UHQ"
      },
      "source": [
        "# Baseline model"
      ],
      "id": "KvhksJDO3UHQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7pTGC0qVNYWQ",
        "outputId": "ff923579-53ad-4fca-da6e-54cdbab1fe97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "28/28 [==============================] - 10s 42ms/step - loss: 0.2600 - auc: 0.5572 - val_loss: 0.2497 - val_auc: 0.5000\n",
            "Epoch 2/1000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.2386 - auc: 0.5312 - val_loss: 0.2450 - val_auc: 0.5482\n",
            "Epoch 3/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.2193 - auc: 0.5763 - val_loss: 0.2420 - val_auc: 0.7278\n",
            "Epoch 4/1000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.2025 - auc: 0.6028 - val_loss: 0.2404 - val_auc: 0.7520\n",
            "Epoch 5/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1885 - auc: 0.6593 - val_loss: 0.2406 - val_auc: 0.7594\n",
            "Epoch 6/1000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.1776 - auc: 0.6957 - val_loss: 0.2422 - val_auc: 0.7671\n",
            "Epoch 7/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.1693 - auc: 0.7281 - val_loss: 0.2444 - val_auc: 0.7534\n",
            "Epoch 8/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1629 - auc: 0.7295 - val_loss: 0.2470 - val_auc: 0.7497\n",
            "Epoch 9/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.1577 - auc: 0.7699 - val_loss: 0.2494 - val_auc: 0.7311\n",
            "Epoch 10/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1540 - auc: 0.7453 - val_loss: 0.2521 - val_auc: 0.7332\n",
            "Epoch 11/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1506 - auc: 0.7400 - val_loss: 0.2543 - val_auc: 0.7119\n",
            "Epoch 12/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1479 - auc: 0.7562 - val_loss: 0.2566 - val_auc: 0.7552\n",
            "Epoch 13/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1458 - auc: 0.7684 - val_loss: 0.2589 - val_auc: 0.7481\n",
            "Epoch 14/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1438 - auc: 0.7636 - val_loss: 0.2606 - val_auc: 0.7578\n",
            "Epoch 15/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.1422 - auc: 0.7490 - val_loss: 0.2621 - val_auc: 0.7050\n",
            "Epoch 16/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1408 - auc: 0.7766 - val_loss: 0.2628 - val_auc: 0.7287\n",
            "Epoch 17/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1391 - auc: 0.7530 - val_loss: 0.2634 - val_auc: 0.7074\n",
            "Epoch 18/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.1377 - auc: 0.7848 - val_loss: 0.2634 - val_auc: 0.7454\n",
            "Epoch 19/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1366 - auc: 0.7881 - val_loss: 0.2635 - val_auc: 0.7561\n",
            "Epoch 20/1000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.1350 - auc: 0.7966 - val_loss: 0.2631 - val_auc: 0.7499\n",
            "Epoch 21/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.1337 - auc: 0.7962 - val_loss: 0.2624 - val_auc: 0.7578\n",
            "Epoch 22/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1325 - auc: 0.8174 - val_loss: 0.2609 - val_auc: 0.7555\n",
            "Epoch 23/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1315 - auc: 0.8026 - val_loss: 0.2591 - val_auc: 0.7509\n",
            "Epoch 24/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.1301 - auc: 0.8173 - val_loss: 0.2529 - val_auc: 0.7588\n",
            "Epoch 25/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1287 - auc: 0.8177 - val_loss: 0.2527 - val_auc: 0.7578\n",
            "Epoch 26/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.1284 - auc: 0.8198 - val_loss: 0.2505 - val_auc: 0.7638\n",
            "Epoch 27/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1272 - auc: 0.8195 - val_loss: 0.2483 - val_auc: 0.7709\n",
            "Epoch 28/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1262 - auc: 0.8336 - val_loss: 0.2464 - val_auc: 0.7592\n",
            "Epoch 29/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1244 - auc: 0.8310 - val_loss: 0.2468 - val_auc: 0.7537\n",
            "Epoch 30/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.1228 - auc: 0.8318 - val_loss: 0.2440 - val_auc: 0.7507\n",
            "Epoch 31/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1216 - auc: 0.8283 - val_loss: 0.2419 - val_auc: 0.7643\n",
            "Epoch 32/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.1213 - auc: 0.8327 - val_loss: 0.2407 - val_auc: 0.7553\n",
            "Epoch 33/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.1207 - auc: 0.8351 - val_loss: 0.2376 - val_auc: 0.7790\n",
            "Epoch 34/1000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.1190 - auc: 0.8390 - val_loss: 0.2356 - val_auc: 0.7522\n",
            "Epoch 35/1000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.1170 - auc: 0.8518 - val_loss: 0.2341 - val_auc: 0.7544\n",
            "Epoch 36/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1170 - auc: 0.8384 - val_loss: 0.2325 - val_auc: 0.7678\n",
            "Epoch 37/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.1147 - auc: 0.8465 - val_loss: 0.2302 - val_auc: 0.7585\n",
            "Epoch 38/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1125 - auc: 0.8489 - val_loss: 0.2279 - val_auc: 0.7625\n",
            "Epoch 39/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.1116 - auc: 0.8418 - val_loss: 0.2225 - val_auc: 0.7636\n",
            "Epoch 40/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.1100 - auc: 0.8525 - val_loss: 0.2219 - val_auc: 0.7563\n",
            "Epoch 41/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.1102 - auc: 0.8466 - val_loss: 0.2233 - val_auc: 0.7586\n",
            "Epoch 42/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.1120 - auc: 0.8421 - val_loss: 0.2187 - val_auc: 0.7639\n",
            "Epoch 43/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.1076 - auc: 0.8645 - val_loss: 0.2189 - val_auc: 0.7492\n",
            "Epoch 44/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.1060 - auc: 0.8715 - val_loss: 0.2146 - val_auc: 0.7608\n",
            "Epoch 45/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1050 - auc: 0.8678 - val_loss: 0.2151 - val_auc: 0.7620\n",
            "Epoch 46/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1048 - auc: 0.8567 - val_loss: 0.2162 - val_auc: 0.7549\n",
            "Epoch 47/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1031 - auc: 0.8646 - val_loss: 0.2161 - val_auc: 0.7593\n",
            "Epoch 48/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1045 - auc: 0.8510 - val_loss: 0.2121 - val_auc: 0.7647\n",
            "Epoch 49/1000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.1040 - auc: 0.8537 - val_loss: 0.2149 - val_auc: 0.7467\n",
            "Epoch 50/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.1010 - auc: 0.8576 - val_loss: 0.2123 - val_auc: 0.7508\n",
            "Epoch 51/1000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.1008 - auc: 0.8555 - val_loss: 0.2090 - val_auc: 0.7627\n",
            "Epoch 52/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0991 - auc: 0.8641 - val_loss: 0.2072 - val_auc: 0.7518\n",
            "Epoch 53/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0986 - auc: 0.8617 - val_loss: 0.2105 - val_auc: 0.7523\n",
            "Epoch 54/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1002 - auc: 0.8543 - val_loss: 0.2132 - val_auc: 0.7483\n",
            "Epoch 55/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0987 - auc: 0.8672 - val_loss: 0.2074 - val_auc: 0.7516\n",
            "Epoch 56/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0966 - auc: 0.8573 - val_loss: 0.2079 - val_auc: 0.7451\n",
            "Epoch 57/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0937 - auc: 0.8720 - val_loss: 0.2080 - val_auc: 0.7483\n",
            "Epoch 58/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0952 - auc: 0.8683 - val_loss: 0.2074 - val_auc: 0.7540\n",
            "Epoch 59/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0977 - auc: 0.8566 - val_loss: 0.2046 - val_auc: 0.7503\n",
            "Epoch 60/1000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0945 - auc: 0.8646 - val_loss: 0.2088 - val_auc: 0.7447\n",
            "Epoch 61/1000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0939 - auc: 0.8629 - val_loss: 0.2139 - val_auc: 0.7491\n",
            "Epoch 62/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0926 - auc: 0.8653 - val_loss: 0.2010 - val_auc: 0.7539\n",
            "Epoch 63/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0938 - auc: 0.8654 - val_loss: 0.2034 - val_auc: 0.7513\n",
            "Epoch 64/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0892 - auc: 0.8663 - val_loss: 0.1989 - val_auc: 0.7553\n",
            "Epoch 65/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0889 - auc: 0.8624 - val_loss: 0.1968 - val_auc: 0.7629\n",
            "Epoch 66/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0892 - auc: 0.8602 - val_loss: 0.2046 - val_auc: 0.7619\n",
            "Epoch 67/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0875 - auc: 0.8679 - val_loss: 0.2012 - val_auc: 0.7641\n",
            "Epoch 68/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0879 - auc: 0.8730 - val_loss: 0.2019 - val_auc: 0.7569\n",
            "Epoch 69/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0897 - auc: 0.8547 - val_loss: 0.1979 - val_auc: 0.7570\n",
            "Epoch 70/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0850 - auc: 0.8722 - val_loss: 0.1934 - val_auc: 0.7587\n",
            "Epoch 71/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0883 - auc: 0.8612 - val_loss: 0.1980 - val_auc: 0.7568\n",
            "Epoch 72/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0868 - auc: 0.8712 - val_loss: 0.2015 - val_auc: 0.7714\n",
            "Epoch 73/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0891 - auc: 0.8729 - val_loss: 0.2010 - val_auc: 0.7577\n",
            "Epoch 74/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0869 - auc: 0.8725 - val_loss: 0.2014 - val_auc: 0.7583\n",
            "Epoch 75/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0843 - auc: 0.8633 - val_loss: 0.2025 - val_auc: 0.7560\n",
            "Epoch 76/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0856 - auc: 0.8636 - val_loss: 0.2030 - val_auc: 0.7527\n",
            "Epoch 77/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0870 - auc: 0.8731 - val_loss: 0.2043 - val_auc: 0.7605\n",
            "Epoch 78/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0856 - auc: 0.8754 - val_loss: 0.1976 - val_auc: 0.7570\n",
            "Epoch 79/1000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0888 - auc: 0.8634 - val_loss: 0.1989 - val_auc: 0.7540\n",
            "Epoch 80/1000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0897 - auc: 0.8615 - val_loss: 0.2042 - val_auc: 0.7512\n",
            "Epoch 81/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0805 - auc: 0.8751 - val_loss: 0.2040 - val_auc: 0.7509\n",
            "Epoch 82/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0877 - auc: 0.8596 - val_loss: 0.2000 - val_auc: 0.7527\n",
            "Epoch 83/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0841 - auc: 0.8747 - val_loss: 0.2020 - val_auc: 0.7602\n",
            "Epoch 84/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0828 - auc: 0.8738 - val_loss: 0.2098 - val_auc: 0.7504\n",
            "Epoch 85/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0852 - auc: 0.8665 - val_loss: 0.1989 - val_auc: 0.7450\n",
            "Epoch 86/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0817 - auc: 0.8599 - val_loss: 0.1987 - val_auc: 0.7448\n",
            "Epoch 87/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0871 - auc: 0.8631 - val_loss: 0.2027 - val_auc: 0.7526\n",
            "Epoch 88/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0827 - auc: 0.8673 - val_loss: 0.2046 - val_auc: 0.7543\n",
            "Epoch 89/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0830 - auc: 0.8794 - val_loss: 0.1999 - val_auc: 0.7531\n",
            "Epoch 90/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0812 - auc: 0.8784 - val_loss: 0.2053 - val_auc: 0.7643\n",
            "Epoch 91/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0866 - auc: 0.8831 - val_loss: 0.2028 - val_auc: 0.7601\n",
            "Epoch 92/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0810 - auc: 0.8868 - val_loss: 0.2000 - val_auc: 0.7555\n",
            "Epoch 93/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0805 - auc: 0.8798 - val_loss: 0.2055 - val_auc: 0.7437\n",
            "Epoch 94/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0800 - auc: 0.8927 - val_loss: 0.1985 - val_auc: 0.7496\n",
            "Epoch 95/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0860 - auc: 0.8669 - val_loss: 0.2026 - val_auc: 0.7573\n",
            "Epoch 96/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0837 - auc: 0.8740 - val_loss: 0.1952 - val_auc: 0.7597\n",
            "Epoch 97/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0852 - auc: 0.8775 - val_loss: 0.1920 - val_auc: 0.7615\n",
            "Epoch 98/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0753 - auc: 0.8886 - val_loss: 0.1959 - val_auc: 0.7659\n",
            "Epoch 99/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0819 - auc: 0.8742 - val_loss: 0.2001 - val_auc: 0.7619\n",
            "Epoch 100/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0769 - auc: 0.8941 - val_loss: 0.1962 - val_auc: 0.7674\n",
            "Epoch 101/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0755 - auc: 0.8890 - val_loss: 0.1978 - val_auc: 0.7598\n",
            "Epoch 102/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0790 - auc: 0.8717 - val_loss: 0.1968 - val_auc: 0.7585\n",
            "Epoch 103/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0782 - auc: 0.8737 - val_loss: 0.1968 - val_auc: 0.7623\n",
            "Epoch 104/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0785 - auc: 0.8789 - val_loss: 0.1929 - val_auc: 0.7657\n",
            "Epoch 105/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0810 - auc: 0.8659 - val_loss: 0.1955 - val_auc: 0.7653\n",
            "Epoch 106/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0773 - auc: 0.8850 - val_loss: 0.1932 - val_auc: 0.7609\n",
            "Epoch 107/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0740 - auc: 0.8846 - val_loss: 0.1957 - val_auc: 0.7686\n",
            "Epoch 108/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0776 - auc: 0.8859 - val_loss: 0.2010 - val_auc: 0.7612\n",
            "Epoch 109/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0775 - auc: 0.8918 - val_loss: 0.1979 - val_auc: 0.7633\n",
            "Epoch 110/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0736 - auc: 0.8886 - val_loss: 0.1993 - val_auc: 0.7545\n",
            "Epoch 111/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0722 - auc: 0.8875 - val_loss: 0.1990 - val_auc: 0.7612\n",
            "Epoch 112/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0756 - auc: 0.8842 - val_loss: 0.1948 - val_auc: 0.7658\n",
            "Epoch 113/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0716 - auc: 0.8968 - val_loss: 0.2049 - val_auc: 0.7554\n",
            "Epoch 114/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0742 - auc: 0.8789 - val_loss: 0.1981 - val_auc: 0.7625\n",
            "Epoch 115/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0830 - auc: 0.8829 - val_loss: 0.2107 - val_auc: 0.7578\n",
            "Epoch 116/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0796 - auc: 0.8770 - val_loss: 0.2062 - val_auc: 0.7526\n",
            "Epoch 117/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0715 - auc: 0.8928 - val_loss: 0.1980 - val_auc: 0.7636\n",
            "Epoch 118/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0798 - auc: 0.8893 - val_loss: 0.1974 - val_auc: 0.7655\n",
            "Epoch 119/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0727 - auc: 0.8963 - val_loss: 0.1977 - val_auc: 0.7669\n",
            "Epoch 120/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0743 - auc: 0.9020 - val_loss: 0.2003 - val_auc: 0.7676\n",
            "Epoch 121/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0701 - auc: 0.9080 - val_loss: 0.2078 - val_auc: 0.7620\n",
            "Epoch 122/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0782 - auc: 0.8866 - val_loss: 0.2045 - val_auc: 0.7582\n",
            "Epoch 123/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0776 - auc: 0.8949 - val_loss: 0.2054 - val_auc: 0.7612\n",
            "Epoch 124/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0745 - auc: 0.8849 - val_loss: 0.2009 - val_auc: 0.7668\n",
            "Epoch 125/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0734 - auc: 0.8896 - val_loss: 0.2055 - val_auc: 0.7607\n",
            "Epoch 126/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0737 - auc: 0.8868 - val_loss: 0.2006 - val_auc: 0.7626\n",
            "Epoch 127/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0787 - auc: 0.8779 - val_loss: 0.2017 - val_auc: 0.7678\n",
            "Epoch 128/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0748 - auc: 0.9032 - val_loss: 0.2057 - val_auc: 0.7656\n",
            "Epoch 129/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0789 - auc: 0.8812 - val_loss: 0.2039 - val_auc: 0.7668\n",
            "Epoch 130/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0724 - auc: 0.8894 - val_loss: 0.2094 - val_auc: 0.7685\n",
            "Epoch 131/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0710 - auc: 0.8902 - val_loss: 0.2088 - val_auc: 0.7681\n",
            "Epoch 132/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0695 - auc: 0.8892 - val_loss: 0.2041 - val_auc: 0.7703\n",
            "Epoch 133/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0746 - auc: 0.8894 - val_loss: 0.2039 - val_auc: 0.7666\n",
            "Epoch 134/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0701 - auc: 0.9029 - val_loss: 0.2048 - val_auc: 0.7651\n",
            "Epoch 135/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0714 - auc: 0.8980 - val_loss: 0.2063 - val_auc: 0.7672\n",
            "Epoch 136/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0695 - auc: 0.8932 - val_loss: 0.2063 - val_auc: 0.7690\n",
            "Epoch 137/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0678 - auc: 0.9049 - val_loss: 0.2027 - val_auc: 0.7694\n",
            "Epoch 138/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0793 - auc: 0.8883 - val_loss: 0.1983 - val_auc: 0.7711\n",
            "Epoch 139/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0695 - auc: 0.8948 - val_loss: 0.2000 - val_auc: 0.7722\n",
            "Epoch 140/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0700 - auc: 0.9052 - val_loss: 0.2045 - val_auc: 0.7734\n",
            "Epoch 141/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0715 - auc: 0.8924 - val_loss: 0.2063 - val_auc: 0.7729\n",
            "Epoch 142/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0734 - auc: 0.8965 - val_loss: 0.2021 - val_auc: 0.7681\n",
            "Epoch 143/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0667 - auc: 0.8978 - val_loss: 0.1968 - val_auc: 0.7697\n",
            "Epoch 144/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0732 - auc: 0.8859 - val_loss: 0.2009 - val_auc: 0.7727\n",
            "Epoch 145/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0706 - auc: 0.8786 - val_loss: 0.2046 - val_auc: 0.7734\n",
            "Epoch 146/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0692 - auc: 0.9107 - val_loss: 0.2054 - val_auc: 0.7702\n",
            "Epoch 147/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0664 - auc: 0.8975 - val_loss: 0.2024 - val_auc: 0.7710\n",
            "Epoch 148/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0735 - auc: 0.8807 - val_loss: 0.2067 - val_auc: 0.7745\n",
            "Epoch 149/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0669 - auc: 0.9016 - val_loss: 0.2016 - val_auc: 0.7727\n",
            "Epoch 150/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0705 - auc: 0.8932 - val_loss: 0.2059 - val_auc: 0.7655\n",
            "Epoch 151/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0761 - auc: 0.8961 - val_loss: 0.2133 - val_auc: 0.7605\n",
            "Epoch 152/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0667 - auc: 0.8928 - val_loss: 0.2085 - val_auc: 0.7659\n",
            "Epoch 153/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0680 - auc: 0.8867 - val_loss: 0.2102 - val_auc: 0.7648\n",
            "Epoch 154/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0649 - auc: 0.8985 - val_loss: 0.2027 - val_auc: 0.7697\n",
            "Epoch 155/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0722 - auc: 0.8971 - val_loss: 0.2088 - val_auc: 0.7736\n",
            "Epoch 156/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0719 - auc: 0.8880 - val_loss: 0.2087 - val_auc: 0.7704\n",
            "Epoch 157/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0649 - auc: 0.9087 - val_loss: 0.2020 - val_auc: 0.7615\n",
            "Epoch 158/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0633 - auc: 0.9096 - val_loss: 0.1964 - val_auc: 0.7671\n",
            "Epoch 159/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0712 - auc: 0.8987 - val_loss: 0.1968 - val_auc: 0.7717\n",
            "Epoch 160/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0666 - auc: 0.9009 - val_loss: 0.2009 - val_auc: 0.7764\n",
            "Epoch 161/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0691 - auc: 0.8981 - val_loss: 0.2017 - val_auc: 0.7778\n",
            "Epoch 162/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0674 - auc: 0.9041 - val_loss: 0.2001 - val_auc: 0.7736\n",
            "Epoch 163/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0620 - auc: 0.9071 - val_loss: 0.2010 - val_auc: 0.7734\n",
            "Epoch 164/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0668 - auc: 0.9055 - val_loss: 0.2016 - val_auc: 0.7607\n",
            "Epoch 165/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0655 - auc: 0.9041 - val_loss: 0.2080 - val_auc: 0.7674\n",
            "Epoch 166/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0687 - auc: 0.9031 - val_loss: 0.2033 - val_auc: 0.7636\n",
            "Epoch 167/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0691 - auc: 0.9061 - val_loss: 0.2084 - val_auc: 0.7648\n",
            "Epoch 168/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0673 - auc: 0.9045 - val_loss: 0.2069 - val_auc: 0.7668\n",
            "Epoch 169/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0636 - auc: 0.9141 - val_loss: 0.2030 - val_auc: 0.7707\n",
            "Epoch 170/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0625 - auc: 0.9171 - val_loss: 0.2040 - val_auc: 0.7692\n",
            "Epoch 171/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0710 - auc: 0.8930 - val_loss: 0.2023 - val_auc: 0.7723\n",
            "Epoch 172/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0655 - auc: 0.9016 - val_loss: 0.1966 - val_auc: 0.7643\n",
            "Epoch 173/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0608 - auc: 0.9107 - val_loss: 0.2005 - val_auc: 0.7658\n",
            "Epoch 174/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0637 - auc: 0.8998 - val_loss: 0.2070 - val_auc: 0.7627\n",
            "Epoch 175/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0724 - auc: 0.9017 - val_loss: 0.2051 - val_auc: 0.7656\n",
            "Epoch 176/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0657 - auc: 0.8938 - val_loss: 0.2035 - val_auc: 0.7723\n",
            "Epoch 177/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0633 - auc: 0.9147 - val_loss: 0.1981 - val_auc: 0.7745\n",
            "Epoch 178/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0673 - auc: 0.9067 - val_loss: 0.2021 - val_auc: 0.7713\n",
            "Epoch 179/1000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0671 - auc: 0.9005 - val_loss: 0.2057 - val_auc: 0.7686\n",
            "Epoch 180/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0610 - auc: 0.8993 - val_loss: 0.2079 - val_auc: 0.7685\n",
            "Epoch 181/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0627 - auc: 0.9091 - val_loss: 0.2106 - val_auc: 0.7640\n",
            "Epoch 182/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0742 - auc: 0.8942 - val_loss: 0.2125 - val_auc: 0.7598\n",
            "Epoch 183/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0667 - auc: 0.8956 - val_loss: 0.2105 - val_auc: 0.7592\n",
            "Epoch 184/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0663 - auc: 0.8973 - val_loss: 0.2128 - val_auc: 0.7602\n",
            "Epoch 185/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0658 - auc: 0.9073 - val_loss: 0.2118 - val_auc: 0.7686\n",
            "Epoch 186/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0596 - auc: 0.8999 - val_loss: 0.2098 - val_auc: 0.7676\n",
            "Epoch 187/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0643 - auc: 0.9049 - val_loss: 0.2121 - val_auc: 0.7697\n",
            "Epoch 188/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0663 - auc: 0.8989 - val_loss: 0.2158 - val_auc: 0.7670\n",
            "Epoch 189/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0682 - auc: 0.9004 - val_loss: 0.2116 - val_auc: 0.7617\n",
            "Epoch 190/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0567 - auc: 0.9087 - val_loss: 0.2082 - val_auc: 0.7728\n",
            "Epoch 191/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0632 - auc: 0.9118 - val_loss: 0.2044 - val_auc: 0.7742\n",
            "Epoch 192/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0616 - auc: 0.9172 - val_loss: 0.2015 - val_auc: 0.7743\n",
            "Epoch 193/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0632 - auc: 0.9167 - val_loss: 0.2044 - val_auc: 0.7715\n",
            "Epoch 194/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0580 - auc: 0.9060 - val_loss: 0.2015 - val_auc: 0.7735\n",
            "Epoch 195/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0711 - auc: 0.9041 - val_loss: 0.2087 - val_auc: 0.7761\n",
            "Epoch 196/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0585 - auc: 0.9108 - val_loss: 0.2080 - val_auc: 0.7720\n",
            "Epoch 197/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0618 - auc: 0.9050 - val_loss: 0.2023 - val_auc: 0.7765\n",
            "Epoch 198/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0650 - auc: 0.9030 - val_loss: 0.2012 - val_auc: 0.7664\n",
            "Epoch 199/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0687 - auc: 0.9100 - val_loss: 0.2039 - val_auc: 0.7734\n",
            "Epoch 200/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0651 - auc: 0.9056 - val_loss: 0.2110 - val_auc: 0.7740\n",
            "Epoch 201/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0680 - auc: 0.9071 - val_loss: 0.1996 - val_auc: 0.7669\n",
            "Epoch 202/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0614 - auc: 0.9037 - val_loss: 0.2006 - val_auc: 0.7761\n",
            "Epoch 203/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0631 - auc: 0.9144 - val_loss: 0.1996 - val_auc: 0.7739\n",
            "Epoch 204/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0659 - auc: 0.8996 - val_loss: 0.2048 - val_auc: 0.7684\n",
            "Epoch 205/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0585 - auc: 0.9069 - val_loss: 0.2026 - val_auc: 0.7714\n",
            "Epoch 206/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0616 - auc: 0.9235 - val_loss: 0.2089 - val_auc: 0.7627\n",
            "Epoch 207/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0591 - auc: 0.9062 - val_loss: 0.2070 - val_auc: 0.7623\n",
            "Epoch 208/1000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0591 - auc: 0.9167 - val_loss: 0.2041 - val_auc: 0.7648\n",
            "Epoch 209/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0655 - auc: 0.9124 - val_loss: 0.2138 - val_auc: 0.7616\n",
            "Epoch 210/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0531 - auc: 0.9254 - val_loss: 0.2144 - val_auc: 0.7591\n",
            "Epoch 211/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0635 - auc: 0.9052 - val_loss: 0.2097 - val_auc: 0.7640\n",
            "Epoch 212/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0609 - auc: 0.9140 - val_loss: 0.2125 - val_auc: 0.7585\n",
            "Epoch 213/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0568 - auc: 0.9052 - val_loss: 0.2068 - val_auc: 0.7621\n",
            "Epoch 214/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0679 - auc: 0.9086 - val_loss: 0.2044 - val_auc: 0.7655\n",
            "Epoch 215/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0594 - auc: 0.9131 - val_loss: 0.2024 - val_auc: 0.7702\n",
            "Epoch 216/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0635 - auc: 0.9028 - val_loss: 0.2012 - val_auc: 0.7701\n",
            "Epoch 217/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0647 - auc: 0.9041 - val_loss: 0.2103 - val_auc: 0.7639\n",
            "Epoch 218/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0611 - auc: 0.9104 - val_loss: 0.2059 - val_auc: 0.7660\n",
            "Epoch 219/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0599 - auc: 0.9060 - val_loss: 0.2038 - val_auc: 0.7684\n",
            "Epoch 220/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0602 - auc: 0.9079 - val_loss: 0.2084 - val_auc: 0.7653\n",
            "Epoch 221/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0638 - auc: 0.9122 - val_loss: 0.2074 - val_auc: 0.7659\n",
            "Epoch 222/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0629 - auc: 0.9074 - val_loss: 0.2095 - val_auc: 0.7630\n",
            "Epoch 223/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0637 - auc: 0.9219 - val_loss: 0.2099 - val_auc: 0.7678\n",
            "Epoch 224/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0552 - auc: 0.9161 - val_loss: 0.2144 - val_auc: 0.7654\n",
            "Epoch 225/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0594 - auc: 0.9176 - val_loss: 0.2098 - val_auc: 0.7653\n",
            "Epoch 226/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0553 - auc: 0.9281 - val_loss: 0.2138 - val_auc: 0.7673\n",
            "Epoch 227/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0631 - auc: 0.9116 - val_loss: 0.2078 - val_auc: 0.7598\n",
            "Epoch 228/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0535 - auc: 0.9219 - val_loss: 0.2122 - val_auc: 0.7658\n",
            "Epoch 229/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0610 - auc: 0.9177 - val_loss: 0.2106 - val_auc: 0.7695\n",
            "Epoch 230/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0619 - auc: 0.9028 - val_loss: 0.2085 - val_auc: 0.7593\n",
            "Epoch 231/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0636 - auc: 0.9110 - val_loss: 0.2113 - val_auc: 0.7611\n",
            "Epoch 232/1000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0559 - auc: 0.9064 - val_loss: 0.2116 - val_auc: 0.7587\n",
            "Epoch 233/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0561 - auc: 0.9112 - val_loss: 0.2161 - val_auc: 0.7615\n",
            "Epoch 234/1000\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0574 - auc: 0.9131 - val_loss: 0.2124 - val_auc: 0.7604\n",
            "Epoch 235/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0620 - auc: 0.9129 - val_loss: 0.2102 - val_auc: 0.7640\n",
            "Epoch 236/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0652 - auc: 0.8966 - val_loss: 0.2121 - val_auc: 0.7609\n",
            "Epoch 237/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0592 - auc: 0.9163 - val_loss: 0.2035 - val_auc: 0.7655\n",
            "Epoch 238/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0555 - auc: 0.9268 - val_loss: 0.2038 - val_auc: 0.7681\n",
            "Epoch 239/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0592 - auc: 0.9058 - val_loss: 0.2094 - val_auc: 0.7712\n",
            "Epoch 240/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0621 - auc: 0.9135 - val_loss: 0.2110 - val_auc: 0.7687\n",
            "Epoch 241/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0711 - auc: 0.9027 - val_loss: 0.2060 - val_auc: 0.7692\n",
            "Epoch 242/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0580 - auc: 0.9152 - val_loss: 0.2031 - val_auc: 0.7699\n",
            "Epoch 243/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0642 - auc: 0.9051 - val_loss: 0.2079 - val_auc: 0.7634\n",
            "Epoch 244/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0607 - auc: 0.9027 - val_loss: 0.2058 - val_auc: 0.7703\n",
            "Epoch 245/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0591 - auc: 0.9119 - val_loss: 0.2058 - val_auc: 0.7735\n",
            "Epoch 246/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0548 - auc: 0.9259 - val_loss: 0.2078 - val_auc: 0.7593\n",
            "Epoch 247/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0576 - auc: 0.9112 - val_loss: 0.2044 - val_auc: 0.7666\n",
            "Epoch 248/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0577 - auc: 0.9116 - val_loss: 0.2063 - val_auc: 0.7735\n",
            "Epoch 249/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0588 - auc: 0.9059 - val_loss: 0.2118 - val_auc: 0.7592\n",
            "Epoch 250/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0558 - auc: 0.9158 - val_loss: 0.2104 - val_auc: 0.7708\n",
            "Epoch 251/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0541 - auc: 0.9146 - val_loss: 0.2198 - val_auc: 0.7495\n",
            "Epoch 252/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0627 - auc: 0.9134 - val_loss: 0.2212 - val_auc: 0.7536\n",
            "Epoch 253/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0646 - auc: 0.9098 - val_loss: 0.2164 - val_auc: 0.7626\n",
            "Epoch 254/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0567 - auc: 0.9124 - val_loss: 0.2183 - val_auc: 0.7627\n",
            "Epoch 255/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0637 - auc: 0.9088 - val_loss: 0.2175 - val_auc: 0.7547\n",
            "Epoch 256/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0635 - auc: 0.9046 - val_loss: 0.2153 - val_auc: 0.7625\n",
            "Epoch 257/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0549 - auc: 0.9144 - val_loss: 0.2138 - val_auc: 0.7626\n",
            "Epoch 258/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0633 - auc: 0.9050 - val_loss: 0.2158 - val_auc: 0.7597\n",
            "Epoch 259/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0586 - auc: 0.9104 - val_loss: 0.2102 - val_auc: 0.7663\n",
            "Epoch 260/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0580 - auc: 0.9094 - val_loss: 0.2158 - val_auc: 0.7579\n",
            "Epoch 261/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0621 - auc: 0.9005 - val_loss: 0.2217 - val_auc: 0.7490\n",
            "Epoch 262/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0545 - auc: 0.9239 - val_loss: 0.2163 - val_auc: 0.7570\n",
            "Epoch 263/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0566 - auc: 0.9200 - val_loss: 0.2232 - val_auc: 0.7495\n",
            "Epoch 264/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0556 - auc: 0.9223 - val_loss: 0.2195 - val_auc: 0.7518\n",
            "Epoch 265/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0592 - auc: 0.9115 - val_loss: 0.2178 - val_auc: 0.7528\n",
            "Epoch 266/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0515 - auc: 0.9235 - val_loss: 0.2209 - val_auc: 0.7451\n",
            "Epoch 267/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0568 - auc: 0.9138 - val_loss: 0.2240 - val_auc: 0.7463\n",
            "Epoch 268/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0578 - auc: 0.9117 - val_loss: 0.2214 - val_auc: 0.7467\n",
            "Epoch 269/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0551 - auc: 0.9143 - val_loss: 0.2186 - val_auc: 0.7501\n",
            "Epoch 270/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0613 - auc: 0.9126 - val_loss: 0.2150 - val_auc: 0.7580\n",
            "Epoch 271/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0571 - auc: 0.9125 - val_loss: 0.2106 - val_auc: 0.7526\n",
            "Epoch 272/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0543 - auc: 0.9198 - val_loss: 0.2161 - val_auc: 0.7510\n",
            "Epoch 273/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0557 - auc: 0.9182 - val_loss: 0.2204 - val_auc: 0.7560\n",
            "Epoch 274/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0597 - auc: 0.9057 - val_loss: 0.2174 - val_auc: 0.7537\n",
            "Epoch 275/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0510 - auc: 0.9177 - val_loss: 0.2247 - val_auc: 0.7523\n",
            "Epoch 276/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0589 - auc: 0.9036 - val_loss: 0.2253 - val_auc: 0.7484\n",
            "Epoch 277/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0578 - auc: 0.9163 - val_loss: 0.2209 - val_auc: 0.7556\n",
            "Epoch 278/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0549 - auc: 0.9189 - val_loss: 0.2256 - val_auc: 0.7513\n",
            "Epoch 279/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0614 - auc: 0.9144 - val_loss: 0.2211 - val_auc: 0.7495\n",
            "Epoch 280/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0544 - auc: 0.9243 - val_loss: 0.2158 - val_auc: 0.7539\n",
            "Epoch 281/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0522 - auc: 0.9262 - val_loss: 0.2152 - val_auc: 0.7576\n",
            "Epoch 282/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0579 - auc: 0.9237 - val_loss: 0.2169 - val_auc: 0.7572\n",
            "Epoch 283/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0566 - auc: 0.9226 - val_loss: 0.2162 - val_auc: 0.7585\n",
            "Epoch 284/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0554 - auc: 0.9229 - val_loss: 0.2118 - val_auc: 0.7584\n",
            "Epoch 285/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0601 - auc: 0.9232 - val_loss: 0.2134 - val_auc: 0.7606\n",
            "Epoch 286/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0544 - auc: 0.9249 - val_loss: 0.2150 - val_auc: 0.7605\n",
            "Epoch 287/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0572 - auc: 0.9174 - val_loss: 0.2128 - val_auc: 0.7618\n",
            "Epoch 288/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0565 - auc: 0.9202 - val_loss: 0.2213 - val_auc: 0.7530\n",
            "Epoch 289/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0550 - auc: 0.9149 - val_loss: 0.2158 - val_auc: 0.7551\n",
            "Epoch 290/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0584 - auc: 0.9184 - val_loss: 0.2140 - val_auc: 0.7560\n",
            "Epoch 291/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0603 - auc: 0.9170 - val_loss: 0.2162 - val_auc: 0.7515\n",
            "Epoch 292/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0593 - auc: 0.9198 - val_loss: 0.2091 - val_auc: 0.7637\n",
            "Epoch 293/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0564 - auc: 0.9131 - val_loss: 0.2114 - val_auc: 0.7571\n",
            "Epoch 294/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0571 - auc: 0.9201 - val_loss: 0.2128 - val_auc: 0.7597\n",
            "Epoch 295/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0554 - auc: 0.9135 - val_loss: 0.2200 - val_auc: 0.7564\n",
            "Epoch 296/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0550 - auc: 0.9169 - val_loss: 0.2139 - val_auc: 0.7652\n",
            "Epoch 297/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0555 - auc: 0.9225 - val_loss: 0.2117 - val_auc: 0.7639\n",
            "Epoch 298/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0601 - auc: 0.9103 - val_loss: 0.2107 - val_auc: 0.7635\n",
            "Epoch 299/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0591 - auc: 0.9178 - val_loss: 0.2122 - val_auc: 0.7627\n",
            "Epoch 300/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0541 - auc: 0.9241 - val_loss: 0.2172 - val_auc: 0.7562\n",
            "Epoch 301/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0566 - auc: 0.9193 - val_loss: 0.2182 - val_auc: 0.7551\n",
            "Epoch 302/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0540 - auc: 0.9260 - val_loss: 0.2142 - val_auc: 0.7577\n",
            "Epoch 303/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0522 - auc: 0.9208 - val_loss: 0.2168 - val_auc: 0.7578\n",
            "Epoch 304/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0507 - auc: 0.9216 - val_loss: 0.2164 - val_auc: 0.7569\n",
            "Epoch 305/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0602 - auc: 0.9212 - val_loss: 0.2170 - val_auc: 0.7515\n",
            "Epoch 306/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0577 - auc: 0.9101 - val_loss: 0.2236 - val_auc: 0.7536\n",
            "Epoch 307/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0576 - auc: 0.9158 - val_loss: 0.2178 - val_auc: 0.7603\n",
            "Epoch 308/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0518 - auc: 0.9277 - val_loss: 0.2148 - val_auc: 0.7583\n",
            "Epoch 309/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0567 - auc: 0.9222 - val_loss: 0.2108 - val_auc: 0.7577\n",
            "Epoch 310/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0526 - auc: 0.9138 - val_loss: 0.2115 - val_auc: 0.7540\n",
            "Epoch 311/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0548 - auc: 0.9177 - val_loss: 0.2153 - val_auc: 0.7620\n",
            "Epoch 312/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0519 - auc: 0.9267 - val_loss: 0.2150 - val_auc: 0.7555\n",
            "Epoch 313/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0556 - auc: 0.9208 - val_loss: 0.2147 - val_auc: 0.7588\n",
            "Epoch 314/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0489 - auc: 0.9151 - val_loss: 0.2147 - val_auc: 0.7603\n",
            "Epoch 315/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0637 - auc: 0.9138 - val_loss: 0.2178 - val_auc: 0.7631\n",
            "Epoch 316/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0572 - auc: 0.9144 - val_loss: 0.2182 - val_auc: 0.7610\n",
            "Epoch 317/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0595 - auc: 0.9086 - val_loss: 0.2177 - val_auc: 0.7601\n",
            "Epoch 318/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0625 - auc: 0.9190 - val_loss: 0.2173 - val_auc: 0.7562\n",
            "Epoch 319/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0554 - auc: 0.9241 - val_loss: 0.2112 - val_auc: 0.7590\n",
            "Epoch 320/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0541 - auc: 0.9162 - val_loss: 0.2126 - val_auc: 0.7554\n",
            "Epoch 321/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0518 - auc: 0.9195 - val_loss: 0.2185 - val_auc: 0.7555\n",
            "Epoch 322/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0513 - auc: 0.9172 - val_loss: 0.2155 - val_auc: 0.7589\n",
            "Epoch 323/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0471 - auc: 0.9220 - val_loss: 0.2210 - val_auc: 0.7576\n",
            "Epoch 324/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0550 - auc: 0.9202 - val_loss: 0.2213 - val_auc: 0.7592\n",
            "Epoch 325/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0545 - auc: 0.9168 - val_loss: 0.2159 - val_auc: 0.7551\n",
            "Epoch 326/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0545 - auc: 0.9262 - val_loss: 0.2153 - val_auc: 0.7560\n",
            "Epoch 327/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0573 - auc: 0.9153 - val_loss: 0.2175 - val_auc: 0.7587\n",
            "Epoch 328/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0577 - auc: 0.9219 - val_loss: 0.2166 - val_auc: 0.7583\n",
            "Epoch 329/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0464 - auc: 0.9232 - val_loss: 0.2164 - val_auc: 0.7597\n",
            "Epoch 330/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0644 - auc: 0.8964 - val_loss: 0.2163 - val_auc: 0.7639\n",
            "Epoch 331/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0557 - auc: 0.9179 - val_loss: 0.2173 - val_auc: 0.7636\n",
            "Epoch 332/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0586 - auc: 0.9163 - val_loss: 0.2196 - val_auc: 0.7593\n",
            "Epoch 333/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0491 - auc: 0.9188 - val_loss: 0.2216 - val_auc: 0.7579\n",
            "Epoch 334/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0527 - auc: 0.9242 - val_loss: 0.2162 - val_auc: 0.7517\n",
            "Epoch 335/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0573 - auc: 0.9169 - val_loss: 0.2172 - val_auc: 0.7526\n",
            "Epoch 336/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0524 - auc: 0.9220 - val_loss: 0.2196 - val_auc: 0.7540\n",
            "Epoch 337/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0513 - auc: 0.9226 - val_loss: 0.2157 - val_auc: 0.7550\n",
            "Epoch 338/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0566 - auc: 0.9196 - val_loss: 0.2136 - val_auc: 0.7513\n",
            "Epoch 339/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0518 - auc: 0.9157 - val_loss: 0.2194 - val_auc: 0.7512\n",
            "Epoch 340/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0589 - auc: 0.9215 - val_loss: 0.2138 - val_auc: 0.7592\n",
            "Epoch 341/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0589 - auc: 0.9162 - val_loss: 0.2147 - val_auc: 0.7558\n",
            "Epoch 342/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0522 - auc: 0.9253 - val_loss: 0.2163 - val_auc: 0.7538\n",
            "Epoch 343/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0590 - auc: 0.9174 - val_loss: 0.2173 - val_auc: 0.7475\n",
            "Epoch 344/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0525 - auc: 0.9335 - val_loss: 0.2191 - val_auc: 0.7497\n",
            "Epoch 345/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0549 - auc: 0.9227 - val_loss: 0.2202 - val_auc: 0.7500\n",
            "Epoch 346/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0608 - auc: 0.9139 - val_loss: 0.2190 - val_auc: 0.7444\n",
            "Epoch 347/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0553 - auc: 0.9176 - val_loss: 0.2211 - val_auc: 0.7547\n",
            "Epoch 348/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0552 - auc: 0.9140 - val_loss: 0.2198 - val_auc: 0.7471\n",
            "Epoch 349/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0501 - auc: 0.9230 - val_loss: 0.2224 - val_auc: 0.7452\n",
            "Epoch 350/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0526 - auc: 0.9226 - val_loss: 0.2228 - val_auc: 0.7432\n",
            "Epoch 351/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0546 - auc: 0.9221 - val_loss: 0.2146 - val_auc: 0.7572\n",
            "Epoch 352/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0547 - auc: 0.9252 - val_loss: 0.2138 - val_auc: 0.7564\n",
            "Epoch 353/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0500 - auc: 0.9223 - val_loss: 0.2183 - val_auc: 0.7480\n",
            "Epoch 354/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0585 - auc: 0.9221 - val_loss: 0.2187 - val_auc: 0.7488\n",
            "Epoch 355/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0457 - auc: 0.9358 - val_loss: 0.2154 - val_auc: 0.7502\n",
            "Epoch 356/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0505 - auc: 0.9246 - val_loss: 0.2232 - val_auc: 0.7406\n",
            "Epoch 357/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0499 - auc: 0.9232 - val_loss: 0.2201 - val_auc: 0.7428\n",
            "Epoch 358/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0539 - auc: 0.9245 - val_loss: 0.2162 - val_auc: 0.7499\n",
            "Epoch 359/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0542 - auc: 0.9243 - val_loss: 0.2176 - val_auc: 0.7502\n",
            "Epoch 360/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0539 - auc: 0.9361 - val_loss: 0.2224 - val_auc: 0.7452\n",
            "Epoch 361/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0520 - auc: 0.9259 - val_loss: 0.2201 - val_auc: 0.7505\n",
            "Epoch 362/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0447 - auc: 0.9262 - val_loss: 0.2224 - val_auc: 0.7487\n",
            "Epoch 363/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0509 - auc: 0.9275 - val_loss: 0.2212 - val_auc: 0.7482\n",
            "Epoch 364/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0587 - auc: 0.9095 - val_loss: 0.2281 - val_auc: 0.7432\n",
            "Epoch 365/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0528 - auc: 0.9259 - val_loss: 0.2133 - val_auc: 0.7578\n",
            "Epoch 366/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0550 - auc: 0.9216 - val_loss: 0.2113 - val_auc: 0.7539\n",
            "Epoch 367/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0512 - auc: 0.9306 - val_loss: 0.2170 - val_auc: 0.7499\n",
            "Epoch 368/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0590 - auc: 0.9219 - val_loss: 0.2185 - val_auc: 0.7526\n",
            "Epoch 369/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0458 - auc: 0.9307 - val_loss: 0.2146 - val_auc: 0.7517\n",
            "Epoch 370/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0538 - auc: 0.9181 - val_loss: 0.2181 - val_auc: 0.7484\n",
            "Epoch 371/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0554 - auc: 0.9183 - val_loss: 0.2194 - val_auc: 0.7488\n",
            "Epoch 372/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0507 - auc: 0.9158 - val_loss: 0.2194 - val_auc: 0.7474\n",
            "Epoch 373/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0495 - auc: 0.9290 - val_loss: 0.2194 - val_auc: 0.7487\n",
            "Epoch 374/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0560 - auc: 0.9173 - val_loss: 0.2159 - val_auc: 0.7513\n",
            "Epoch 375/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0547 - auc: 0.9229 - val_loss: 0.2100 - val_auc: 0.7595\n",
            "Epoch 376/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0500 - auc: 0.9264 - val_loss: 0.2129 - val_auc: 0.7584\n",
            "Epoch 377/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0534 - auc: 0.9259 - val_loss: 0.2119 - val_auc: 0.7523\n",
            "Epoch 378/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0506 - auc: 0.9201 - val_loss: 0.2133 - val_auc: 0.7524\n",
            "Epoch 379/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0537 - auc: 0.9139 - val_loss: 0.2193 - val_auc: 0.7439\n",
            "Epoch 380/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0500 - auc: 0.9219 - val_loss: 0.2172 - val_auc: 0.7495\n",
            "Epoch 381/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0554 - auc: 0.9225 - val_loss: 0.2162 - val_auc: 0.7508\n",
            "Epoch 382/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0469 - auc: 0.9255 - val_loss: 0.2212 - val_auc: 0.7458\n",
            "Epoch 383/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0588 - auc: 0.9178 - val_loss: 0.2235 - val_auc: 0.7462\n",
            "Epoch 384/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0464 - auc: 0.9324 - val_loss: 0.2173 - val_auc: 0.7557\n",
            "Epoch 385/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0491 - auc: 0.9294 - val_loss: 0.2098 - val_auc: 0.7596\n",
            "Epoch 386/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0528 - auc: 0.9198 - val_loss: 0.2129 - val_auc: 0.7434\n",
            "Epoch 387/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0523 - auc: 0.9123 - val_loss: 0.2132 - val_auc: 0.7479\n",
            "Epoch 388/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0498 - auc: 0.9316 - val_loss: 0.2118 - val_auc: 0.7486\n",
            "Epoch 389/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0594 - auc: 0.9249 - val_loss: 0.2094 - val_auc: 0.7498\n",
            "Epoch 390/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0495 - auc: 0.9232 - val_loss: 0.2098 - val_auc: 0.7510\n",
            "Epoch 391/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0557 - auc: 0.9167 - val_loss: 0.2151 - val_auc: 0.7474\n",
            "Epoch 392/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0489 - auc: 0.9254 - val_loss: 0.2127 - val_auc: 0.7516\n",
            "Epoch 393/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0487 - auc: 0.9340 - val_loss: 0.2114 - val_auc: 0.7517\n",
            "Epoch 394/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0480 - auc: 0.9364 - val_loss: 0.2079 - val_auc: 0.7557\n",
            "Epoch 395/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0492 - auc: 0.9375 - val_loss: 0.2100 - val_auc: 0.7527\n",
            "Epoch 396/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0536 - auc: 0.9214 - val_loss: 0.2126 - val_auc: 0.7501\n",
            "Epoch 397/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0555 - auc: 0.9234 - val_loss: 0.2117 - val_auc: 0.7474\n",
            "Epoch 398/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0491 - auc: 0.9246 - val_loss: 0.2108 - val_auc: 0.7513\n",
            "Epoch 399/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0508 - auc: 0.9207 - val_loss: 0.2095 - val_auc: 0.7551\n",
            "Epoch 400/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0516 - auc: 0.9193 - val_loss: 0.2151 - val_auc: 0.7463\n",
            "Epoch 401/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0511 - auc: 0.9246 - val_loss: 0.2138 - val_auc: 0.7475\n",
            "Epoch 402/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0547 - auc: 0.9215 - val_loss: 0.2195 - val_auc: 0.7445\n",
            "Epoch 403/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0546 - auc: 0.9219 - val_loss: 0.2212 - val_auc: 0.7471\n",
            "Epoch 404/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0556 - auc: 0.9387 - val_loss: 0.2149 - val_auc: 0.7447\n",
            "Epoch 405/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0525 - auc: 0.9254 - val_loss: 0.2158 - val_auc: 0.7422\n",
            "Epoch 406/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0527 - auc: 0.9240 - val_loss: 0.2132 - val_auc: 0.7485\n",
            "Epoch 407/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0506 - auc: 0.9249 - val_loss: 0.2191 - val_auc: 0.7396\n",
            "Epoch 408/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0527 - auc: 0.9229 - val_loss: 0.2214 - val_auc: 0.7383\n",
            "Epoch 409/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0498 - auc: 0.9250 - val_loss: 0.2247 - val_auc: 0.7410\n",
            "Epoch 410/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0511 - auc: 0.9283 - val_loss: 0.2221 - val_auc: 0.7387\n",
            "Epoch 411/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0519 - auc: 0.9277 - val_loss: 0.2209 - val_auc: 0.7454\n",
            "Epoch 412/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0528 - auc: 0.9214 - val_loss: 0.2204 - val_auc: 0.7447\n",
            "Epoch 413/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0530 - auc: 0.9228 - val_loss: 0.2198 - val_auc: 0.7482\n",
            "Epoch 414/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0530 - auc: 0.9218 - val_loss: 0.2175 - val_auc: 0.7471\n",
            "Epoch 415/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0540 - auc: 0.9237 - val_loss: 0.2138 - val_auc: 0.7520\n",
            "Epoch 416/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0510 - auc: 0.9229 - val_loss: 0.2113 - val_auc: 0.7517\n",
            "Epoch 417/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0441 - auc: 0.9300 - val_loss: 0.2127 - val_auc: 0.7511\n",
            "Epoch 418/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0474 - auc: 0.9273 - val_loss: 0.2116 - val_auc: 0.7543\n",
            "Epoch 419/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0440 - auc: 0.9345 - val_loss: 0.2061 - val_auc: 0.7565\n",
            "Epoch 420/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0537 - auc: 0.9277 - val_loss: 0.2102 - val_auc: 0.7543\n",
            "Epoch 421/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0493 - auc: 0.9336 - val_loss: 0.2114 - val_auc: 0.7480\n",
            "Epoch 422/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0490 - auc: 0.9302 - val_loss: 0.2148 - val_auc: 0.7459\n",
            "Epoch 423/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0523 - auc: 0.9218 - val_loss: 0.2118 - val_auc: 0.7471\n",
            "Epoch 424/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0482 - auc: 0.9304 - val_loss: 0.2103 - val_auc: 0.7518\n",
            "Epoch 425/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0526 - auc: 0.9305 - val_loss: 0.2135 - val_auc: 0.7461\n",
            "Epoch 426/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0507 - auc: 0.9268 - val_loss: 0.2154 - val_auc: 0.7467\n",
            "Epoch 427/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0513 - auc: 0.9247 - val_loss: 0.2131 - val_auc: 0.7477\n",
            "Epoch 428/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0498 - auc: 0.9195 - val_loss: 0.2124 - val_auc: 0.7498\n",
            "Epoch 429/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0479 - auc: 0.9232 - val_loss: 0.2111 - val_auc: 0.7513\n",
            "Epoch 430/1000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.0527 - auc: 0.9330 - val_loss: 0.2174 - val_auc: 0.7510\n",
            "Epoch 431/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0510 - auc: 0.9238 - val_loss: 0.2174 - val_auc: 0.7472\n",
            "Epoch 432/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0531 - auc: 0.9316 - val_loss: 0.2163 - val_auc: 0.7460\n",
            "Epoch 433/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0512 - auc: 0.9259 - val_loss: 0.2153 - val_auc: 0.7480\n",
            "Epoch 434/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0526 - auc: 0.9274 - val_loss: 0.2152 - val_auc: 0.7515\n",
            "Epoch 435/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0488 - auc: 0.9267 - val_loss: 0.2169 - val_auc: 0.7520\n",
            "Epoch 436/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0453 - auc: 0.9293 - val_loss: 0.2179 - val_auc: 0.7545\n",
            "Epoch 437/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0493 - auc: 0.9222 - val_loss: 0.2196 - val_auc: 0.7523\n",
            "Epoch 438/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0586 - auc: 0.9195 - val_loss: 0.2210 - val_auc: 0.7603\n",
            "Epoch 439/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0475 - auc: 0.9318 - val_loss: 0.2179 - val_auc: 0.7581\n",
            "Epoch 440/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0486 - auc: 0.9183 - val_loss: 0.2227 - val_auc: 0.7589\n",
            "Epoch 441/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0474 - auc: 0.9423 - val_loss: 0.2189 - val_auc: 0.7564\n",
            "Epoch 442/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0532 - auc: 0.9190 - val_loss: 0.2153 - val_auc: 0.7531\n",
            "Epoch 443/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0467 - auc: 0.9340 - val_loss: 0.2173 - val_auc: 0.7589\n",
            "Epoch 444/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0462 - auc: 0.9382 - val_loss: 0.2211 - val_auc: 0.7499\n",
            "Epoch 445/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0540 - auc: 0.9216 - val_loss: 0.2184 - val_auc: 0.7606\n",
            "Epoch 446/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0418 - auc: 0.9264 - val_loss: 0.2181 - val_auc: 0.7625\n",
            "Epoch 447/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0473 - auc: 0.9291 - val_loss: 0.2211 - val_auc: 0.7507\n",
            "Epoch 448/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0460 - auc: 0.9246 - val_loss: 0.2237 - val_auc: 0.7521\n",
            "Epoch 449/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0494 - auc: 0.9308 - val_loss: 0.2205 - val_auc: 0.7591\n",
            "Epoch 450/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0500 - auc: 0.9332 - val_loss: 0.2216 - val_auc: 0.7525\n",
            "Epoch 451/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0505 - auc: 0.9272 - val_loss: 0.2199 - val_auc: 0.7531\n",
            "Epoch 452/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0476 - auc: 0.9270 - val_loss: 0.2240 - val_auc: 0.7530\n",
            "Epoch 453/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0468 - auc: 0.9332 - val_loss: 0.2263 - val_auc: 0.7454\n",
            "Epoch 454/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0437 - auc: 0.9329 - val_loss: 0.2273 - val_auc: 0.7461\n",
            "Epoch 455/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0497 - auc: 0.9162 - val_loss: 0.2251 - val_auc: 0.7531\n",
            "Epoch 456/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0469 - auc: 0.9269 - val_loss: 0.2271 - val_auc: 0.7423\n",
            "Epoch 457/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0451 - auc: 0.9234 - val_loss: 0.2270 - val_auc: 0.7453\n",
            "Epoch 458/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0490 - auc: 0.9284 - val_loss: 0.2263 - val_auc: 0.7487\n",
            "Epoch 459/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0470 - auc: 0.9302 - val_loss: 0.2258 - val_auc: 0.7445\n",
            "Epoch 460/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0450 - auc: 0.9283 - val_loss: 0.2248 - val_auc: 0.7423\n",
            "Epoch 461/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0513 - auc: 0.9231 - val_loss: 0.2243 - val_auc: 0.7488\n",
            "Epoch 462/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0525 - auc: 0.9236 - val_loss: 0.2223 - val_auc: 0.7474\n",
            "Epoch 463/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0491 - auc: 0.9352 - val_loss: 0.2241 - val_auc: 0.7433\n",
            "Epoch 464/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0502 - auc: 0.9271 - val_loss: 0.2248 - val_auc: 0.7444\n",
            "Epoch 465/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0443 - auc: 0.9289 - val_loss: 0.2243 - val_auc: 0.7390\n",
            "Epoch 466/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0460 - auc: 0.9304 - val_loss: 0.2234 - val_auc: 0.7412\n",
            "Epoch 467/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0461 - auc: 0.9225 - val_loss: 0.2223 - val_auc: 0.7414\n",
            "Epoch 468/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0475 - auc: 0.9289 - val_loss: 0.2210 - val_auc: 0.7495\n",
            "Epoch 469/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0581 - auc: 0.9383 - val_loss: 0.2183 - val_auc: 0.7504\n",
            "Epoch 470/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0464 - auc: 0.9400 - val_loss: 0.2151 - val_auc: 0.7487\n",
            "Epoch 471/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0456 - auc: 0.9269 - val_loss: 0.2164 - val_auc: 0.7528\n",
            "Epoch 472/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0444 - auc: 0.9243 - val_loss: 0.2194 - val_auc: 0.7492\n",
            "Epoch 473/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0544 - auc: 0.9139 - val_loss: 0.2161 - val_auc: 0.7560\n",
            "Epoch 474/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0543 - auc: 0.9186 - val_loss: 0.2163 - val_auc: 0.7511\n",
            "Epoch 475/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0514 - auc: 0.9304 - val_loss: 0.2183 - val_auc: 0.7485\n",
            "Epoch 476/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0472 - auc: 0.9266 - val_loss: 0.2214 - val_auc: 0.7469\n",
            "Epoch 477/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0538 - auc: 0.9197 - val_loss: 0.2236 - val_auc: 0.7435\n",
            "Epoch 478/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0485 - auc: 0.9245 - val_loss: 0.2242 - val_auc: 0.7395\n",
            "Epoch 479/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0503 - auc: 0.9232 - val_loss: 0.2222 - val_auc: 0.7381\n",
            "Epoch 480/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0513 - auc: 0.9145 - val_loss: 0.2290 - val_auc: 0.7431\n",
            "Epoch 481/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0499 - auc: 0.9379 - val_loss: 0.2238 - val_auc: 0.7353\n",
            "Epoch 482/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0486 - auc: 0.9293 - val_loss: 0.2235 - val_auc: 0.7434\n",
            "Epoch 483/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0471 - auc: 0.9355 - val_loss: 0.2238 - val_auc: 0.7414\n",
            "Epoch 484/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0493 - auc: 0.9184 - val_loss: 0.2225 - val_auc: 0.7431\n",
            "Epoch 485/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0567 - auc: 0.9264 - val_loss: 0.2266 - val_auc: 0.7403\n",
            "Epoch 486/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0488 - auc: 0.9302 - val_loss: 0.2231 - val_auc: 0.7423\n",
            "Epoch 487/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0407 - auc: 0.9306 - val_loss: 0.2250 - val_auc: 0.7386\n",
            "Epoch 488/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0490 - auc: 0.9254 - val_loss: 0.2252 - val_auc: 0.7413\n",
            "Epoch 489/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0434 - auc: 0.9377 - val_loss: 0.2241 - val_auc: 0.7514\n",
            "Epoch 490/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0447 - auc: 0.9264 - val_loss: 0.2244 - val_auc: 0.7470\n",
            "Epoch 491/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0416 - auc: 0.9353 - val_loss: 0.2291 - val_auc: 0.7414\n",
            "Epoch 492/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0437 - auc: 0.9353 - val_loss: 0.2225 - val_auc: 0.7515\n",
            "Epoch 493/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0419 - auc: 0.9324 - val_loss: 0.2244 - val_auc: 0.7449\n",
            "Epoch 494/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0501 - auc: 0.9167 - val_loss: 0.2278 - val_auc: 0.7467\n",
            "Epoch 495/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0538 - auc: 0.9251 - val_loss: 0.2260 - val_auc: 0.7498\n",
            "Epoch 496/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0466 - auc: 0.9335 - val_loss: 0.2265 - val_auc: 0.7507\n",
            "Epoch 497/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0526 - auc: 0.9246 - val_loss: 0.2265 - val_auc: 0.7408\n",
            "Epoch 498/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0396 - auc: 0.9397 - val_loss: 0.2245 - val_auc: 0.7483\n",
            "Epoch 499/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0449 - auc: 0.9308 - val_loss: 0.2234 - val_auc: 0.7473\n",
            "Epoch 500/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0481 - auc: 0.9235 - val_loss: 0.2215 - val_auc: 0.7462\n",
            "Epoch 501/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0477 - auc: 0.9336 - val_loss: 0.2229 - val_auc: 0.7417\n",
            "Epoch 502/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0458 - auc: 0.9349 - val_loss: 0.2263 - val_auc: 0.7443\n",
            "Epoch 503/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0519 - auc: 0.9261 - val_loss: 0.2270 - val_auc: 0.7379\n",
            "Epoch 504/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0443 - auc: 0.9416 - val_loss: 0.2268 - val_auc: 0.7382\n",
            "Epoch 505/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0483 - auc: 0.9283 - val_loss: 0.2239 - val_auc: 0.7435\n",
            "Epoch 506/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0415 - auc: 0.9337 - val_loss: 0.2206 - val_auc: 0.7443\n",
            "Epoch 507/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0514 - auc: 0.9226 - val_loss: 0.2199 - val_auc: 0.7437\n",
            "Epoch 508/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0456 - auc: 0.9329 - val_loss: 0.2226 - val_auc: 0.7483\n",
            "Epoch 509/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0489 - auc: 0.9314 - val_loss: 0.2232 - val_auc: 0.7525\n",
            "Epoch 510/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0481 - auc: 0.9206 - val_loss: 0.2224 - val_auc: 0.7486\n",
            "Epoch 511/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0562 - auc: 0.9209 - val_loss: 0.2265 - val_auc: 0.7471\n",
            "Epoch 512/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0456 - auc: 0.9316 - val_loss: 0.2303 - val_auc: 0.7452\n",
            "Epoch 513/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0466 - auc: 0.9274 - val_loss: 0.2235 - val_auc: 0.7454\n",
            "Epoch 514/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0508 - auc: 0.9264 - val_loss: 0.2242 - val_auc: 0.7473\n",
            "Epoch 515/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0555 - auc: 0.9192 - val_loss: 0.2247 - val_auc: 0.7415\n",
            "Epoch 516/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0484 - auc: 0.9355 - val_loss: 0.2233 - val_auc: 0.7422\n",
            "Epoch 517/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0498 - auc: 0.9340 - val_loss: 0.2230 - val_auc: 0.7430\n",
            "Epoch 518/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0463 - auc: 0.9267 - val_loss: 0.2260 - val_auc: 0.7406\n",
            "Epoch 519/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0522 - auc: 0.9335 - val_loss: 0.2233 - val_auc: 0.7434\n",
            "Epoch 520/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0478 - auc: 0.9327 - val_loss: 0.2293 - val_auc: 0.7431\n",
            "Epoch 521/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0521 - auc: 0.9267 - val_loss: 0.2294 - val_auc: 0.7429\n",
            "Epoch 522/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0499 - auc: 0.9299 - val_loss: 0.2294 - val_auc: 0.7485\n",
            "Epoch 523/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0519 - auc: 0.9279 - val_loss: 0.2254 - val_auc: 0.7471\n",
            "Epoch 524/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0456 - auc: 0.9309 - val_loss: 0.2232 - val_auc: 0.7491\n",
            "Epoch 525/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0467 - auc: 0.9296 - val_loss: 0.2244 - val_auc: 0.7504\n",
            "Epoch 526/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0447 - auc: 0.9255 - val_loss: 0.2309 - val_auc: 0.7340\n",
            "Epoch 527/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0458 - auc: 0.9340 - val_loss: 0.2228 - val_auc: 0.7490\n",
            "Epoch 528/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0555 - auc: 0.9227 - val_loss: 0.2224 - val_auc: 0.7540\n",
            "Epoch 529/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0472 - auc: 0.9257 - val_loss: 0.2237 - val_auc: 0.7485\n",
            "Epoch 530/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0456 - auc: 0.9278 - val_loss: 0.2256 - val_auc: 0.7468\n",
            "Epoch 531/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0454 - auc: 0.9277 - val_loss: 0.2208 - val_auc: 0.7398\n",
            "Epoch 532/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0394 - auc: 0.9319 - val_loss: 0.2235 - val_auc: 0.7385\n",
            "Epoch 533/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0515 - auc: 0.9337 - val_loss: 0.2205 - val_auc: 0.7457\n",
            "Epoch 534/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0518 - auc: 0.9226 - val_loss: 0.2197 - val_auc: 0.7484\n",
            "Epoch 535/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0476 - auc: 0.9304 - val_loss: 0.2199 - val_auc: 0.7477\n",
            "Epoch 536/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0501 - auc: 0.9224 - val_loss: 0.2168 - val_auc: 0.7526\n",
            "Epoch 537/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0488 - auc: 0.9268 - val_loss: 0.2194 - val_auc: 0.7512\n",
            "Epoch 538/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0457 - auc: 0.9256 - val_loss: 0.2258 - val_auc: 0.7420\n",
            "Epoch 539/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0438 - auc: 0.9316 - val_loss: 0.2248 - val_auc: 0.7448\n",
            "Epoch 540/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0472 - auc: 0.9336 - val_loss: 0.2235 - val_auc: 0.7445\n",
            "Epoch 541/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0430 - auc: 0.9287 - val_loss: 0.2257 - val_auc: 0.7451\n",
            "Epoch 542/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0486 - auc: 0.9337 - val_loss: 0.2253 - val_auc: 0.7405\n",
            "Epoch 543/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0447 - auc: 0.9320 - val_loss: 0.2249 - val_auc: 0.7367\n",
            "Epoch 544/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0458 - auc: 0.9335 - val_loss: 0.2230 - val_auc: 0.7497\n",
            "Epoch 545/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0424 - auc: 0.9304 - val_loss: 0.2217 - val_auc: 0.7503\n",
            "Epoch 546/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0455 - auc: 0.9350 - val_loss: 0.2176 - val_auc: 0.7486\n",
            "Epoch 547/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0451 - auc: 0.9303 - val_loss: 0.2208 - val_auc: 0.7441\n",
            "Epoch 548/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0481 - auc: 0.9324 - val_loss: 0.2198 - val_auc: 0.7416\n",
            "Epoch 549/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0446 - auc: 0.9359 - val_loss: 0.2219 - val_auc: 0.7451\n",
            "Epoch 550/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0473 - auc: 0.9270 - val_loss: 0.2240 - val_auc: 0.7507\n",
            "Epoch 551/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0492 - auc: 0.9270 - val_loss: 0.2187 - val_auc: 0.7515\n",
            "Epoch 552/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0489 - auc: 0.9260 - val_loss: 0.2225 - val_auc: 0.7493\n",
            "Epoch 553/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0524 - auc: 0.9270 - val_loss: 0.2253 - val_auc: 0.7441\n",
            "Epoch 554/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0449 - auc: 0.9309 - val_loss: 0.2249 - val_auc: 0.7508\n",
            "Epoch 555/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0502 - auc: 0.9263 - val_loss: 0.2280 - val_auc: 0.7454\n",
            "Epoch 556/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0486 - auc: 0.9338 - val_loss: 0.2269 - val_auc: 0.7469\n",
            "Epoch 557/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0465 - auc: 0.9262 - val_loss: 0.2255 - val_auc: 0.7506\n",
            "Epoch 558/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0426 - auc: 0.9370 - val_loss: 0.2247 - val_auc: 0.7531\n",
            "Epoch 559/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0467 - auc: 0.9324 - val_loss: 0.2271 - val_auc: 0.7435\n",
            "Epoch 560/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0521 - auc: 0.9244 - val_loss: 0.2259 - val_auc: 0.7520\n",
            "Epoch 561/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0477 - auc: 0.9331 - val_loss: 0.2247 - val_auc: 0.7536\n",
            "Epoch 562/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0486 - auc: 0.9280 - val_loss: 0.2231 - val_auc: 0.7536\n",
            "Epoch 563/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0405 - auc: 0.9402 - val_loss: 0.2254 - val_auc: 0.7562\n",
            "Epoch 564/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0482 - auc: 0.9392 - val_loss: 0.2248 - val_auc: 0.7519\n",
            "Epoch 565/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0472 - auc: 0.9243 - val_loss: 0.2279 - val_auc: 0.7525\n",
            "Epoch 566/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0415 - auc: 0.9319 - val_loss: 0.2258 - val_auc: 0.7520\n",
            "Epoch 567/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0468 - auc: 0.9278 - val_loss: 0.2293 - val_auc: 0.7453\n",
            "Epoch 568/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0411 - auc: 0.9316 - val_loss: 0.2255 - val_auc: 0.7472\n",
            "Epoch 569/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0391 - auc: 0.9324 - val_loss: 0.2255 - val_auc: 0.7475\n",
            "Epoch 570/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0501 - auc: 0.9219 - val_loss: 0.2248 - val_auc: 0.7520\n",
            "Epoch 571/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0480 - auc: 0.9304 - val_loss: 0.2260 - val_auc: 0.7476\n",
            "Epoch 572/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0499 - auc: 0.9335 - val_loss: 0.2272 - val_auc: 0.7429\n",
            "Epoch 573/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0394 - auc: 0.9486 - val_loss: 0.2279 - val_auc: 0.7550\n",
            "Epoch 574/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0472 - auc: 0.9271 - val_loss: 0.2270 - val_auc: 0.7531\n",
            "Epoch 575/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0442 - auc: 0.9380 - val_loss: 0.2289 - val_auc: 0.7525\n",
            "Epoch 576/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0397 - auc: 0.9377 - val_loss: 0.2246 - val_auc: 0.7482\n",
            "Epoch 577/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0463 - auc: 0.9347 - val_loss: 0.2245 - val_auc: 0.7477\n",
            "Epoch 578/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0434 - auc: 0.9383 - val_loss: 0.2243 - val_auc: 0.7525\n",
            "Epoch 579/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0386 - auc: 0.9408 - val_loss: 0.2222 - val_auc: 0.7513\n",
            "Epoch 580/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0454 - auc: 0.9336 - val_loss: 0.2249 - val_auc: 0.7532\n",
            "Epoch 581/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0453 - auc: 0.9318 - val_loss: 0.2233 - val_auc: 0.7510\n",
            "Epoch 582/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0533 - auc: 0.9175 - val_loss: 0.2233 - val_auc: 0.7541\n",
            "Epoch 583/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0507 - auc: 0.9349 - val_loss: 0.2245 - val_auc: 0.7515\n",
            "Epoch 584/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0463 - auc: 0.9316 - val_loss: 0.2265 - val_auc: 0.7485\n",
            "Epoch 585/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0519 - auc: 0.9349 - val_loss: 0.2248 - val_auc: 0.7556\n",
            "Epoch 586/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0495 - auc: 0.9396 - val_loss: 0.2224 - val_auc: 0.7558\n",
            "Epoch 587/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0489 - auc: 0.9302 - val_loss: 0.2223 - val_auc: 0.7582\n",
            "Epoch 588/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0454 - auc: 0.9299 - val_loss: 0.2244 - val_auc: 0.7524\n",
            "Epoch 589/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0418 - auc: 0.9344 - val_loss: 0.2243 - val_auc: 0.7469\n",
            "Epoch 590/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0493 - auc: 0.9360 - val_loss: 0.2226 - val_auc: 0.7491\n",
            "Epoch 591/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0392 - auc: 0.9390 - val_loss: 0.2248 - val_auc: 0.7485\n",
            "Epoch 592/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0519 - auc: 0.9233 - val_loss: 0.2245 - val_auc: 0.7467\n",
            "Epoch 593/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0449 - auc: 0.9300 - val_loss: 0.2262 - val_auc: 0.7483\n",
            "Epoch 594/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0438 - auc: 0.9387 - val_loss: 0.2264 - val_auc: 0.7489\n",
            "Epoch 595/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0436 - auc: 0.9391 - val_loss: 0.2247 - val_auc: 0.7496\n",
            "Epoch 596/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0492 - auc: 0.9340 - val_loss: 0.2238 - val_auc: 0.7507\n",
            "Epoch 597/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0499 - auc: 0.9366 - val_loss: 0.2181 - val_auc: 0.7533\n",
            "Epoch 598/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0389 - auc: 0.9359 - val_loss: 0.2203 - val_auc: 0.7524\n",
            "Epoch 599/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0514 - auc: 0.9282 - val_loss: 0.2224 - val_auc: 0.7474\n",
            "Epoch 600/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0480 - auc: 0.9296 - val_loss: 0.2236 - val_auc: 0.7517\n",
            "Epoch 601/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0417 - auc: 0.9303 - val_loss: 0.2259 - val_auc: 0.7459\n",
            "Epoch 602/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0524 - auc: 0.9246 - val_loss: 0.2244 - val_auc: 0.7451\n",
            "Epoch 603/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0427 - auc: 0.9302 - val_loss: 0.2247 - val_auc: 0.7429\n",
            "Epoch 604/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0440 - auc: 0.9314 - val_loss: 0.2242 - val_auc: 0.7479\n",
            "Epoch 605/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0418 - auc: 0.9407 - val_loss: 0.2246 - val_auc: 0.7499\n",
            "Epoch 606/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0502 - auc: 0.9227 - val_loss: 0.2268 - val_auc: 0.7488\n",
            "Epoch 607/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0398 - auc: 0.9437 - val_loss: 0.2294 - val_auc: 0.7437\n",
            "Epoch 608/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0478 - auc: 0.9221 - val_loss: 0.2282 - val_auc: 0.7502\n",
            "Epoch 609/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0472 - auc: 0.9263 - val_loss: 0.2240 - val_auc: 0.7559\n",
            "Epoch 610/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0490 - auc: 0.9369 - val_loss: 0.2236 - val_auc: 0.7532\n",
            "Epoch 611/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0432 - auc: 0.9337 - val_loss: 0.2218 - val_auc: 0.7515\n",
            "Epoch 612/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0451 - auc: 0.9285 - val_loss: 0.2213 - val_auc: 0.7554\n",
            "Epoch 613/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0464 - auc: 0.9264 - val_loss: 0.2251 - val_auc: 0.7492\n",
            "Epoch 614/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0493 - auc: 0.9226 - val_loss: 0.2264 - val_auc: 0.7451\n",
            "Epoch 615/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0461 - auc: 0.9268 - val_loss: 0.2250 - val_auc: 0.7415\n",
            "Epoch 616/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0447 - auc: 0.9466 - val_loss: 0.2215 - val_auc: 0.7464\n",
            "Epoch 617/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0528 - auc: 0.9228 - val_loss: 0.2240 - val_auc: 0.7484\n",
            "Epoch 618/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0433 - auc: 0.9319 - val_loss: 0.2216 - val_auc: 0.7542\n",
            "Epoch 619/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0474 - auc: 0.9390 - val_loss: 0.2180 - val_auc: 0.7619\n",
            "Epoch 620/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0450 - auc: 0.9343 - val_loss: 0.2234 - val_auc: 0.7524\n",
            "Epoch 621/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0434 - auc: 0.9431 - val_loss: 0.2242 - val_auc: 0.7550\n",
            "Epoch 622/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0405 - auc: 0.9377 - val_loss: 0.2220 - val_auc: 0.7519\n",
            "Epoch 623/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0427 - auc: 0.9422 - val_loss: 0.2191 - val_auc: 0.7569\n",
            "Epoch 624/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0410 - auc: 0.9383 - val_loss: 0.2201 - val_auc: 0.7536\n",
            "Epoch 625/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0477 - auc: 0.9323 - val_loss: 0.2222 - val_auc: 0.7463\n",
            "Epoch 626/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0456 - auc: 0.9324 - val_loss: 0.2191 - val_auc: 0.7507\n",
            "Epoch 627/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0454 - auc: 0.9370 - val_loss: 0.2200 - val_auc: 0.7544\n",
            "Epoch 628/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0476 - auc: 0.9332 - val_loss: 0.2235 - val_auc: 0.7509\n",
            "Epoch 629/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0465 - auc: 0.9319 - val_loss: 0.2265 - val_auc: 0.7515\n",
            "Epoch 630/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0453 - auc: 0.9346 - val_loss: 0.2260 - val_auc: 0.7532\n",
            "Epoch 631/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0436 - auc: 0.9374 - val_loss: 0.2280 - val_auc: 0.7515\n",
            "Epoch 632/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0462 - auc: 0.9295 - val_loss: 0.2279 - val_auc: 0.7480\n",
            "Epoch 633/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0486 - auc: 0.9285 - val_loss: 0.2263 - val_auc: 0.7439\n",
            "Epoch 634/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0360 - auc: 0.9486 - val_loss: 0.2295 - val_auc: 0.7502\n",
            "Epoch 635/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0452 - auc: 0.9320 - val_loss: 0.2295 - val_auc: 0.7424\n",
            "Epoch 636/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0396 - auc: 0.9399 - val_loss: 0.2273 - val_auc: 0.7411\n",
            "Epoch 637/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0430 - auc: 0.9395 - val_loss: 0.2221 - val_auc: 0.7477\n",
            "Epoch 638/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0441 - auc: 0.9311 - val_loss: 0.2256 - val_auc: 0.7345\n",
            "Epoch 639/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0414 - auc: 0.9389 - val_loss: 0.2231 - val_auc: 0.7474\n",
            "Epoch 640/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0495 - auc: 0.9305 - val_loss: 0.2201 - val_auc: 0.7510\n",
            "Epoch 641/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0430 - auc: 0.9415 - val_loss: 0.2217 - val_auc: 0.7490\n",
            "Epoch 642/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0465 - auc: 0.9353 - val_loss: 0.2241 - val_auc: 0.7465\n",
            "Epoch 643/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0428 - auc: 0.9358 - val_loss: 0.2284 - val_auc: 0.7436\n",
            "Epoch 644/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0446 - auc: 0.9311 - val_loss: 0.2258 - val_auc: 0.7482\n",
            "Epoch 645/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0401 - auc: 0.9386 - val_loss: 0.2261 - val_auc: 0.7492\n",
            "Epoch 646/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0375 - auc: 0.9371 - val_loss: 0.2274 - val_auc: 0.7502\n",
            "Epoch 647/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0385 - auc: 0.9436 - val_loss: 0.2308 - val_auc: 0.7450\n",
            "Epoch 648/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0475 - auc: 0.9325 - val_loss: 0.2300 - val_auc: 0.7446\n",
            "Epoch 649/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0408 - auc: 0.9487 - val_loss: 0.2302 - val_auc: 0.7403\n",
            "Epoch 650/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0479 - auc: 0.9245 - val_loss: 0.2260 - val_auc: 0.7491\n",
            "Epoch 651/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0495 - auc: 0.9346 - val_loss: 0.2265 - val_auc: 0.7379\n",
            "Epoch 652/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0478 - auc: 0.9302 - val_loss: 0.2262 - val_auc: 0.7398\n",
            "Epoch 653/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0427 - auc: 0.9292 - val_loss: 0.2199 - val_auc: 0.7545\n",
            "Epoch 654/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0420 - auc: 0.9367 - val_loss: 0.2194 - val_auc: 0.7584\n",
            "Epoch 655/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0447 - auc: 0.9320 - val_loss: 0.2192 - val_auc: 0.7529\n",
            "Epoch 656/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0396 - auc: 0.9439 - val_loss: 0.2194 - val_auc: 0.7524\n",
            "Epoch 657/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0457 - auc: 0.9379 - val_loss: 0.2209 - val_auc: 0.7519\n",
            "Epoch 658/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0428 - auc: 0.9452 - val_loss: 0.2202 - val_auc: 0.7485\n",
            "Epoch 659/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0513 - auc: 0.9324 - val_loss: 0.2202 - val_auc: 0.7506\n",
            "Epoch 660/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0503 - auc: 0.9304 - val_loss: 0.2207 - val_auc: 0.7498\n",
            "Epoch 661/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0371 - auc: 0.9411 - val_loss: 0.2220 - val_auc: 0.7449\n",
            "Epoch 662/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0463 - auc: 0.9345 - val_loss: 0.2212 - val_auc: 0.7521\n",
            "Epoch 663/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0440 - auc: 0.9376 - val_loss: 0.2205 - val_auc: 0.7521\n",
            "Epoch 664/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0409 - auc: 0.9398 - val_loss: 0.2183 - val_auc: 0.7549\n",
            "Epoch 665/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0454 - auc: 0.9410 - val_loss: 0.2197 - val_auc: 0.7531\n",
            "Epoch 666/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0423 - auc: 0.9312 - val_loss: 0.2220 - val_auc: 0.7501\n",
            "Epoch 667/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0463 - auc: 0.9294 - val_loss: 0.2212 - val_auc: 0.7528\n",
            "Epoch 668/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0467 - auc: 0.9383 - val_loss: 0.2199 - val_auc: 0.7505\n",
            "Epoch 669/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0415 - auc: 0.9331 - val_loss: 0.2237 - val_auc: 0.7460\n",
            "Epoch 670/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0500 - auc: 0.9284 - val_loss: 0.2261 - val_auc: 0.7406\n",
            "Epoch 671/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0411 - auc: 0.9350 - val_loss: 0.2219 - val_auc: 0.7452\n",
            "Epoch 672/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0393 - auc: 0.9374 - val_loss: 0.2237 - val_auc: 0.7465\n",
            "Epoch 673/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0402 - auc: 0.9377 - val_loss: 0.2199 - val_auc: 0.7537\n",
            "Epoch 674/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0404 - auc: 0.9366 - val_loss: 0.2204 - val_auc: 0.7503\n",
            "Epoch 675/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0394 - auc: 0.9389 - val_loss: 0.2236 - val_auc: 0.7495\n",
            "Epoch 676/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0390 - auc: 0.9357 - val_loss: 0.2236 - val_auc: 0.7463\n",
            "Epoch 677/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0374 - auc: 0.9313 - val_loss: 0.2224 - val_auc: 0.7497\n",
            "Epoch 678/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0346 - auc: 0.9327 - val_loss: 0.2252 - val_auc: 0.7545\n",
            "Epoch 679/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0435 - auc: 0.9378 - val_loss: 0.2288 - val_auc: 0.7443\n",
            "Epoch 680/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0448 - auc: 0.9298 - val_loss: 0.2320 - val_auc: 0.7550\n",
            "Epoch 681/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0431 - auc: 0.9247 - val_loss: 0.2330 - val_auc: 0.7496\n",
            "Epoch 682/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0394 - auc: 0.9374 - val_loss: 0.2316 - val_auc: 0.7466\n",
            "Epoch 683/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0407 - auc: 0.9368 - val_loss: 0.2338 - val_auc: 0.7431\n",
            "Epoch 684/1000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.0503 - auc: 0.9294 - val_loss: 0.2323 - val_auc: 0.7431\n",
            "Epoch 685/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0441 - auc: 0.9498 - val_loss: 0.2289 - val_auc: 0.7436\n",
            "Epoch 686/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0369 - auc: 0.9435 - val_loss: 0.2246 - val_auc: 0.7432\n",
            "Epoch 687/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0360 - auc: 0.9475 - val_loss: 0.2237 - val_auc: 0.7428\n",
            "Epoch 688/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0408 - auc: 0.9495 - val_loss: 0.2241 - val_auc: 0.7454\n",
            "Epoch 689/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0445 - auc: 0.9320 - val_loss: 0.2250 - val_auc: 0.7457\n",
            "Epoch 690/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0450 - auc: 0.9371 - val_loss: 0.2245 - val_auc: 0.7472\n",
            "Epoch 691/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0503 - auc: 0.9221 - val_loss: 0.2236 - val_auc: 0.7522\n",
            "Epoch 692/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0452 - auc: 0.9376 - val_loss: 0.2249 - val_auc: 0.7573\n",
            "Epoch 693/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0453 - auc: 0.9345 - val_loss: 0.2248 - val_auc: 0.7506\n",
            "Epoch 694/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0449 - auc: 0.9334 - val_loss: 0.2285 - val_auc: 0.7542\n",
            "Epoch 695/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0482 - auc: 0.9297 - val_loss: 0.2293 - val_auc: 0.7482\n",
            "Epoch 696/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0492 - auc: 0.9331 - val_loss: 0.2247 - val_auc: 0.7531\n",
            "Epoch 697/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0469 - auc: 0.9306 - val_loss: 0.2211 - val_auc: 0.7539\n",
            "Epoch 698/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0403 - auc: 0.9361 - val_loss: 0.2234 - val_auc: 0.7492\n",
            "Epoch 699/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0411 - auc: 0.9374 - val_loss: 0.2277 - val_auc: 0.7460\n",
            "Epoch 700/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0473 - auc: 0.9334 - val_loss: 0.2231 - val_auc: 0.7522\n",
            "Epoch 701/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0486 - auc: 0.9272 - val_loss: 0.2241 - val_auc: 0.7517\n",
            "Epoch 702/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0431 - auc: 0.9433 - val_loss: 0.2221 - val_auc: 0.7546\n",
            "Epoch 703/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0475 - auc: 0.9341 - val_loss: 0.2248 - val_auc: 0.7476\n",
            "Epoch 704/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0437 - auc: 0.9397 - val_loss: 0.2237 - val_auc: 0.7498\n",
            "Epoch 705/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0412 - auc: 0.9336 - val_loss: 0.2240 - val_auc: 0.7505\n",
            "Epoch 706/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0423 - auc: 0.9334 - val_loss: 0.2247 - val_auc: 0.7599\n",
            "Epoch 707/1000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.0440 - auc: 0.9338 - val_loss: 0.2275 - val_auc: 0.7581\n",
            "Epoch 708/1000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.0373 - auc: 0.9431 - val_loss: 0.2294 - val_auc: 0.7481\n",
            "Epoch 709/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0414 - auc: 0.9397 - val_loss: 0.2275 - val_auc: 0.7526\n",
            "Epoch 710/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0478 - auc: 0.9435 - val_loss: 0.2234 - val_auc: 0.7531\n",
            "Epoch 711/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0468 - auc: 0.9279 - val_loss: 0.2208 - val_auc: 0.7572\n",
            "Epoch 712/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0412 - auc: 0.9402 - val_loss: 0.2240 - val_auc: 0.7477\n",
            "Epoch 713/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0434 - auc: 0.9313 - val_loss: 0.2269 - val_auc: 0.7435\n",
            "Epoch 714/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0411 - auc: 0.9471 - val_loss: 0.2259 - val_auc: 0.7472\n",
            "Epoch 715/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0408 - auc: 0.9420 - val_loss: 0.2261 - val_auc: 0.7470\n",
            "Epoch 716/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0430 - auc: 0.9303 - val_loss: 0.2291 - val_auc: 0.7365\n",
            "Epoch 717/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0346 - auc: 0.9455 - val_loss: 0.2264 - val_auc: 0.7404\n",
            "Epoch 718/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0419 - auc: 0.9398 - val_loss: 0.2262 - val_auc: 0.7459\n",
            "Epoch 719/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0463 - auc: 0.9301 - val_loss: 0.2248 - val_auc: 0.7514\n",
            "Epoch 720/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0431 - auc: 0.9311 - val_loss: 0.2221 - val_auc: 0.7472\n",
            "Epoch 721/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0423 - auc: 0.9394 - val_loss: 0.2227 - val_auc: 0.7511\n",
            "Epoch 722/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0426 - auc: 0.9451 - val_loss: 0.2246 - val_auc: 0.7488\n",
            "Epoch 723/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0353 - auc: 0.9422 - val_loss: 0.2272 - val_auc: 0.7445\n",
            "Epoch 724/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0410 - auc: 0.9346 - val_loss: 0.2241 - val_auc: 0.7553\n",
            "Epoch 725/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0417 - auc: 0.9402 - val_loss: 0.2230 - val_auc: 0.7580\n",
            "Epoch 726/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0454 - auc: 0.9361 - val_loss: 0.2190 - val_auc: 0.7574\n",
            "Epoch 727/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0459 - auc: 0.9320 - val_loss: 0.2143 - val_auc: 0.7629\n",
            "Epoch 728/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0472 - auc: 0.9335 - val_loss: 0.2229 - val_auc: 0.7589\n",
            "Epoch 729/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0443 - auc: 0.9321 - val_loss: 0.2258 - val_auc: 0.7533\n",
            "Epoch 730/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0364 - auc: 0.9409 - val_loss: 0.2227 - val_auc: 0.7637\n",
            "Epoch 731/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0448 - auc: 0.9258 - val_loss: 0.2267 - val_auc: 0.7538\n",
            "Epoch 732/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0410 - auc: 0.9314 - val_loss: 0.2258 - val_auc: 0.7559\n",
            "Epoch 733/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0397 - auc: 0.9328 - val_loss: 0.2260 - val_auc: 0.7523\n",
            "Epoch 734/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0434 - auc: 0.9394 - val_loss: 0.2279 - val_auc: 0.7553\n",
            "Epoch 735/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0382 - auc: 0.9421 - val_loss: 0.2250 - val_auc: 0.7615\n",
            "Epoch 736/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0480 - auc: 0.9269 - val_loss: 0.2253 - val_auc: 0.7590\n",
            "Epoch 737/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0464 - auc: 0.9371 - val_loss: 0.2279 - val_auc: 0.7557\n",
            "Epoch 738/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0403 - auc: 0.9433 - val_loss: 0.2290 - val_auc: 0.7505\n",
            "Epoch 739/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0414 - auc: 0.9351 - val_loss: 0.2298 - val_auc: 0.7551\n",
            "Epoch 740/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0372 - auc: 0.9445 - val_loss: 0.2307 - val_auc: 0.7526\n",
            "Epoch 741/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0524 - auc: 0.9313 - val_loss: 0.2293 - val_auc: 0.7528\n",
            "Epoch 742/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0429 - auc: 0.9365 - val_loss: 0.2275 - val_auc: 0.7562\n",
            "Epoch 743/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0426 - auc: 0.9266 - val_loss: 0.2256 - val_auc: 0.7562\n",
            "Epoch 744/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0437 - auc: 0.9293 - val_loss: 0.2201 - val_auc: 0.7584\n",
            "Epoch 745/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0420 - auc: 0.9393 - val_loss: 0.2203 - val_auc: 0.7545\n",
            "Epoch 746/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0395 - auc: 0.9454 - val_loss: 0.2210 - val_auc: 0.7459\n",
            "Epoch 747/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0394 - auc: 0.9319 - val_loss: 0.2227 - val_auc: 0.7458\n",
            "Epoch 748/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0480 - auc: 0.9397 - val_loss: 0.2180 - val_auc: 0.7505\n",
            "Epoch 749/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0469 - auc: 0.9285 - val_loss: 0.2176 - val_auc: 0.7531\n",
            "Epoch 750/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0518 - auc: 0.9353 - val_loss: 0.2215 - val_auc: 0.7461\n",
            "Epoch 751/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0387 - auc: 0.9417 - val_loss: 0.2230 - val_auc: 0.7422\n",
            "Epoch 752/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0424 - auc: 0.9381 - val_loss: 0.2208 - val_auc: 0.7488\n",
            "Epoch 753/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0363 - auc: 0.9450 - val_loss: 0.2212 - val_auc: 0.7463\n",
            "Epoch 754/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0457 - auc: 0.9366 - val_loss: 0.2241 - val_auc: 0.7454\n",
            "Epoch 755/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0433 - auc: 0.9343 - val_loss: 0.2221 - val_auc: 0.7460\n",
            "Epoch 756/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0455 - auc: 0.9331 - val_loss: 0.2237 - val_auc: 0.7430\n",
            "Epoch 757/1000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.0426 - auc: 0.9414 - val_loss: 0.2276 - val_auc: 0.7492\n",
            "Epoch 758/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0354 - auc: 0.9326 - val_loss: 0.2266 - val_auc: 0.7471\n",
            "Epoch 759/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0448 - auc: 0.9274 - val_loss: 0.2226 - val_auc: 0.7505\n",
            "Epoch 760/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0504 - auc: 0.9376 - val_loss: 0.2219 - val_auc: 0.7457\n",
            "Epoch 761/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0385 - auc: 0.9428 - val_loss: 0.2206 - val_auc: 0.7483\n",
            "Epoch 762/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0392 - auc: 0.9480 - val_loss: 0.2247 - val_auc: 0.7398\n",
            "Epoch 763/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0448 - auc: 0.9436 - val_loss: 0.2278 - val_auc: 0.7370\n",
            "Epoch 764/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0483 - auc: 0.9346 - val_loss: 0.2282 - val_auc: 0.7442\n",
            "Epoch 765/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0436 - auc: 0.9295 - val_loss: 0.2278 - val_auc: 0.7403\n",
            "Epoch 766/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0423 - auc: 0.9320 - val_loss: 0.2275 - val_auc: 0.7433\n",
            "Epoch 767/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0472 - auc: 0.9309 - val_loss: 0.2270 - val_auc: 0.7457\n",
            "Epoch 768/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0503 - auc: 0.9358 - val_loss: 0.2262 - val_auc: 0.7452\n",
            "Epoch 769/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0343 - auc: 0.9537 - val_loss: 0.2278 - val_auc: 0.7412\n",
            "Epoch 770/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0456 - auc: 0.9321 - val_loss: 0.2286 - val_auc: 0.7442\n",
            "Epoch 771/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0398 - auc: 0.9377 - val_loss: 0.2279 - val_auc: 0.7447\n",
            "Epoch 772/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0412 - auc: 0.9383 - val_loss: 0.2281 - val_auc: 0.7442\n",
            "Epoch 773/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0432 - auc: 0.9417 - val_loss: 0.2300 - val_auc: 0.7431\n",
            "Epoch 774/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0418 - auc: 0.9500 - val_loss: 0.2262 - val_auc: 0.7426\n",
            "Epoch 775/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0392 - auc: 0.9431 - val_loss: 0.2261 - val_auc: 0.7397\n",
            "Epoch 776/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0339 - auc: 0.9549 - val_loss: 0.2250 - val_auc: 0.7363\n",
            "Epoch 777/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0416 - auc: 0.9341 - val_loss: 0.2251 - val_auc: 0.7350\n",
            "Epoch 778/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0397 - auc: 0.9407 - val_loss: 0.2242 - val_auc: 0.7393\n",
            "Epoch 779/1000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.0425 - auc: 0.9371 - val_loss: 0.2263 - val_auc: 0.7398\n",
            "Epoch 780/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0396 - auc: 0.9450 - val_loss: 0.2279 - val_auc: 0.7318\n",
            "Epoch 781/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0435 - auc: 0.9297 - val_loss: 0.2248 - val_auc: 0.7391\n",
            "Epoch 782/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0397 - auc: 0.9350 - val_loss: 0.2259 - val_auc: 0.7370\n",
            "Epoch 783/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0430 - auc: 0.9417 - val_loss: 0.2254 - val_auc: 0.7509\n",
            "Epoch 784/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0524 - auc: 0.9291 - val_loss: 0.2263 - val_auc: 0.7471\n",
            "Epoch 785/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0442 - auc: 0.9314 - val_loss: 0.2229 - val_auc: 0.7399\n",
            "Epoch 786/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0380 - auc: 0.9399 - val_loss: 0.2231 - val_auc: 0.7368\n",
            "Epoch 787/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0405 - auc: 0.9360 - val_loss: 0.2241 - val_auc: 0.7428\n",
            "Epoch 788/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0403 - auc: 0.9325 - val_loss: 0.2263 - val_auc: 0.7364\n",
            "Epoch 789/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0469 - auc: 0.9406 - val_loss: 0.2254 - val_auc: 0.7320\n",
            "Epoch 790/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0417 - auc: 0.9349 - val_loss: 0.2273 - val_auc: 0.7383\n",
            "Epoch 791/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0485 - auc: 0.9224 - val_loss: 0.2288 - val_auc: 0.7279\n",
            "Epoch 792/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0436 - auc: 0.9407 - val_loss: 0.2305 - val_auc: 0.7339\n",
            "Epoch 793/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0455 - auc: 0.9310 - val_loss: 0.2299 - val_auc: 0.7327\n",
            "Epoch 794/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0375 - auc: 0.9395 - val_loss: 0.2291 - val_auc: 0.7346\n",
            "Epoch 795/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0425 - auc: 0.9347 - val_loss: 0.2287 - val_auc: 0.7363\n",
            "Epoch 796/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0385 - auc: 0.9422 - val_loss: 0.2234 - val_auc: 0.7392\n",
            "Epoch 797/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0409 - auc: 0.9376 - val_loss: 0.2231 - val_auc: 0.7449\n",
            "Epoch 798/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0445 - auc: 0.9282 - val_loss: 0.2250 - val_auc: 0.7413\n",
            "Epoch 799/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0378 - auc: 0.9413 - val_loss: 0.2254 - val_auc: 0.7396\n",
            "Epoch 800/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0363 - auc: 0.9414 - val_loss: 0.2271 - val_auc: 0.7389\n",
            "Epoch 801/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0415 - auc: 0.9395 - val_loss: 0.2264 - val_auc: 0.7431\n",
            "Epoch 802/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0450 - auc: 0.9392 - val_loss: 0.2240 - val_auc: 0.7434\n",
            "Epoch 803/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0366 - auc: 0.9495 - val_loss: 0.2292 - val_auc: 0.7373\n",
            "Epoch 804/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0397 - auc: 0.9395 - val_loss: 0.2336 - val_auc: 0.7360\n",
            "Epoch 805/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0353 - auc: 0.9440 - val_loss: 0.2332 - val_auc: 0.7344\n",
            "Epoch 806/1000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.0464 - auc: 0.9456 - val_loss: 0.2310 - val_auc: 0.7342\n",
            "Epoch 807/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0443 - auc: 0.9367 - val_loss: 0.2275 - val_auc: 0.7431\n",
            "Epoch 808/1000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.0475 - auc: 0.9341 - val_loss: 0.2272 - val_auc: 0.7416\n",
            "Epoch 809/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0362 - auc: 0.9468 - val_loss: 0.2280 - val_auc: 0.7382\n",
            "Epoch 810/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0389 - auc: 0.9520 - val_loss: 0.2281 - val_auc: 0.7354\n",
            "Epoch 811/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0442 - auc: 0.9369 - val_loss: 0.2271 - val_auc: 0.7498\n",
            "Epoch 812/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0336 - auc: 0.9391 - val_loss: 0.2250 - val_auc: 0.7528\n",
            "Epoch 813/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0412 - auc: 0.9440 - val_loss: 0.2264 - val_auc: 0.7478\n",
            "Epoch 814/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0470 - auc: 0.9381 - val_loss: 0.2245 - val_auc: 0.7438\n",
            "Epoch 815/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0400 - auc: 0.9467 - val_loss: 0.2246 - val_auc: 0.7343\n",
            "Epoch 816/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0434 - auc: 0.9377 - val_loss: 0.2220 - val_auc: 0.7493\n",
            "Epoch 817/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0458 - auc: 0.9345 - val_loss: 0.2199 - val_auc: 0.7444\n",
            "Epoch 818/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0411 - auc: 0.9388 - val_loss: 0.2215 - val_auc: 0.7435\n",
            "Epoch 819/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0436 - auc: 0.9405 - val_loss: 0.2225 - val_auc: 0.7485\n",
            "Epoch 820/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0409 - auc: 0.9426 - val_loss: 0.2221 - val_auc: 0.7501\n",
            "Epoch 821/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0487 - auc: 0.9345 - val_loss: 0.2247 - val_auc: 0.7541\n",
            "Epoch 822/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0358 - auc: 0.9362 - val_loss: 0.2287 - val_auc: 0.7550\n",
            "Epoch 823/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0371 - auc: 0.9453 - val_loss: 0.2261 - val_auc: 0.7536\n",
            "Epoch 824/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0394 - auc: 0.9408 - val_loss: 0.2254 - val_auc: 0.7526\n",
            "Epoch 825/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0419 - auc: 0.9432 - val_loss: 0.2252 - val_auc: 0.7402\n",
            "Epoch 826/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0418 - auc: 0.9308 - val_loss: 0.2255 - val_auc: 0.7380\n",
            "Epoch 827/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0380 - auc: 0.9444 - val_loss: 0.2237 - val_auc: 0.7465\n",
            "Epoch 828/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0392 - auc: 0.9399 - val_loss: 0.2258 - val_auc: 0.7416\n",
            "Epoch 829/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0466 - auc: 0.9322 - val_loss: 0.2246 - val_auc: 0.7440\n",
            "Epoch 830/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0456 - auc: 0.9325 - val_loss: 0.2261 - val_auc: 0.7430\n",
            "Epoch 831/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0395 - auc: 0.9391 - val_loss: 0.2270 - val_auc: 0.7491\n",
            "Epoch 832/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0422 - auc: 0.9296 - val_loss: 0.2254 - val_auc: 0.7475\n",
            "Epoch 833/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0525 - auc: 0.9258 - val_loss: 0.2257 - val_auc: 0.7469\n",
            "Epoch 834/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0432 - auc: 0.9329 - val_loss: 0.2254 - val_auc: 0.7499\n",
            "Epoch 835/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0419 - auc: 0.9356 - val_loss: 0.2249 - val_auc: 0.7472\n",
            "Epoch 836/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0456 - auc: 0.9362 - val_loss: 0.2230 - val_auc: 0.7482\n",
            "Epoch 837/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0430 - auc: 0.9262 - val_loss: 0.2247 - val_auc: 0.7490\n",
            "Epoch 838/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0401 - auc: 0.9389 - val_loss: 0.2232 - val_auc: 0.7494\n",
            "Epoch 839/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0422 - auc: 0.9296 - val_loss: 0.2231 - val_auc: 0.7453\n",
            "Epoch 840/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0358 - auc: 0.9361 - val_loss: 0.2251 - val_auc: 0.7477\n",
            "Epoch 841/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0399 - auc: 0.9324 - val_loss: 0.2261 - val_auc: 0.7493\n",
            "Epoch 842/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0393 - auc: 0.9330 - val_loss: 0.2276 - val_auc: 0.7402\n",
            "Epoch 843/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0445 - auc: 0.9356 - val_loss: 0.2265 - val_auc: 0.7405\n",
            "Epoch 844/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0419 - auc: 0.9401 - val_loss: 0.2275 - val_auc: 0.7375\n",
            "Epoch 845/1000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.0401 - auc: 0.9392 - val_loss: 0.2285 - val_auc: 0.7347\n",
            "Epoch 846/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0386 - auc: 0.9462 - val_loss: 0.2292 - val_auc: 0.7373\n",
            "Epoch 847/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0488 - auc: 0.9255 - val_loss: 0.2308 - val_auc: 0.7361\n",
            "Epoch 848/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0430 - auc: 0.9330 - val_loss: 0.2306 - val_auc: 0.7342\n",
            "Epoch 849/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0371 - auc: 0.9425 - val_loss: 0.2271 - val_auc: 0.7373\n",
            "Epoch 850/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0485 - auc: 0.9378 - val_loss: 0.2268 - val_auc: 0.7359\n",
            "Epoch 851/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0371 - auc: 0.9474 - val_loss: 0.2272 - val_auc: 0.7308\n",
            "Epoch 852/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0398 - auc: 0.9444 - val_loss: 0.2292 - val_auc: 0.7311\n",
            "Epoch 853/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0429 - auc: 0.9303 - val_loss: 0.2320 - val_auc: 0.7339\n",
            "Epoch 854/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0439 - auc: 0.9351 - val_loss: 0.2311 - val_auc: 0.7400\n",
            "Epoch 855/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0400 - auc: 0.9323 - val_loss: 0.2308 - val_auc: 0.7397\n",
            "Epoch 856/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0422 - auc: 0.9382 - val_loss: 0.2301 - val_auc: 0.7319\n",
            "Epoch 857/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0473 - auc: 0.9338 - val_loss: 0.2310 - val_auc: 0.7322\n",
            "Epoch 858/1000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.0401 - auc: 0.9342 - val_loss: 0.2309 - val_auc: 0.7286\n",
            "Epoch 859/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0419 - auc: 0.9426 - val_loss: 0.2317 - val_auc: 0.7357\n",
            "Epoch 860/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0431 - auc: 0.9448 - val_loss: 0.2314 - val_auc: 0.7315\n",
            "Epoch 861/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0383 - auc: 0.9500 - val_loss: 0.2303 - val_auc: 0.7401\n",
            "Epoch 862/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0445 - auc: 0.9409 - val_loss: 0.2326 - val_auc: 0.7324\n",
            "Epoch 863/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0417 - auc: 0.9396 - val_loss: 0.2353 - val_auc: 0.7333\n",
            "Epoch 864/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0427 - auc: 0.9383 - val_loss: 0.2336 - val_auc: 0.7346\n",
            "Epoch 865/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0467 - auc: 0.9441 - val_loss: 0.2330 - val_auc: 0.7389\n",
            "Epoch 866/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0370 - auc: 0.9415 - val_loss: 0.2316 - val_auc: 0.7296\n",
            "Epoch 867/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0391 - auc: 0.9453 - val_loss: 0.2293 - val_auc: 0.7426\n",
            "Epoch 868/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0474 - auc: 0.9363 - val_loss: 0.2292 - val_auc: 0.7444\n",
            "Epoch 869/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0439 - auc: 0.9374 - val_loss: 0.2303 - val_auc: 0.7434\n",
            "Epoch 870/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0341 - auc: 0.9533 - val_loss: 0.2304 - val_auc: 0.7461\n",
            "Epoch 871/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0365 - auc: 0.9434 - val_loss: 0.2309 - val_auc: 0.7386\n",
            "Epoch 872/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0469 - auc: 0.9359 - val_loss: 0.2355 - val_auc: 0.7342\n",
            "Epoch 873/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0494 - auc: 0.9469 - val_loss: 0.2311 - val_auc: 0.7451\n",
            "Epoch 874/1000\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0437 - auc: 0.9409 - val_loss: 0.2291 - val_auc: 0.7441\n",
            "Epoch 875/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0390 - auc: 0.9351 - val_loss: 0.2282 - val_auc: 0.7480\n",
            "Epoch 876/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0419 - auc: 0.9271 - val_loss: 0.2283 - val_auc: 0.7459\n",
            "Epoch 877/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0503 - auc: 0.9352 - val_loss: 0.2282 - val_auc: 0.7422\n",
            "Epoch 878/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0365 - auc: 0.9346 - val_loss: 0.2304 - val_auc: 0.7393\n",
            "Epoch 879/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0397 - auc: 0.9433 - val_loss: 0.2291 - val_auc: 0.7422\n",
            "Epoch 880/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0395 - auc: 0.9345 - val_loss: 0.2299 - val_auc: 0.7490\n",
            "Epoch 881/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0429 - auc: 0.9369 - val_loss: 0.2323 - val_auc: 0.7439\n",
            "Epoch 882/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0495 - auc: 0.9322 - val_loss: 0.2315 - val_auc: 0.7360\n",
            "Epoch 883/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0437 - auc: 0.9418 - val_loss: 0.2275 - val_auc: 0.7463\n",
            "Epoch 884/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0380 - auc: 0.9436 - val_loss: 0.2271 - val_auc: 0.7479\n",
            "Epoch 885/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0433 - auc: 0.9350 - val_loss: 0.2277 - val_auc: 0.7413\n",
            "Epoch 886/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0445 - auc: 0.9198 - val_loss: 0.2301 - val_auc: 0.7463\n",
            "Epoch 887/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0471 - auc: 0.9297 - val_loss: 0.2308 - val_auc: 0.7452\n",
            "Epoch 888/1000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.0469 - auc: 0.9382 - val_loss: 0.2302 - val_auc: 0.7445\n",
            "Epoch 889/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0396 - auc: 0.9402 - val_loss: 0.2301 - val_auc: 0.7432\n",
            "Epoch 890/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0415 - auc: 0.9432 - val_loss: 0.2290 - val_auc: 0.7421\n",
            "Epoch 891/1000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.0413 - auc: 0.9330 - val_loss: 0.2308 - val_auc: 0.7511\n",
            "Epoch 892/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0444 - auc: 0.9396 - val_loss: 0.2308 - val_auc: 0.7475\n",
            "Epoch 893/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0410 - auc: 0.9384 - val_loss: 0.2297 - val_auc: 0.7433\n",
            "Epoch 894/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0372 - auc: 0.9442 - val_loss: 0.2291 - val_auc: 0.7436\n",
            "Epoch 895/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0436 - auc: 0.9428 - val_loss: 0.2281 - val_auc: 0.7458\n",
            "Epoch 896/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0385 - auc: 0.9423 - val_loss: 0.2257 - val_auc: 0.7452\n",
            "Epoch 897/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0483 - auc: 0.9287 - val_loss: 0.2240 - val_auc: 0.7431\n",
            "Epoch 898/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0418 - auc: 0.9413 - val_loss: 0.2224 - val_auc: 0.7491\n",
            "Epoch 899/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0377 - auc: 0.9422 - val_loss: 0.2236 - val_auc: 0.7439\n",
            "Epoch 900/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0462 - auc: 0.9335 - val_loss: 0.2225 - val_auc: 0.7477\n",
            "Epoch 901/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0373 - auc: 0.9369 - val_loss: 0.2226 - val_auc: 0.7473\n",
            "Epoch 902/1000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.0393 - auc: 0.9537 - val_loss: 0.2209 - val_auc: 0.7482\n",
            "Epoch 903/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0451 - auc: 0.9401 - val_loss: 0.2202 - val_auc: 0.7524\n",
            "Epoch 904/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0416 - auc: 0.9381 - val_loss: 0.2222 - val_auc: 0.7485\n",
            "Epoch 905/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0422 - auc: 0.9374 - val_loss: 0.2238 - val_auc: 0.7441\n",
            "Epoch 906/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0423 - auc: 0.9385 - val_loss: 0.2228 - val_auc: 0.7498\n",
            "Epoch 907/1000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.0382 - auc: 0.9321 - val_loss: 0.2228 - val_auc: 0.7569\n",
            "Epoch 908/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0374 - auc: 0.9351 - val_loss: 0.2246 - val_auc: 0.7460\n",
            "Epoch 909/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0424 - auc: 0.9361 - val_loss: 0.2234 - val_auc: 0.7526\n",
            "Epoch 910/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0413 - auc: 0.9438 - val_loss: 0.2245 - val_auc: 0.7535\n",
            "Epoch 911/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0371 - auc: 0.9438 - val_loss: 0.2246 - val_auc: 0.7541\n",
            "Epoch 912/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0432 - auc: 0.9350 - val_loss: 0.2252 - val_auc: 0.7467\n",
            "Epoch 913/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0426 - auc: 0.9429 - val_loss: 0.2278 - val_auc: 0.7506\n",
            "Epoch 914/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0380 - auc: 0.9420 - val_loss: 0.2269 - val_auc: 0.7538\n",
            "Epoch 915/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0327 - auc: 0.9440 - val_loss: 0.2238 - val_auc: 0.7514\n",
            "Epoch 916/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0393 - auc: 0.9463 - val_loss: 0.2243 - val_auc: 0.7507\n",
            "Epoch 917/1000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.0435 - auc: 0.9403 - val_loss: 0.2264 - val_auc: 0.7515\n",
            "Epoch 918/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0345 - auc: 0.9404 - val_loss: 0.2214 - val_auc: 0.7535\n",
            "Epoch 919/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0385 - auc: 0.9419 - val_loss: 0.2179 - val_auc: 0.7500\n",
            "Epoch 920/1000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.0413 - auc: 0.9370 - val_loss: 0.2231 - val_auc: 0.7418\n",
            "Epoch 921/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0421 - auc: 0.9397 - val_loss: 0.2230 - val_auc: 0.7513\n",
            "Epoch 922/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0330 - auc: 0.9511 - val_loss: 0.2262 - val_auc: 0.7449\n",
            "Epoch 923/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0421 - auc: 0.9349 - val_loss: 0.2271 - val_auc: 0.7474\n",
            "Epoch 924/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0428 - auc: 0.9293 - val_loss: 0.2278 - val_auc: 0.7467\n",
            "Epoch 925/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0345 - auc: 0.9382 - val_loss: 0.2275 - val_auc: 0.7495\n",
            "Epoch 926/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0423 - auc: 0.9412 - val_loss: 0.2269 - val_auc: 0.7511\n",
            "Epoch 927/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0366 - auc: 0.9460 - val_loss: 0.2254 - val_auc: 0.7513\n",
            "Epoch 928/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0360 - auc: 0.9459 - val_loss: 0.2262 - val_auc: 0.7465\n",
            "Epoch 929/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0333 - auc: 0.9414 - val_loss: 0.2282 - val_auc: 0.7479\n",
            "Epoch 930/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0412 - auc: 0.9386 - val_loss: 0.2248 - val_auc: 0.7526\n",
            "Epoch 931/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0413 - auc: 0.9337 - val_loss: 0.2257 - val_auc: 0.7496\n",
            "Epoch 932/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0391 - auc: 0.9409 - val_loss: 0.2234 - val_auc: 0.7507\n",
            "Epoch 933/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0453 - auc: 0.9370 - val_loss: 0.2219 - val_auc: 0.7485\n",
            "Epoch 934/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0468 - auc: 0.9335 - val_loss: 0.2220 - val_auc: 0.7469\n",
            "Epoch 935/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0376 - auc: 0.9426 - val_loss: 0.2225 - val_auc: 0.7577\n",
            "Epoch 936/1000\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.0351 - auc: 0.9426 - val_loss: 0.2270 - val_auc: 0.7545\n",
            "Epoch 937/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0436 - auc: 0.9340 - val_loss: 0.2258 - val_auc: 0.7555\n",
            "Epoch 938/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0418 - auc: 0.9337 - val_loss: 0.2236 - val_auc: 0.7586\n",
            "Epoch 939/1000\n",
            "28/28 [==============================] - 0s 13ms/step - loss: 0.0377 - auc: 0.9422 - val_loss: 0.2231 - val_auc: 0.7582\n",
            "Epoch 940/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0411 - auc: 0.9471 - val_loss: 0.2216 - val_auc: 0.7571\n",
            "Epoch 941/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0452 - auc: 0.9365 - val_loss: 0.2216 - val_auc: 0.7540\n",
            "Epoch 942/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0436 - auc: 0.9375 - val_loss: 0.2203 - val_auc: 0.7598\n",
            "Epoch 943/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0396 - auc: 0.9507 - val_loss: 0.2198 - val_auc: 0.7560\n",
            "Epoch 944/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0449 - auc: 0.9281 - val_loss: 0.2198 - val_auc: 0.7538\n",
            "Epoch 945/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0342 - auc: 0.9448 - val_loss: 0.2220 - val_auc: 0.7540\n",
            "Epoch 946/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0453 - auc: 0.9240 - val_loss: 0.2197 - val_auc: 0.7635\n",
            "Epoch 947/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0441 - auc: 0.9392 - val_loss: 0.2218 - val_auc: 0.7611\n",
            "Epoch 948/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0339 - auc: 0.9427 - val_loss: 0.2218 - val_auc: 0.7539\n",
            "Epoch 949/1000\n",
            "28/28 [==============================] - 0s 11ms/step - loss: 0.0446 - auc: 0.9430 - val_loss: 0.2215 - val_auc: 0.7551\n",
            "Epoch 950/1000\n",
            "28/28 [==============================] - 0s 12ms/step - loss: 0.0423 - auc: 0.9350 - val_loss: 0.2204 - val_auc: 0.7578\n",
            "Epoch 951/1000\n",
            " 1/28 [>.............................] - ETA: 0s - loss: 0.0133 - auc: 1.0000"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-6f709949a662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m         )\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, callbacks=EarlyStopping(monitor='val_loss', patience=25), batch_size=25) #batch_size=75,monitor='val_loss'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mtrain_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "dropout_rate = 0.1# 0.2 #from grid\n",
        "learning_rate = 0.001 #0.001 #from grid\n",
        "recurrent_rate = 0.#0.2\n",
        "\n",
        "\n",
        "# make stratified KFOLD \n",
        "n_split=10\n",
        "\n",
        "skf = StratifiedKFold(n_splits=n_split, shuffle=False)\n",
        "mean = 0\n",
        "accuracy = []\n",
        "\n",
        "\n",
        "for train_index, test_index in skf.split(df_all, y):\n",
        "    Xtrain = df_all[train_index]\n",
        "    ytrain = y[train_index]\n",
        "    Xtest = df_all[test_index]\n",
        "    ytest = y[test_index]\n",
        "\n",
        "    inputs = Input(shape=(Xtrain.shape[1], Xtrain.shape[2]))\n",
        "\n",
        "    model = Sequential()\n",
        "    x= layers.BatchNormalization(axis=-1,\n",
        "                                 momentum=0.99,\n",
        "                                 epsilon=0.0001)(inputs)\n",
        "\n",
        "    x = layers.LSTM(10, #Bidirectional\n",
        "                    activation =\"tanh\",\n",
        "                    recurrent_activation = \"sigmoid\",\n",
        "                    return_sequences=True, \n",
        "                    dropout=dropout_rate, \n",
        "                    recurrent_dropout=recurrent_rate,\n",
        "                    input_shape=(None, Xtrain.shape[1], Xtrain.shape[2]))(x)\n",
        "    \n",
        "    x = layers.LSTM(7, \n",
        "                    activation =\"tanh\",\n",
        "                    recurrent_activation = \"sigmoid\",\n",
        "                    dropout=dropout_rate, \n",
        "                    recurrent_dropout=recurrent_rate,\n",
        "                    return_sequences=False)(x)\n",
        "\n",
        "    x = layers.Dense(15,activation = \"softmax\")(x)#softmax\n",
        "    #x = layers.Dropout(dropout_rate) (x)\n",
        "\n",
        "    x = layers.Dense(5, activation = \"sigmoid\")(x)\n",
        "    #x = layers.Dropout(dropout_rate) (x)\n",
        "\n",
        "    outputs = layers.Dense(1, activation = \"sigmoid\")(x)  \n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"LSTM_clinical_data\")\n",
        "\n",
        "    # define how to compile\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\"),\n",
        "        #loss=tf.keras.losses.KLDivergence(reduction=\"auto\", name=\"kl_divergence\"),\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        metrics=[\"AUC\"], #,\"Recall\",\"accuracy\"\n",
        "        )\n",
        "    \n",
        "    history = model.fit(Xtrain, ytrain, epochs=1000, validation_split=0.20, verbose=1)#, callbacks=EarlyStopping(monitor='val_loss', patience=25), batch_size=25) #batch_size=75,monitor='val_loss'\n",
        "\n",
        "    train_scores = model.evaluate(Xtrain, ytrain, verbose=2)\n",
        "    test_scores = model.evaluate(Xtest, ytest, verbose=2)\n",
        "    print(\"Train loss:\", train_scores[0])\n",
        "    print(\"Train accuracy:\", train_scores[1])\n",
        "    print(\"Test loss:\", test_scores[0])\n",
        "    print(\"Test accuracy:\", test_scores[1])\n",
        "    accuracy.append(test_scores[1])\n",
        "    \n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title(\"Model Loss\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.legend([\"Train\",\"Test\"],loc=\"best\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"Mean acc: \", np.mean(accuracy))\n",
        "#create 95% confidence interval for population mean weight\n",
        "print(\"Confidence interval: \", get_standard_diviation(accuracy) )\n",
        "print(\"All: \", accuracy )"
      ],
      "id": "7pTGC0qVNYWQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8bfda3a"
      },
      "outputs": [],
      "source": [
        "accuracy"
      ],
      "id": "b8bfda3a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUtwMjdEpL4Z"
      },
      "source": [
        "# Image model "
      ],
      "id": "vUtwMjdEpL4Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "961rPd8epLiT",
        "outputId": "599d8d63-d2d6-4efb-f6ff-d5e15675dcac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/1000\n",
            "36/36 [==============================] - 12s 91ms/step - loss: 0.6990 - auc: 0.5822 - val_loss: 0.9815 - val_auc: 0.8067\n",
            "Epoch 2/1000\n",
            "36/36 [==============================] - 2s 56ms/step - loss: 0.6430 - auc: 0.6114 - val_loss: 0.7361 - val_auc: 0.8110\n",
            "Epoch 3/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.6141 - auc: 0.6213 - val_loss: 0.7847 - val_auc: 0.8031\n",
            "Epoch 4/1000\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.5852 - auc: 0.6857 - val_loss: 0.7764 - val_auc: 0.8068\n",
            "Epoch 5/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.5704 - auc: 0.6440 - val_loss: 0.7092 - val_auc: 0.8094\n",
            "Epoch 6/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.5397 - auc: 0.7102 - val_loss: 0.8801 - val_auc: 0.8057\n",
            "Epoch 7/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.5355 - auc: 0.6988 - val_loss: 0.7161 - val_auc: 0.7947\n",
            "Epoch 8/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.5108 - auc: 0.7333 - val_loss: 0.7446 - val_auc: 0.7902\n",
            "Epoch 9/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.5015 - auc: 0.7318 - val_loss: 0.7001 - val_auc: 0.7876\n",
            "Epoch 10/1000\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.5068 - auc: 0.7229 - val_loss: 0.7203 - val_auc: 0.7893\n",
            "Epoch 11/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.5053 - auc: 0.7132 - val_loss: 0.8098 - val_auc: 0.7462\n",
            "Epoch 12/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4828 - auc: 0.7441 - val_loss: 0.7769 - val_auc: 0.7913\n",
            "Epoch 13/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4805 - auc: 0.7304 - val_loss: 0.6984 - val_auc: 0.7839\n",
            "Epoch 14/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4723 - auc: 0.7433 - val_loss: 0.6582 - val_auc: 0.7863\n",
            "Epoch 15/1000\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.4638 - auc: 0.7569 - val_loss: 0.7090 - val_auc: 0.7874\n",
            "Epoch 16/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4671 - auc: 0.7565 - val_loss: 0.7159 - val_auc: 0.7832\n",
            "Epoch 17/1000\n",
            "36/36 [==============================] - 2s 56ms/step - loss: 0.4485 - auc: 0.7825 - val_loss: 0.7547 - val_auc: 0.7903\n",
            "Epoch 18/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4497 - auc: 0.7677 - val_loss: 0.7012 - val_auc: 0.7682\n",
            "Epoch 19/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4423 - auc: 0.7690 - val_loss: 0.6635 - val_auc: 0.7884\n",
            "Epoch 20/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4488 - auc: 0.7564 - val_loss: 0.7210 - val_auc: 0.7786\n",
            "Epoch 21/1000\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.4495 - auc: 0.7803 - val_loss: 0.7473 - val_auc: 0.7817\n",
            "Epoch 22/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4389 - auc: 0.7834 - val_loss: 0.6820 - val_auc: 0.7847\n",
            "Epoch 23/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4413 - auc: 0.7628 - val_loss: 0.6989 - val_auc: 0.7847\n",
            "Epoch 24/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4486 - auc: 0.7639 - val_loss: 0.6936 - val_auc: 0.7862\n",
            "Epoch 25/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.4340 - auc: 0.7873 - val_loss: 0.6982 - val_auc: 0.7741\n",
            "Epoch 26/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4253 - auc: 0.8006 - val_loss: 0.7038 - val_auc: 0.7817\n",
            "Epoch 27/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4319 - auc: 0.7788 - val_loss: 0.7714 - val_auc: 0.7709\n",
            "Epoch 28/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.4334 - auc: 0.7712 - val_loss: 0.6825 - val_auc: 0.7849\n",
            "Epoch 29/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4332 - auc: 0.7495 - val_loss: 0.6846 - val_auc: 0.7852\n",
            "35/35 - 0s - loss: 0.4466 - auc: 0.8407 - 244ms/epoch - 7ms/step\n",
            "4/4 - 0s - loss: 0.3830 - auc: 0.9089 - 45ms/epoch - 11ms/step\n",
            "Train loss: 0.44662460684776306\n",
            "Train accuracy: 0.840678870677948\n",
            "Test loss: 0.38296353816986084\n",
            "Test accuracy: 0.9088541865348816\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/1000\n",
            "36/36 [==============================] - 11s 89ms/step - loss: 0.7565 - auc: 0.4908 - val_loss: 1.0109 - val_auc: 0.7628\n",
            "Epoch 2/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.6931 - auc: 0.4966 - val_loss: 0.9926 - val_auc: 0.8094\n",
            "Epoch 3/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.6256 - auc: 0.5777 - val_loss: 0.8674 - val_auc: 0.8009\n",
            "Epoch 4/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.5775 - auc: 0.6624 - val_loss: 0.7931 - val_auc: 0.8033\n",
            "Epoch 5/1000\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.5359 - auc: 0.7115 - val_loss: 0.7552 - val_auc: 0.7986\n",
            "Epoch 6/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.5335 - auc: 0.7135 - val_loss: 0.7227 - val_auc: 0.8089\n",
            "Epoch 7/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.5208 - auc: 0.7051 - val_loss: 0.7902 - val_auc: 0.8042\n",
            "Epoch 8/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.5062 - auc: 0.7298 - val_loss: 0.7301 - val_auc: 0.8047\n",
            "Epoch 9/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4870 - auc: 0.7552 - val_loss: 0.6546 - val_auc: 0.8052\n",
            "Epoch 10/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4856 - auc: 0.7574 - val_loss: 0.7621 - val_auc: 0.8021\n",
            "Epoch 11/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4792 - auc: 0.7457 - val_loss: 0.7960 - val_auc: 0.7951\n",
            "Epoch 12/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.4646 - auc: 0.7644 - val_loss: 0.6539 - val_auc: 0.7959\n",
            "Epoch 13/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4725 - auc: 0.7341 - val_loss: 0.6759 - val_auc: 0.7960\n",
            "Epoch 14/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4647 - auc: 0.7532 - val_loss: 0.6869 - val_auc: 0.7937\n",
            "Epoch 15/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4591 - auc: 0.7546 - val_loss: 0.7091 - val_auc: 0.7914\n",
            "Epoch 16/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4529 - auc: 0.7608 - val_loss: 0.6501 - val_auc: 0.7896\n",
            "Epoch 17/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4296 - auc: 0.8001 - val_loss: 0.7619 - val_auc: 0.7933\n",
            "Epoch 18/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4372 - auc: 0.7795 - val_loss: 0.6516 - val_auc: 0.7980\n",
            "Epoch 19/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4380 - auc: 0.7823 - val_loss: 0.6680 - val_auc: 0.7946\n",
            "Epoch 20/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4249 - auc: 0.7993 - val_loss: 0.7502 - val_auc: 0.7930\n",
            "Epoch 21/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4317 - auc: 0.7850 - val_loss: 0.6808 - val_auc: 0.7950\n",
            "Epoch 22/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4294 - auc: 0.7763 - val_loss: 0.6146 - val_auc: 0.7975\n",
            "Epoch 23/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.4268 - auc: 0.7869 - val_loss: 0.6223 - val_auc: 0.8014\n",
            "Epoch 24/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4302 - auc: 0.7826 - val_loss: 0.7427 - val_auc: 0.7990\n",
            "Epoch 25/1000\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.4242 - auc: 0.7961 - val_loss: 0.6739 - val_auc: 0.7995\n",
            "Epoch 26/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4200 - auc: 0.7887 - val_loss: 0.6933 - val_auc: 0.7893\n",
            "Epoch 27/1000\n",
            "36/36 [==============================] - 2s 56ms/step - loss: 0.4050 - auc: 0.8046 - val_loss: 0.6406 - val_auc: 0.7940\n",
            "Epoch 28/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4130 - auc: 0.7955 - val_loss: 0.6809 - val_auc: 0.7934\n",
            "Epoch 29/1000\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.4120 - auc: 0.8001 - val_loss: 0.7294 - val_auc: 0.7948\n",
            "Epoch 30/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4124 - auc: 0.7938 - val_loss: 0.6693 - val_auc: 0.7955\n",
            "Epoch 31/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4245 - auc: 0.7852 - val_loss: 0.6751 - val_auc: 0.7952\n",
            "Epoch 32/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4295 - auc: 0.7726 - val_loss: 0.6639 - val_auc: 0.8029\n",
            "Epoch 33/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4083 - auc: 0.7961 - val_loss: 0.6328 - val_auc: 0.8002\n",
            "Epoch 34/1000\n",
            "36/36 [==============================] - 2s 55ms/step - loss: 0.4007 - auc: 0.8045 - val_loss: 0.6515 - val_auc: 0.7967\n",
            "Epoch 35/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4193 - auc: 0.7846 - val_loss: 0.6569 - val_auc: 0.7989\n",
            "Epoch 36/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.4040 - auc: 0.8060 - val_loss: 0.7376 - val_auc: 0.7967\n",
            "Epoch 37/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4078 - auc: 0.7857 - val_loss: 0.6438 - val_auc: 0.7897\n",
            "35/35 - 0s - loss: 0.4257 - auc: 0.8448 - 250ms/epoch - 7ms/step\n",
            "4/4 - 0s - loss: 0.3900 - auc: 0.8997 - 43ms/epoch - 11ms/step\n",
            "Train loss: 0.4257133901119232\n",
            "Train accuracy: 0.8448166251182556\n",
            "Test loss: 0.3900173604488373\n",
            "Test accuracy: 0.8997395038604736\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/1000\n",
            "36/36 [==============================] - 12s 89ms/step - loss: 0.7309 - auc: 0.5370 - val_loss: 1.0662 - val_auc: 0.8027\n",
            "Epoch 2/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.6718 - auc: 0.5841 - val_loss: 0.9213 - val_auc: 0.8158\n",
            "Epoch 3/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.6177 - auc: 0.6460 - val_loss: 0.8221 - val_auc: 0.8152\n",
            "Epoch 4/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.5658 - auc: 0.7240 - val_loss: 0.7303 - val_auc: 0.8140\n",
            "Epoch 5/1000\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.5437 - auc: 0.7433 - val_loss: 0.6864 - val_auc: 0.8060\n",
            "Epoch 6/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.5432 - auc: 0.7275 - val_loss: 0.7131 - val_auc: 0.8051\n",
            "Epoch 7/1000\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.4984 - auc: 0.7779 - val_loss: 0.7553 - val_auc: 0.8045\n",
            "Epoch 8/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.5177 - auc: 0.7292 - val_loss: 0.7297 - val_auc: 0.7987\n",
            "Epoch 9/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.5037 - auc: 0.7552 - val_loss: 0.6976 - val_auc: 0.8030\n",
            "Epoch 10/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4805 - auc: 0.7745 - val_loss: 0.8345 - val_auc: 0.7228\n",
            "Epoch 11/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4712 - auc: 0.7769 - val_loss: 0.7242 - val_auc: 0.7971\n",
            "Epoch 12/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4702 - auc: 0.7735 - val_loss: 0.7568 - val_auc: 0.7857\n",
            "Epoch 13/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4575 - auc: 0.7909 - val_loss: 0.7339 - val_auc: 0.7933\n",
            "Epoch 14/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4677 - auc: 0.7649 - val_loss: 0.7414 - val_auc: 0.7947\n",
            "Epoch 15/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4490 - auc: 0.7925 - val_loss: 0.7321 - val_auc: 0.7915\n",
            "Epoch 16/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4658 - auc: 0.7659 - val_loss: 0.8704 - val_auc: 0.7678\n",
            "Epoch 17/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4617 - auc: 0.7630 - val_loss: 0.7513 - val_auc: 0.7870\n",
            "Epoch 18/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4322 - auc: 0.8000 - val_loss: 0.7524 - val_auc: 0.7818\n",
            "Epoch 19/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4381 - auc: 0.7918 - val_loss: 0.6592 - val_auc: 0.7936\n",
            "Epoch 20/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4390 - auc: 0.7928 - val_loss: 0.6969 - val_auc: 0.7842\n",
            "Epoch 21/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.4447 - auc: 0.7807 - val_loss: 0.6632 - val_auc: 0.7887\n",
            "Epoch 22/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.4392 - auc: 0.7929 - val_loss: 0.6886 - val_auc: 0.7944\n",
            "Epoch 23/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4194 - auc: 0.8206 - val_loss: 0.7186 - val_auc: 0.7930\n",
            "Epoch 24/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4198 - auc: 0.8058 - val_loss: 0.7062 - val_auc: 0.7842\n",
            "Epoch 25/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.4185 - auc: 0.8117 - val_loss: 0.6862 - val_auc: 0.7854\n",
            "Epoch 26/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4151 - auc: 0.8172 - val_loss: 0.7815 - val_auc: 0.7652\n",
            "Epoch 27/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.4184 - auc: 0.8109 - val_loss: 0.6163 - val_auc: 0.7855\n",
            "Epoch 28/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4209 - auc: 0.8057 - val_loss: 0.6662 - val_auc: 0.7819\n",
            "Epoch 29/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4039 - auc: 0.8204 - val_loss: 0.6509 - val_auc: 0.7836\n",
            "Epoch 30/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4088 - auc: 0.8197 - val_loss: 0.6939 - val_auc: 0.7870\n",
            "Epoch 31/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4060 - auc: 0.8106 - val_loss: 0.6999 - val_auc: 0.7886\n",
            "Epoch 32/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4071 - auc: 0.8083 - val_loss: 0.6255 - val_auc: 0.7859\n",
            "Epoch 33/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4011 - auc: 0.8264 - val_loss: 0.7471 - val_auc: 0.7832\n",
            "Epoch 34/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.3909 - auc: 0.8337 - val_loss: 0.6596 - val_auc: 0.7883\n",
            "Epoch 35/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4037 - auc: 0.8107 - val_loss: 0.6434 - val_auc: 0.7859\n",
            "Epoch 36/1000\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.3936 - auc: 0.8314 - val_loss: 0.6799 - val_auc: 0.7854\n",
            "Epoch 37/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4014 - auc: 0.8228 - val_loss: 0.6755 - val_auc: 0.7866\n",
            "Epoch 38/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.3950 - auc: 0.8224 - val_loss: 0.6160 - val_auc: 0.7925\n",
            "Epoch 39/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.3880 - auc: 0.8321 - val_loss: 0.6369 - val_auc: 0.7905\n",
            "Epoch 40/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4082 - auc: 0.8175 - val_loss: 0.7352 - val_auc: 0.7881\n",
            "Epoch 41/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4149 - auc: 0.8004 - val_loss: 0.7345 - val_auc: 0.7904\n",
            "Epoch 42/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4179 - auc: 0.7876 - val_loss: 0.6805 - val_auc: 0.7934\n",
            "Epoch 43/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4077 - auc: 0.8143 - val_loss: 0.6662 - val_auc: 0.7931\n",
            "Epoch 44/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.3873 - auc: 0.8338 - val_loss: 0.6282 - val_auc: 0.7984\n",
            "Epoch 45/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.3863 - auc: 0.8381 - val_loss: 0.6449 - val_auc: 0.7931\n",
            "Epoch 46/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.3929 - auc: 0.8278 - val_loss: 0.6233 - val_auc: 0.7816\n",
            "Epoch 47/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.3889 - auc: 0.8289 - val_loss: 0.6672 - val_auc: 0.7872\n",
            "Epoch 48/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.3752 - auc: 0.8459 - val_loss: 0.7183 - val_auc: 0.7881\n",
            "Epoch 49/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.3861 - auc: 0.8334 - val_loss: 0.5970 - val_auc: 0.7952\n",
            "Epoch 50/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.3807 - auc: 0.8378 - val_loss: 0.6162 - val_auc: 0.7940\n",
            "Epoch 51/1000\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.3937 - auc: 0.8230 - val_loss: 0.6437 - val_auc: 0.7869\n",
            "Epoch 52/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.3827 - auc: 0.8339 - val_loss: 0.6966 - val_auc: 0.7792\n",
            "Epoch 53/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4022 - auc: 0.8136 - val_loss: 0.6951 - val_auc: 0.7840\n",
            "Epoch 54/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.3965 - auc: 0.8169 - val_loss: 0.6048 - val_auc: 0.7872\n",
            "Epoch 55/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4035 - auc: 0.8183 - val_loss: 0.6767 - val_auc: 0.7913\n",
            "Epoch 56/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.3899 - auc: 0.8253 - val_loss: 0.6577 - val_auc: 0.7899\n",
            "Epoch 57/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.3937 - auc: 0.8185 - val_loss: 0.6584 - val_auc: 0.7903\n",
            "Epoch 58/1000\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.3803 - auc: 0.8358 - val_loss: 0.6719 - val_auc: 0.7927\n",
            "Epoch 59/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.3818 - auc: 0.8327 - val_loss: 0.6226 - val_auc: 0.7960\n",
            "Epoch 60/1000\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.3953 - auc: 0.8197 - val_loss: 0.6416 - val_auc: 0.8002\n",
            "Epoch 61/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.3850 - auc: 0.8311 - val_loss: 0.6404 - val_auc: 0.7974\n",
            "Epoch 62/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.3876 - auc: 0.8301 - val_loss: 0.6367 - val_auc: 0.7953\n",
            "Epoch 63/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.3852 - auc: 0.8265 - val_loss: 0.6158 - val_auc: 0.7944\n",
            "Epoch 64/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.3911 - auc: 0.8159 - val_loss: 0.5933 - val_auc: 0.7976\n",
            "Epoch 65/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.3836 - auc: 0.8288 - val_loss: 0.6924 - val_auc: 0.7909\n",
            "Epoch 66/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.3964 - auc: 0.8194 - val_loss: 0.6756 - val_auc: 0.7914\n",
            "Epoch 67/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.3764 - auc: 0.8407 - val_loss: 0.6352 - val_auc: 0.7925\n",
            "Epoch 68/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.3743 - auc: 0.8492 - val_loss: 0.7255 - val_auc: 0.7881\n",
            "Epoch 69/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.3706 - auc: 0.8413 - val_loss: 0.6576 - val_auc: 0.7914\n",
            "Epoch 70/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.3758 - auc: 0.8423 - val_loss: 0.6942 - val_auc: 0.7876\n",
            "Epoch 71/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.3735 - auc: 0.8419 - val_loss: 0.6272 - val_auc: 0.7954\n",
            "Epoch 72/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.3857 - auc: 0.8287 - val_loss: 0.6545 - val_auc: 0.7937\n",
            "Epoch 73/1000\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.3749 - auc: 0.8376 - val_loss: 0.6292 - val_auc: 0.7916\n",
            "Epoch 74/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.3809 - auc: 0.8339 - val_loss: 0.6863 - val_auc: 0.7881\n",
            "Epoch 75/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.3689 - auc: 0.8453 - val_loss: 0.6192 - val_auc: 0.7948\n",
            "Epoch 76/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.3723 - auc: 0.8428 - val_loss: 0.7096 - val_auc: 0.7932\n",
            "Epoch 77/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.3812 - auc: 0.8345 - val_loss: 0.6241 - val_auc: 0.7967\n",
            "Epoch 78/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.3860 - auc: 0.8256 - val_loss: 0.6304 - val_auc: 0.7915\n",
            "Epoch 79/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.3810 - auc: 0.8370 - val_loss: 0.6176 - val_auc: 0.7921\n",
            "35/35 - 0s - loss: 0.3979 - auc: 0.8615 - 256ms/epoch - 7ms/step\n",
            "4/4 - 0s - loss: 0.4610 - auc: 0.8038 - 49ms/epoch - 12ms/step\n",
            "Train loss: 0.39791637659072876\n",
            "Train accuracy: 0.8615252375602722\n",
            "Test loss: 0.46099764108657837\n",
            "Test accuracy: 0.8038194179534912\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/1000\n",
            "36/36 [==============================] - 12s 91ms/step - loss: 0.6987 - auc: 0.6072 - val_loss: 0.9273 - val_auc: 0.7974\n",
            "Epoch 2/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.6274 - auc: 0.6570 - val_loss: 0.8009 - val_auc: 0.8026\n",
            "Epoch 3/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.5925 - auc: 0.6934 - val_loss: 0.7388 - val_auc: 0.8033\n",
            "Epoch 4/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.5795 - auc: 0.6881 - val_loss: 0.7102 - val_auc: 0.8068\n",
            "Epoch 5/1000\n",
            "36/36 [==============================] - 2s 56ms/step - loss: 0.5375 - auc: 0.7455 - val_loss: 0.7090 - val_auc: 0.8061\n",
            "Epoch 6/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.5207 - auc: 0.7604 - val_loss: 0.7100 - val_auc: 0.8045\n",
            "Epoch 7/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4937 - auc: 0.7829 - val_loss: 0.6948 - val_auc: 0.8009\n",
            "Epoch 8/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4987 - auc: 0.7627 - val_loss: 0.7438 - val_auc: 0.7977\n",
            "Epoch 9/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.5025 - auc: 0.7504 - val_loss: 0.7543 - val_auc: 0.7948\n",
            "Epoch 10/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4804 - auc: 0.7747 - val_loss: 0.6870 - val_auc: 0.7913\n",
            "Epoch 11/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4581 - auc: 0.7904 - val_loss: 0.6603 - val_auc: 0.7828\n",
            "Epoch 12/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4522 - auc: 0.8118 - val_loss: 0.7069 - val_auc: 0.7774\n",
            "Epoch 13/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4439 - auc: 0.8042 - val_loss: 0.7170 - val_auc: 0.7748\n",
            "Epoch 14/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4710 - auc: 0.7750 - val_loss: 0.7514 - val_auc: 0.7783\n",
            "Epoch 15/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4464 - auc: 0.8017 - val_loss: 0.6963 - val_auc: 0.7776\n",
            "Epoch 16/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.4522 - auc: 0.7983 - val_loss: 0.7297 - val_auc: 0.7747\n",
            "Epoch 17/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4507 - auc: 0.7899 - val_loss: 0.7066 - val_auc: 0.7758\n",
            "Epoch 18/1000\n",
            "36/36 [==============================] - 2s 69ms/step - loss: 0.4331 - auc: 0.8041 - val_loss: 0.6496 - val_auc: 0.7756\n",
            "Epoch 19/1000\n",
            "36/36 [==============================] - 2s 68ms/step - loss: 0.4429 - auc: 0.8005 - val_loss: 0.6808 - val_auc: 0.7753\n",
            "Epoch 20/1000\n",
            "36/36 [==============================] - 2s 68ms/step - loss: 0.4355 - auc: 0.8009 - val_loss: 0.6489 - val_auc: 0.7754\n",
            "Epoch 21/1000\n",
            "36/36 [==============================] - 2s 67ms/step - loss: 0.4303 - auc: 0.8060 - val_loss: 0.7676 - val_auc: 0.7689\n",
            "Epoch 22/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4320 - auc: 0.8017 - val_loss: 0.7901 - val_auc: 0.7689\n",
            "Epoch 23/1000\n",
            "36/36 [==============================] - 2s 68ms/step - loss: 0.4289 - auc: 0.8041 - val_loss: 0.7312 - val_auc: 0.7740\n",
            "Epoch 24/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4110 - auc: 0.8267 - val_loss: 0.6460 - val_auc: 0.7778\n",
            "Epoch 25/1000\n",
            "36/36 [==============================] - 2s 67ms/step - loss: 0.4220 - auc: 0.8125 - val_loss: 0.6555 - val_auc: 0.7787\n",
            "Epoch 26/1000\n",
            "36/36 [==============================] - 2s 68ms/step - loss: 0.4118 - auc: 0.8235 - val_loss: 0.6791 - val_auc: 0.7803\n",
            "Epoch 27/1000\n",
            "36/36 [==============================] - 2s 68ms/step - loss: 0.4214 - auc: 0.8048 - val_loss: 0.7172 - val_auc: 0.7765\n",
            "Epoch 28/1000\n",
            "36/36 [==============================] - 2s 68ms/step - loss: 0.4061 - auc: 0.8293 - val_loss: 0.7188 - val_auc: 0.7804\n",
            "Epoch 29/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4200 - auc: 0.8088 - val_loss: 0.6446 - val_auc: 0.7763\n",
            "Epoch 30/1000\n",
            "36/36 [==============================] - 2s 68ms/step - loss: 0.4120 - auc: 0.8121 - val_loss: 0.6366 - val_auc: 0.7808\n",
            "Epoch 31/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4172 - auc: 0.8121 - val_loss: 0.7004 - val_auc: 0.7803\n",
            "Epoch 32/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.3986 - auc: 0.8335 - val_loss: 0.7043 - val_auc: 0.7712\n",
            "Epoch 33/1000\n",
            "36/36 [==============================] - 2s 56ms/step - loss: 0.4120 - auc: 0.8104 - val_loss: 0.7509 - val_auc: 0.7726\n",
            "Epoch 34/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.3996 - auc: 0.8207 - val_loss: 0.7221 - val_auc: 0.7691\n",
            "Epoch 35/1000\n",
            "36/36 [==============================] - 2s 67ms/step - loss: 0.4059 - auc: 0.8129 - val_loss: 0.6862 - val_auc: 0.7559\n",
            "Epoch 36/1000\n",
            "36/36 [==============================] - 2s 67ms/step - loss: 0.4054 - auc: 0.8205 - val_loss: 0.7269 - val_auc: 0.7671\n",
            "Epoch 37/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.3897 - auc: 0.8335 - val_loss: 0.7755 - val_auc: 0.7713\n",
            "Epoch 38/1000\n",
            "36/36 [==============================] - 2s 68ms/step - loss: 0.3978 - auc: 0.8251 - val_loss: 0.6990 - val_auc: 0.7710\n",
            "Epoch 39/1000\n",
            "36/36 [==============================] - 2s 68ms/step - loss: 0.3798 - auc: 0.8403 - val_loss: 0.6701 - val_auc: 0.7733\n",
            "Epoch 40/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.3978 - auc: 0.8190 - val_loss: 0.7249 - val_auc: 0.7696\n",
            "Epoch 41/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4041 - auc: 0.8078 - val_loss: 0.6864 - val_auc: 0.7693\n",
            "Epoch 42/1000\n",
            "36/36 [==============================] - 2s 68ms/step - loss: 0.4022 - auc: 0.8261 - val_loss: 0.8061 - val_auc: 0.7750\n",
            "Epoch 43/1000\n",
            "36/36 [==============================] - 3s 69ms/step - loss: 0.4040 - auc: 0.8169 - val_loss: 0.6840 - val_auc: 0.7661\n",
            "Epoch 44/1000\n",
            "36/36 [==============================] - 2s 68ms/step - loss: 0.4018 - auc: 0.8193 - val_loss: 0.6759 - val_auc: 0.7710\n",
            "Epoch 45/1000\n",
            "36/36 [==============================] - 2s 68ms/step - loss: 0.3920 - auc: 0.8354 - val_loss: 0.7592 - val_auc: 0.7750\n",
            "35/35 - 0s - loss: 0.4401 - auc: 0.8556 - 275ms/epoch - 8ms/step\n",
            "4/4 - 0s - loss: 0.4841 - auc: 0.7814 - 48ms/epoch - 12ms/step\n",
            "Train loss: 0.44005873799324036\n",
            "Train accuracy: 0.8555641770362854\n",
            "Test loss: 0.48409226536750793\n",
            "Test accuracy: 0.7814428806304932\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/1000\n",
            "36/36 [==============================] - 13s 101ms/step - loss: 0.7216 - auc: 0.5201 - val_loss: 1.0157 - val_auc: 0.7781\n",
            "Epoch 2/1000\n",
            "36/36 [==============================] - 2s 68ms/step - loss: 0.6667 - auc: 0.5657 - val_loss: 0.9390 - val_auc: 0.7883\n",
            "Epoch 3/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.5966 - auc: 0.6974 - val_loss: 0.7582 - val_auc: 0.7845\n",
            "Epoch 4/1000\n",
            "36/36 [==============================] - 2s 67ms/step - loss: 0.5676 - auc: 0.7312 - val_loss: 0.8156 - val_auc: 0.7864\n",
            "Epoch 5/1000\n",
            "36/36 [==============================] - 2s 69ms/step - loss: 0.5443 - auc: 0.7441 - val_loss: 0.7079 - val_auc: 0.7844\n",
            "Epoch 6/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4991 - auc: 0.7914 - val_loss: 0.7159 - val_auc: 0.7829\n",
            "Epoch 7/1000\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.5249 - auc: 0.7336 - val_loss: 0.7177 - val_auc: 0.7830\n",
            "Epoch 8/1000\n",
            "36/36 [==============================] - 2s 67ms/step - loss: 0.5063 - auc: 0.7623 - val_loss: 0.8339 - val_auc: 0.7864\n",
            "Epoch 9/1000\n",
            "36/36 [==============================] - 3s 69ms/step - loss: 0.5106 - auc: 0.7489 - val_loss: 0.7468 - val_auc: 0.7824\n",
            "Epoch 10/1000\n",
            "36/36 [==============================] - 2s 67ms/step - loss: 0.4841 - auc: 0.7682 - val_loss: 0.6707 - val_auc: 0.7819\n",
            "Epoch 11/1000\n",
            "36/36 [==============================] - 2s 69ms/step - loss: 0.4656 - auc: 0.7951 - val_loss: 0.6854 - val_auc: 0.7757\n",
            "Epoch 12/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4803 - auc: 0.7544 - val_loss: 0.6827 - val_auc: 0.7774\n",
            "Epoch 13/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4437 - auc: 0.8091 - val_loss: 0.7601 - val_auc: 0.7777\n",
            "Epoch 14/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4643 - auc: 0.7786 - val_loss: 0.7060 - val_auc: 0.7743\n",
            "Epoch 15/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.4518 - auc: 0.7877 - val_loss: 0.7005 - val_auc: 0.7710\n",
            "Epoch 16/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4459 - auc: 0.7910 - val_loss: 0.6805 - val_auc: 0.7687\n",
            "Epoch 17/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4427 - auc: 0.8032 - val_loss: 0.6975 - val_auc: 0.7676\n",
            "Epoch 18/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4596 - auc: 0.7642 - val_loss: 0.7041 - val_auc: 0.7668\n",
            "Epoch 19/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.4475 - auc: 0.7822 - val_loss: 0.7340 - val_auc: 0.7609\n",
            "Epoch 20/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4185 - auc: 0.8109 - val_loss: 0.6954 - val_auc: 0.7677\n",
            "Epoch 21/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.4332 - auc: 0.7856 - val_loss: 0.6989 - val_auc: 0.7761\n",
            "Epoch 22/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4246 - auc: 0.8045 - val_loss: 0.7385 - val_auc: 0.7811\n",
            "Epoch 23/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4216 - auc: 0.8174 - val_loss: 0.6936 - val_auc: 0.7811\n",
            "Epoch 24/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4370 - auc: 0.7947 - val_loss: 0.7298 - val_auc: 0.7772\n",
            "Epoch 25/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4136 - auc: 0.8210 - val_loss: 0.6298 - val_auc: 0.7801\n",
            "Epoch 26/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4217 - auc: 0.8071 - val_loss: 0.7346 - val_auc: 0.7717\n",
            "Epoch 27/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4234 - auc: 0.7995 - val_loss: 0.6817 - val_auc: 0.7684\n",
            "Epoch 28/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4287 - auc: 0.7876 - val_loss: 0.6626 - val_auc: 0.7711\n",
            "Epoch 29/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4151 - auc: 0.8131 - val_loss: 0.6598 - val_auc: 0.7701\n",
            "Epoch 30/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4101 - auc: 0.8180 - val_loss: 0.7706 - val_auc: 0.7518\n",
            "Epoch 31/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.3979 - auc: 0.8307 - val_loss: 0.6892 - val_auc: 0.7614\n",
            "Epoch 32/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4139 - auc: 0.8136 - val_loss: 0.7228 - val_auc: 0.7571\n",
            "Epoch 33/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4267 - auc: 0.7907 - val_loss: 0.6505 - val_auc: 0.7558\n",
            "Epoch 34/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4105 - auc: 0.8127 - val_loss: 0.6734 - val_auc: 0.7476\n",
            "Epoch 35/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.3949 - auc: 0.8296 - val_loss: 0.7618 - val_auc: 0.7543\n",
            "Epoch 36/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4049 - auc: 0.8261 - val_loss: 0.6776 - val_auc: 0.7737\n",
            "Epoch 37/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4032 - auc: 0.8249 - val_loss: 0.6744 - val_auc: 0.7707\n",
            "Epoch 38/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4049 - auc: 0.8145 - val_loss: 0.6620 - val_auc: 0.7705\n",
            "Epoch 39/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.3944 - auc: 0.8292 - val_loss: 0.6820 - val_auc: 0.7653\n",
            "Epoch 40/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4137 - auc: 0.8076 - val_loss: 0.6723 - val_auc: 0.7690\n",
            "35/35 - 0s - loss: 0.4160 - auc: 0.8620 - 239ms/epoch - 7ms/step\n",
            "4/4 - 0s - loss: 0.5185 - auc: 0.7346 - 43ms/epoch - 11ms/step\n",
            "Train loss: 0.41604453325271606\n",
            "Train accuracy: 0.8619804978370667\n",
            "Test loss: 0.5185454487800598\n",
            "Test accuracy: 0.7345678806304932\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/1000\n",
            "36/36 [==============================] - 12s 109ms/step - loss: 0.6907 - auc: 0.5413 - val_loss: 0.8892 - val_auc: 0.7980\n",
            "Epoch 2/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.6148 - auc: 0.6574 - val_loss: 0.8175 - val_auc: 0.7971\n",
            "Epoch 3/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.5695 - auc: 0.7174 - val_loss: 0.7626 - val_auc: 0.7955\n",
            "Epoch 4/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.5582 - auc: 0.6941 - val_loss: 0.7626 - val_auc: 0.7896\n",
            "Epoch 5/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.5429 - auc: 0.7127 - val_loss: 0.7748 - val_auc: 0.7912\n",
            "Epoch 6/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.5312 - auc: 0.7106 - val_loss: 0.7746 - val_auc: 0.7893\n",
            "Epoch 7/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.5067 - auc: 0.7397 - val_loss: 0.8041 - val_auc: 0.7819\n",
            "Epoch 8/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4845 - auc: 0.7622 - val_loss: 0.7682 - val_auc: 0.7756\n",
            "Epoch 9/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4979 - auc: 0.7351 - val_loss: 0.7309 - val_auc: 0.7902\n",
            "Epoch 10/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.4821 - auc: 0.7516 - val_loss: 0.6541 - val_auc: 0.7904\n",
            "Epoch 11/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4585 - auc: 0.7891 - val_loss: 0.7745 - val_auc: 0.7770\n",
            "Epoch 12/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4625 - auc: 0.7779 - val_loss: 0.7394 - val_auc: 0.7751\n",
            "Epoch 13/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4731 - auc: 0.7341 - val_loss: 0.6773 - val_auc: 0.7761\n",
            "Epoch 14/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4403 - auc: 0.7912 - val_loss: 0.6775 - val_auc: 0.7721\n",
            "Epoch 15/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4427 - auc: 0.7833 - val_loss: 0.7065 - val_auc: 0.7675\n",
            "Epoch 16/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4464 - auc: 0.7850 - val_loss: 0.6693 - val_auc: 0.7674\n",
            "Epoch 17/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4288 - auc: 0.7981 - val_loss: 0.6730 - val_auc: 0.7676\n",
            "Epoch 18/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4590 - auc: 0.7468 - val_loss: 0.7184 - val_auc: 0.7681\n",
            "Epoch 19/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4209 - auc: 0.8074 - val_loss: 0.7226 - val_auc: 0.7768\n",
            "Epoch 20/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4363 - auc: 0.7713 - val_loss: 0.6963 - val_auc: 0.7761\n",
            "Epoch 21/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4146 - auc: 0.8044 - val_loss: 0.6747 - val_auc: 0.7668\n",
            "Epoch 22/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4094 - auc: 0.8145 - val_loss: 0.7131 - val_auc: 0.7615\n",
            "Epoch 23/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4245 - auc: 0.7900 - val_loss: 0.7333 - val_auc: 0.7548\n",
            "Epoch 24/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4153 - auc: 0.8073 - val_loss: 0.7007 - val_auc: 0.7608\n",
            "Epoch 25/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4210 - auc: 0.7916 - val_loss: 0.6811 - val_auc: 0.7637\n",
            "35/35 - 0s - loss: 0.4374 - auc: 0.8431 - 240ms/epoch - 7ms/step\n",
            "4/4 - 0s - loss: 0.3852 - auc: 0.8941 - 53ms/epoch - 13ms/step\n",
            "Train loss: 0.4374469518661499\n",
            "Train accuracy: 0.843095600605011\n",
            "Test loss: 0.38518083095550537\n",
            "Test accuracy: 0.8940972089767456\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/1000\n",
            "36/36 [==============================] - 12s 96ms/step - loss: 0.8003 - auc: 0.5134 - val_loss: 1.0060 - val_auc: 0.7564\n",
            "Epoch 2/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.6948 - auc: 0.5424 - val_loss: 0.9039 - val_auc: 0.8042\n",
            "Epoch 3/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.6581 - auc: 0.5517 - val_loss: 0.8218 - val_auc: 0.8128\n",
            "Epoch 4/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.5882 - auc: 0.6937 - val_loss: 0.7274 - val_auc: 0.8136\n",
            "Epoch 5/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.5720 - auc: 0.7108 - val_loss: 0.7191 - val_auc: 0.8137\n",
            "Epoch 6/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.5561 - auc: 0.7172 - val_loss: 0.6447 - val_auc: 0.8133\n",
            "Epoch 7/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.5378 - auc: 0.7486 - val_loss: 0.7808 - val_auc: 0.7308\n",
            "Epoch 8/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.5200 - auc: 0.7575 - val_loss: 0.6808 - val_auc: 0.8006\n",
            "Epoch 9/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.5292 - auc: 0.7407 - val_loss: 0.7330 - val_auc: 0.7963\n",
            "Epoch 10/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.5022 - auc: 0.7815 - val_loss: 0.7646 - val_auc: 0.7827\n",
            "Epoch 11/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.5074 - auc: 0.7609 - val_loss: 0.6735 - val_auc: 0.7997\n",
            "Epoch 12/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4884 - auc: 0.7759 - val_loss: 0.6918 - val_auc: 0.7941\n",
            "Epoch 13/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4775 - auc: 0.7850 - val_loss: 0.7128 - val_auc: 0.7970\n",
            "Epoch 14/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4732 - auc: 0.7780 - val_loss: 0.6430 - val_auc: 0.8014\n",
            "Epoch 15/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4734 - auc: 0.7861 - val_loss: 0.6373 - val_auc: 0.7991\n",
            "Epoch 16/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4649 - auc: 0.7832 - val_loss: 0.6922 - val_auc: 0.7969\n",
            "Epoch 17/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4700 - auc: 0.7809 - val_loss: 0.6618 - val_auc: 0.8011\n",
            "Epoch 18/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4636 - auc: 0.7811 - val_loss: 0.6689 - val_auc: 0.7962\n",
            "Epoch 19/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4515 - auc: 0.7899 - val_loss: 0.6617 - val_auc: 0.7948\n",
            "Epoch 20/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4498 - auc: 0.8025 - val_loss: 0.6221 - val_auc: 0.8006\n",
            "Epoch 21/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4369 - auc: 0.8004 - val_loss: 0.6736 - val_auc: 0.7895\n",
            "Epoch 22/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4517 - auc: 0.7879 - val_loss: 0.6376 - val_auc: 0.8032\n",
            "Epoch 23/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4528 - auc: 0.7967 - val_loss: 0.6260 - val_auc: 0.7988\n",
            "Epoch 24/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4358 - auc: 0.7990 - val_loss: 0.6522 - val_auc: 0.7935\n",
            "Epoch 25/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4377 - auc: 0.8012 - val_loss: 0.6074 - val_auc: 0.7951\n",
            "Epoch 26/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4170 - auc: 0.8396 - val_loss: 0.6918 - val_auc: 0.7931\n",
            "Epoch 27/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4328 - auc: 0.8034 - val_loss: 0.6669 - val_auc: 0.7899\n",
            "Epoch 28/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4377 - auc: 0.7930 - val_loss: 0.6468 - val_auc: 0.7965\n",
            "Epoch 29/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4373 - auc: 0.7809 - val_loss: 0.6674 - val_auc: 0.7895\n",
            "Epoch 30/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4167 - auc: 0.8178 - val_loss: 0.6042 - val_auc: 0.7942\n",
            "Epoch 31/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4100 - auc: 0.8309 - val_loss: 0.6495 - val_auc: 0.7917\n",
            "Epoch 32/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4273 - auc: 0.8092 - val_loss: 0.6357 - val_auc: 0.7980\n",
            "Epoch 33/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4077 - auc: 0.8257 - val_loss: 0.7252 - val_auc: 0.7882\n",
            "Epoch 34/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.4039 - auc: 0.8270 - val_loss: 0.5913 - val_auc: 0.7959\n",
            "Epoch 35/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4335 - auc: 0.8010 - val_loss: 0.6173 - val_auc: 0.8004\n",
            "Epoch 36/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4144 - auc: 0.8082 - val_loss: 0.6083 - val_auc: 0.8034\n",
            "Epoch 37/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4238 - auc: 0.8098 - val_loss: 0.6278 - val_auc: 0.8014\n",
            "Epoch 38/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4058 - auc: 0.8244 - val_loss: 0.6342 - val_auc: 0.7993\n",
            "Epoch 39/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.3974 - auc: 0.8332 - val_loss: 0.6993 - val_auc: 0.7883\n",
            "Epoch 40/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4206 - auc: 0.8015 - val_loss: 0.6823 - val_auc: 0.7918\n",
            "Epoch 41/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4098 - auc: 0.8079 - val_loss: 0.6248 - val_auc: 0.7908\n",
            "Epoch 42/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4074 - auc: 0.8201 - val_loss: 0.6361 - val_auc: 0.7940\n",
            "Epoch 43/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4027 - auc: 0.8145 - val_loss: 0.6351 - val_auc: 0.7935\n",
            "Epoch 44/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.3941 - auc: 0.8229 - val_loss: 0.5967 - val_auc: 0.7937\n",
            "Epoch 45/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4100 - auc: 0.8123 - val_loss: 0.6254 - val_auc: 0.7977\n",
            "Epoch 46/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4001 - auc: 0.8160 - val_loss: 0.5854 - val_auc: 0.8009\n",
            "Epoch 47/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.3967 - auc: 0.8280 - val_loss: 0.5965 - val_auc: 0.8010\n",
            "Epoch 48/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4048 - auc: 0.8215 - val_loss: 0.6144 - val_auc: 0.8078\n",
            "Epoch 49/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4118 - auc: 0.8085 - val_loss: 0.5706 - val_auc: 0.8076\n",
            "Epoch 50/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4047 - auc: 0.8234 - val_loss: 0.6563 - val_auc: 0.8040\n",
            "Epoch 51/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4073 - auc: 0.8185 - val_loss: 0.6341 - val_auc: 0.7986\n",
            "Epoch 52/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.3944 - auc: 0.8309 - val_loss: 0.5807 - val_auc: 0.8058\n",
            "Epoch 53/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4097 - auc: 0.8106 - val_loss: 0.6302 - val_auc: 0.8078\n",
            "Epoch 54/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.3904 - auc: 0.8349 - val_loss: 0.5905 - val_auc: 0.8051\n",
            "Epoch 55/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.3947 - auc: 0.8215 - val_loss: 0.6047 - val_auc: 0.8006\n",
            "Epoch 56/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.3930 - auc: 0.8308 - val_loss: 0.6066 - val_auc: 0.7962\n",
            "Epoch 57/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4070 - auc: 0.8177 - val_loss: 0.6309 - val_auc: 0.7889\n",
            "Epoch 58/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4084 - auc: 0.8127 - val_loss: 0.6521 - val_auc: 0.7869\n",
            "Epoch 59/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.3971 - auc: 0.8252 - val_loss: 0.6269 - val_auc: 0.7991\n",
            "Epoch 60/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.3940 - auc: 0.8291 - val_loss: 0.6505 - val_auc: 0.7959\n",
            "Epoch 61/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.3829 - auc: 0.8397 - val_loss: 0.6187 - val_auc: 0.8028\n",
            "Epoch 62/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.3915 - auc: 0.8295 - val_loss: 0.5811 - val_auc: 0.8040\n",
            "Epoch 63/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.3969 - auc: 0.8317 - val_loss: 0.7023 - val_auc: 0.7955\n",
            "Epoch 64/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.3997 - auc: 0.8245 - val_loss: 0.5797 - val_auc: 0.8055\n",
            "35/35 - 0s - loss: 0.4053 - auc: 0.8596 - 253ms/epoch - 7ms/step\n",
            "4/4 - 0s - loss: 0.4621 - auc: 0.8184 - 44ms/epoch - 11ms/step\n",
            "Train loss: 0.4053405821323395\n",
            "Train accuracy: 0.8595594167709351\n",
            "Test loss: 0.4621351659297943\n",
            "Test accuracy: 0.8184210062026978\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/1000\n",
            "36/36 [==============================] - 13s 91ms/step - loss: 0.7811 - auc: 0.5296 - val_loss: 0.8122 - val_auc: 0.7915\n",
            "Epoch 2/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.6913 - auc: 0.5826 - val_loss: 0.8069 - val_auc: 0.7888\n",
            "Epoch 3/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.6280 - auc: 0.6494 - val_loss: 0.6597 - val_auc: 0.8117\n",
            "Epoch 4/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.5940 - auc: 0.6989 - val_loss: 0.6098 - val_auc: 0.8150\n",
            "Epoch 5/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.5701 - auc: 0.7314 - val_loss: 0.6170 - val_auc: 0.8066\n",
            "Epoch 6/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.5658 - auc: 0.7101 - val_loss: 0.6338 - val_auc: 0.8116\n",
            "Epoch 7/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.5467 - auc: 0.7550 - val_loss: 0.6016 - val_auc: 0.8080\n",
            "Epoch 8/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.5323 - auc: 0.7437 - val_loss: 0.5931 - val_auc: 0.8069\n",
            "Epoch 9/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.5161 - auc: 0.7718 - val_loss: 0.5971 - val_auc: 0.8064\n",
            "Epoch 10/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.5099 - auc: 0.7612 - val_loss: 0.5728 - val_auc: 0.8055\n",
            "Epoch 11/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.5204 - auc: 0.7353 - val_loss: 0.5755 - val_auc: 0.8036\n",
            "Epoch 12/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4867 - auc: 0.7966 - val_loss: 0.5989 - val_auc: 0.7928\n",
            "Epoch 13/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4987 - auc: 0.7498 - val_loss: 0.5497 - val_auc: 0.8091\n",
            "Epoch 14/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4879 - auc: 0.7712 - val_loss: 0.5617 - val_auc: 0.8070\n",
            "Epoch 15/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4635 - auc: 0.8092 - val_loss: 0.5604 - val_auc: 0.8056\n",
            "Epoch 16/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4713 - auc: 0.7828 - val_loss: 0.5767 - val_auc: 0.7912\n",
            "Epoch 17/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4754 - auc: 0.7875 - val_loss: 0.6114 - val_auc: 0.7924\n",
            "Epoch 18/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4813 - auc: 0.7654 - val_loss: 0.5604 - val_auc: 0.8007\n",
            "Epoch 19/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4547 - auc: 0.8104 - val_loss: 0.5562 - val_auc: 0.8038\n",
            "Epoch 20/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4636 - auc: 0.8015 - val_loss: 0.5419 - val_auc: 0.8078\n",
            "Epoch 21/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4722 - auc: 0.7782 - val_loss: 0.5578 - val_auc: 0.8057\n",
            "Epoch 22/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4481 - auc: 0.8060 - val_loss: 0.5352 - val_auc: 0.8091\n",
            "Epoch 23/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4612 - auc: 0.7816 - val_loss: 0.5436 - val_auc: 0.8091\n",
            "Epoch 24/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4660 - auc: 0.7792 - val_loss: 0.5355 - val_auc: 0.8028\n",
            "Epoch 25/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4376 - auc: 0.8117 - val_loss: 0.5462 - val_auc: 0.8107\n",
            "Epoch 26/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4419 - auc: 0.8039 - val_loss: 0.5393 - val_auc: 0.8099\n",
            "Epoch 27/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4423 - auc: 0.8183 - val_loss: 0.5611 - val_auc: 0.8130\n",
            "Epoch 28/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4637 - auc: 0.7849 - val_loss: 0.5414 - val_auc: 0.8152\n",
            "Epoch 29/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4531 - auc: 0.7872 - val_loss: 0.5509 - val_auc: 0.8174\n",
            "Epoch 30/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4512 - auc: 0.7875 - val_loss: 0.5132 - val_auc: 0.8169\n",
            "Epoch 31/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4394 - auc: 0.8138 - val_loss: 0.5486 - val_auc: 0.8123\n",
            "Epoch 32/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4441 - auc: 0.8082 - val_loss: 0.5237 - val_auc: 0.8148\n",
            "Epoch 33/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4437 - auc: 0.8014 - val_loss: 0.5146 - val_auc: 0.8116\n",
            "Epoch 34/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4280 - auc: 0.8220 - val_loss: 0.5222 - val_auc: 0.8139\n",
            "Epoch 35/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4133 - auc: 0.8336 - val_loss: 0.5273 - val_auc: 0.8056\n",
            "Epoch 36/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4441 - auc: 0.7987 - val_loss: 0.5580 - val_auc: 0.8039\n",
            "Epoch 37/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4216 - auc: 0.8244 - val_loss: 0.5144 - val_auc: 0.8056\n",
            "Epoch 38/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4268 - auc: 0.8182 - val_loss: 0.5781 - val_auc: 0.7994\n",
            "Epoch 39/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4223 - auc: 0.8225 - val_loss: 0.5304 - val_auc: 0.8033\n",
            "Epoch 40/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4308 - auc: 0.8130 - val_loss: 0.5356 - val_auc: 0.8018\n",
            "Epoch 41/1000\n",
            "36/36 [==============================] - 2s 57ms/step - loss: 0.4273 - auc: 0.8159 - val_loss: 0.5664 - val_auc: 0.7996\n",
            "Epoch 42/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4157 - auc: 0.8245 - val_loss: 0.5492 - val_auc: 0.7899\n",
            "Epoch 43/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4354 - auc: 0.8073 - val_loss: 0.5904 - val_auc: 0.7912\n",
            "Epoch 44/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4205 - auc: 0.8217 - val_loss: 0.5518 - val_auc: 0.7960\n",
            "Epoch 45/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4364 - auc: 0.8031 - val_loss: 0.5341 - val_auc: 0.7985\n",
            "35/35 - 0s - loss: 0.4197 - auc: 0.8522 - 254ms/epoch - 7ms/step\n",
            "4/4 - 0s - loss: 0.4117 - auc: 0.8682 - 47ms/epoch - 12ms/step\n",
            "Train loss: 0.41966238617897034\n",
            "Train accuracy: 0.8521621823310852\n",
            "Test loss: 0.41165825724601746\n",
            "Test accuracy: 0.8682330846786499\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU5b3H8c8ve8hOFghJICBrEASJgltlEUVFXK5acbe2XnurtrWtS23rcq9tbW9btdWqtW6tG1drXcAVRVREFmXfCVsgJCEh+5787h/PACEkJAGGCTm/9+s1r8ycc+bkmQM53/Ms5xlRVYwxxnhXUKALYIwxJrAsCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuMsCIxph4hkioiKSEgHtr1eRD4/GuUy5kixIDDdiohsFpE6EUlqsfwb38k8MzAl61ygGHM0WRCY7mgTMH3PCxEZAfQIXHGM6dosCEx39A/g2mavrwNeaL6BiMSJyAsiUigiW0TkFyIS5FsXLCL/KyK7RCQHOL+V9/5dRPJEZLuI/I+IBB9OgUWkj4i8JSLFIrJBRL7XbN3JIrJIRMpEJF9E/uhbHiEi/xSRIhEpEZGFItLrcMphvMmCwHRH84FYERnmO0FfAfyzxTZ/BuKAAcCZuOC4wbfue8BUYDSQDVza4r3PAQ3AQN82ZwPfPcwyvwLkAn18v+/XIjLRt+4R4BFVjQWOA2b4ll/n+wwZQCJwM1B9mOUwHmRBYLqrPbWCycBqYPueFc3C4W5VLVfVzcAfgGt8m1wOPKyq21S1GPhNs/f2As4DfqSqlapaAPzJt79DIiIZwGnAnapao6pLgKfZV6upBwaKSJKqVqjq/GbLE4GBqtqoqotVtexQy2G8y4LAdFf/AK4ErqdFsxCQBIQCW5ot2wKk+Z73Aba1WLdHP99783zNMSXAk0DKYZS1D1CsquVtlOdGYDCwxtf8M9W3/B/A+8ArIrJDRH4nIqGHUQ7jURYEpltS1S24TuPzgH+1WL0LdzXdr9myvuyrNeThmluar9tjG1ALJKlqvO8Rq6rDD6O4O4CeIhLTWnlUdb2qTseFzUPAayISpar1qnq/qmYBp+Kas67FmE6yIDDd2Y3ARFWtbL5QVRtx7ewPikiMiPQDbmdfP8IM4DYRSReRBOCuZu/NAz4A/iAisSISJCLHiciZnShXuK+jN0JEInAn/HnAb3zLRvrK/k8AEblaRJJVtQko8e2jSUQmiMgIX1NXGS7cmjpRDmMACwLTjanqRlVd1MbqW4FKIAf4HHgJeMa37m+4JpelwNccWKO4FggDVgG7gdeA1E4UrQLXqbvnMRE33DUTVzt4A7hXVT/ybT8FWCkiFbiO4ytUtRro7fvdZbh+kE9xzUXGdIrYF9MYY4y3WY3AGGM8zoLAGGM8zoLAGGM8zoLAGGM87pibBTEpKUkzMzMDXQxjjDmmLF68eJeqJre27pgLgszMTBYtamtEoDHGmNaIyJa21lnTkDHGeJwFgTHGeJwFgTHGeNwx10dgjDGdVV9fT25uLjU1NYEuit9FRESQnp5OaGjHJ6K1IDDGdHu5ubnExMSQmZmJiAS6OH6jqhQVFZGbm0v//v07/D5rGjLGdHs1NTUkJiZ26xAAEBESExM7XfOxIDDGeEJ3D4E9DuVzWhAYY4zHWRAYY4yfFRUVMWrUKEaNGkXv3r1JS0vb+7quru6g7120aBG33XabX8tnncXGGONniYmJLFmyBID77ruP6OhofvrTn+5d39DQQEhI66fj7OxssrOz/Vo+v9UIROQZESkQkRXtbHeSiDSIyKX+KosxxnQ1119/PTfffDNjx47ljjvuYMGCBZxyyimMHj2aU089lbVr1wIwZ84cpk6dCrgQ+c53vsP48eMZMGAAjz766BEpiz9rBM8BfwFeaGsD33etPoT7DlhjjPG7+99eyaodZUd0n1l9Yrn3guGdfl9ubi7z5s0jODiYsrIyPvvsM0JCQvjoo4/4+c9/zuuvv37Ae9asWcMnn3xCeXk5Q4YM4fvf/36n7hlojd+CQFXnikhmO5vdCrwOnOSvchhjTFd12WWXERwcDEBpaSnXXXcd69evR0Sor69v9T3nn38+4eHhhIeHk5KSQn5+Punp6YdVjoD1EYhIGnAxMIF2gkBEbgJuAujbt6//C2eM6bYO5crdX6KiovY+/+Uvf8mECRN444032Lx5M+PHj2/1PeHh4XufBwcH09DQcNjlCOSooYeBO1W1qb0NVfUpVc1W1ezk5Fan0zbGmGNaaWkpaWlpADz33HNH9XcHMgiygVdEZDNwKfC4iFwUwPIYY0zA3HHHHdx9992MHj36iFzld4aoqv927voI3lHV49vZ7jnfdq+1t8/s7Gy1L6YxxnTG6tWrGTZsWKCLcdS09nlFZLGqtjoO1W99BCLyMjAeSBKRXOBeIBRAVZ/w1+81xhjTOf4cNTS9E9te769yGGOMOTibYsIYYzzOgsAYYzzOgsAYYzzOgsAYYzzOZh81xhg/KyoqYtKkSQDs3LmT4OBg9twcu2DBAsLCwg76/jlz5hAWFsapp57ql/JZEBhjjJ+1Nw11e+bMmUN0dLTfgsCahowxJgAWL17MmWeeyZgxYzjnnHPIy8sD4NFHHyUrK4uRI0dyxRVXsHnzZp544gn+9Kc/MWrUKD777LMjXharERhjvOXdu2Dn8iO7z94j4NzfdnhzVeXWW2/lzTffJDk5mVdffZV77rmHZ555ht/+9rds2rSJ8PBwSkpKiI+P5+abb+50LaIzLAiMMeYoq62tZcWKFUyePBmAxsZGUlNTARg5ciRXXXUVF110ERdddHSmX7MgMMZ4Syeu3P1FVRk+fDhffvnlAetmzpzJ3Llzefvtt3nwwQdZvvwI115aYX0ExhhzlIWHh1NYWLg3COrr61m5ciVNTU1s27aNCRMm8NBDD1FaWkpFRQUxMTGUl5f7rTwWBMYYc5QFBQXx2muvceedd3LCCScwatQo5s2bR2NjI1dffTUjRoxg9OjR3HbbbcTHx3PBBRfwxhtv+K2z2K/TUPuDTUNtjOksm4b64NNQW43AGGM8zoLAGGM8zoLAGOMJx1oz+KE6lM9pQWCM6fYiIiIoKirq9mGgqhQVFREREdGp99l9BMaYbi89PZ3c3FwKCwsDXRS/i4iIID09vVPvsSAwxnR7oaGh9O/fP9DF6LKsacgYYzzOb0EgIs+ISIGIrGhj/VUiskxElovIPBE5wV9lMcYY0zZ/1gieA6YcZP0m4ExVHQH8N/CUH8tijDGmDX7rI1DVuSKSeZD185q9nA90rnfDGGPMEdFV+ghuBN5ta6WI3CQii0RkkRd6/Y0x5mgKeBCIyARcENzZ1jaq+pSqZqtq9p7v+TTGGHNkBHT4qIiMBJ4GzlXVokCWxRhjvCpgNQIR6Qv8C7hGVdcFqhzGGON1fqsRiMjLwHggSURygXuBUABVfQL4FZAIPC4iAA1tTZFqjDHGf/w5amh6O+u/C3zXX7/fGGNMxwS8s9gYY0xgWRAYY4zHWRAYY4zHWRAYY4zHWRAYY4zHWRAYY4zHWRAYY4zHWRAYY4zHWRAYY4zHWRAYY4zHWRAYY4zHWRAYY4zHWRAYY4zHWRAYY4zHWRAYY4zHWRAYY4zHWRAYY4zHWRAYY4zHWRAYY4zHWRAYY4zH+S0IROQZESkQkRVtrBcReVRENojIMhE50V9lMcYY0zZ/1gieA6YcZP25wCDf4ybgr34sizHGmDb4LQhUdS5QfJBNLgReUGc+EC8iqf4qjzHGmNYFso8gDdjW7HWub9kBROQmEVkkIosKCwuPSuGMMcYrjonOYlV9SlWzVTU7OTk50MUxxphuJZBBsB3IaPY63bfMGGPMURTIIHgLuNY3emgcUKqqeQEsjzHGeFKIv3YsIi8D44EkEckF7gVCAVT1CWAWcB6wAagCbvBXWYwxxrTNb0GgqtPbWa/AD/z1+40xxnTMMdFZbIwxxn8sCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuMsCIwxxuP89uX1ACIyBXgECAaeVtXftljfF3geiPdtc5eqzvJLYbYtgM/+CHFpEJsGcekQ28c9j+0DIeF++bXGGNPV+S0IRCQYeAyYDOQCC0XkLVVd1WyzXwAzVPWvIpIFzAIy/VKg2jIo2Qpb50FN6YHrk4bA92ZDeIxffr0xxnRVHQoCEYkCqlW1SUQGA0OBd1W1/iBvOxnYoKo5vn28AlwINA8CBWJ9z+OAHZ0sf8cNPMs9AGoroGwHlOW6nztXwFd/hXXvw4hL/VYEY4zpijpaI5gLnCEiCcAHwELg28BVB3lPGrCt2etcYGyLbe4DPhCRW4Eo4KzWdiQiNwE3AfTt27eDRT6I8GhIHuweAE2NsOI1WP22BYExxnM62lksqloFXAI8rqqXAcOPwO+fDjynqunAecA/ROSAMqnqU6qararZycnJh/SLVJVFm4tbXxkUDEOnwvoPob76kPZvjDHHqg4HgYicgqsBzPQtC27nPduBjGav033LmrsRmAGgql8CEUBSB8vUKa8u3MalT3zJ4i1thEHWNKivhA2z/fHrjTGmy+poEPwIuBt4Q1VXisgA4JN23rMQGCQi/UUkDLgCeKvFNluBSQAiMgwXBIUdLXxnTBvVh+SYcB56dy2qeuAGmWdARDysbllEY4zp3joUBKr6qapOU9WHfE03u1T1tnbe0wDcArwPrMaNDlopIg+IyDTfZj8BviciS4GXgeu11bP04esRFsJtkwaxYHMxc9a1kjXBoTD0fFj7HjTU+aMIxhjTJXUoCETkJRGJ9Y0eWgGsEpGftfc+VZ2lqoNV9ThVfdC37Feq+pbv+SpVPU1VT1DVUar6weF8mPZccVIG/RJ78Lv31tLU1EreDJsGtaWw6VN/FsMYY7qUjjYNZalqGXAR8C7QH7jGb6Xyk9DgIG6fPJjVeWW8vayVkarHTYCwGFj15tEvnDHGBEhHgyBUREJxQfCW7/4BvzTh+NsFI/swLDWWP3ywjrqGpv1XhoTD4HNgzUxobAhMAY0x5ijraBA8CWzGjfWfKyL9gDJ/FcqfgoKEO6YMYWtxFa8u3HrgBlnToLoYtnxx9AtnjDEB0NHO4kdVNU1Vz1NnCzDBz2Xzm/GDkzm5f08emb2BqroWV/4Dz4KQSBs9ZIzxjI52FseJyB9FZJHv8Qdc7eCYJCLcOWUIuypqefaLzfuvDIuCQWfB6negqanV9xtjTHfS0aahZ4By4HLfowx41l+FOhrG9OvJWcN68cScjeyubDFcdNiFULETchcEpnDGGHMUdTQIjlPVe1U1x/e4Hxjgz4IdDT87ZwgVdQ088enG/VcMPgeCw2CVNQ8ZY7q/jgZBtYicvueFiJwGHPOT8gzpHcPFo9N4bt5m8kqbfZyIWBgwwU1C55/724wxpsvoaBDcDDwmIptFZDPwF+A//Vaqo+jHZw2mSZVHZ6/ff0XWNCjdCju+CUzBjDHmKOnoqKGlqnoCMBIYqaqjgYl+LdlRktGzB1eN7ceMRbn7T0g35DyQYBs9ZIzp9jr1ncWqWua7wxjgdj+UJyBuP3swafGR3PrSN/s6jnv0hP5nuH4Cax4yxnRjh/Pl9XLEShFgsRGhPH7VieyqqOP2GUv2zUM0bBoUb4SCVQffgTHGHMMOJwi61WXy8Wlx/HLqMD5ZW8iTc3PcwqFTAbHRQ8aYbu2gQSAi5SJS1sqjHOhzlMp41Fw9rh9TR6byvx+sZeHmYojpBX1PgVX/tpvLjDHd1kGDQFVjVDW2lUeMqnb0+46PGSLCby4ZQd+ePbjlpa8pqqiFE6+FwjUw88fWV2CM6ZYOp2moW4qJCOUvV45md1U9P56xlKaRV8Dpt8Pi5+D9n1sYGGO6HQuCVgzvE8d9Fwxn7rpCHp+zASb9CsbeDPMfh08eDHTxjDHmiOp2zTtHyvSTM/hqUxF//HAdY/r15JQpv4X6Kpj7ewjtAWccwujZukpY+y6sfAOikmDSvW6YqjHGBJAFQRtEhAcvHsHy3FJ+8NLXPHLFKM6Y+jDUV8Ps+10YjLu5/R011sPGT2D5/7kvvKmvhOjeULXLhcIFj8CQc/3/gYwxpg0WBAcRHR7C367L5uZ/LOaavy/gu6f352cXPEZ4fTW8dyeE9XCdyc011ELxJiha7wJg5Rvui24i4mHkZTDiMuh7KuSvgH9/H16+Ak64Eqb8BiLjA/NBjTGeJurHzk8RmQI8AgQDT6vqb1vZ5nLgPtx9CUtV9cqD7TM7O1sXLVrkh9K2raa+kV/PWs0LX25hWGosf75sGAM//k/YMNs1EdXXuBP/rvVQsgXUN9Q0JBKGnudO/sdNgpCw/XfcUOeamj77A0T3gml/dt+FYPa3ZiZUl8DoqwJdEmOOWSKyWFWzW13nryAQkWBgHTAZyAUWAtNVdVWzbQYBM4CJqrpbRFJUteBg+w1EEOwxe3U+d7y2jIraBu6d0p/pG3+GbJrrTviJAyFpICQOgqTB7nnSEFdraM/2r13toHCNq2FM/BVEJ/v/A3V1dZXw3l3w9Qvu9Tm/gVP+K7BlMuYYFaggOAW4T1XP8b2+G0BVf9Nsm98B61T16Y7uN5BBAFBYXsvPXlvKnLWFTBqSxO/OTSUxJR2CDnMAVn0NzPkNzHvU1ShiUqHXcN/jeEjJcgETFAI1JVC5y/UzVBa6R/VuyPwW9B17+B+ysQFWvA6r3oRJv4SUYYe/z87auQJeu8HVsk7/ERRtcNOCX/gYjL766JfHmGPcwYLAn30EacC2Zq9zgZZnqcEAIvIFrvnoPlV9r+WOROQm4CaAvn37+qWwHZUcE86z15/E8/M28+t31zDxyVK+d0Yt152aSUxE6KHvODQCJt8PIy6FnDmQv9L1I2yaC42+ifCCfP9cTQ1t7oaBZ8GEn0PamM6XobEelr0Kc/8Xdm9ys6/mLoQb3nU1nKNBFRY+De/f4/pMrnkDjpvg+l5e+ja8dSuEx7ppwo05Fq16C8Kj4biuM4GzP2sElwJTVPW7vtfXAGNV9ZZm27wD1OO+/jIdmAuMUNWStvYb6BpBc+vyy3no3TXMXlNAXGQo3zuj/+EHQkuN9e5qOH+lewBEJfseSft+hkbComfhi0dc5/Tgc10gpI5s/3c01MGSF+HzP0LJVkg9Ac680zV3PXc+BIXCDbOgZ/8j97laU1UMb/4A1s6CgZPhor/u30RWVwkvXAR5S+DKGS4gjDlWNDW5EYdfPOxej/85fOtnh9+a0EFduWnoCeArVX3W93o2cJeqLmxrv10pCPZYllvCIx+t928gdFRNGXz1JHz5Z6gpdTOojr8beg6A2nKoLXOPGt/P3Zth/hNQlgtp2XDmHTDobBDf5LL5q1wYhEXDDTMh3g81spoy1yE8+wHXzDX5fhj7/db/QKp3w3NT3cisa/8NGScf+fKY7mfHN+7/+dn/DdEpR//311e7fsCVb8CYG9zrZa+4iS0vfgLCY/xehEAFQQius3gSsB3XWXylqq5sts0UXAfydSKSBHwDjFLVorb22xWDYI+WgXDBCamkxkWSHB1OUkwYydERJMWEkRgVTliIn68Cqkvgy8dg/l+hrvzg22aMcwFw3MR9AdDcjiXwwjSI7OlqBrFHYL7BukpY977ri1j/ITTWuj6QS56CPqMP/t7yfHh2ClQVwfWzoPfxh18e031VFcMTZ7iLnaQhcP07Ry4MVFv/m2muche8PB1yF8DkB+DU29zy+Y/DB79wZZr+krtY86OABIHvF58HPIxr/39GVR8UkQeARar6logI8AdgCtAIPKiqrxxsn105CPZYllvCo7M3MD+niIra1tvzh/aO4YqTMrh4dDpxPfxYc6gqds0+DbWubT0ittnPGIhMgLiM9v8z5y5yzTIxvV0YtPaH1NQEBSuhYDUEBUNwOASHuWGzweHuZ9kOd1W09l13p3Z0L8i6CI7/D0g/qePV5JKt8PdzXH/J9FdcH0Z4bPufo7uoLnHHoPcI73zmQ6Hq7tXZ+DFM/m/XNBOXcfhhsHM5vP1D2LXB9VedcIW7P6jl/99dG+DFS6E8Dy5+EoZftP/6jZ+4QRGqcNmzfu03CFgQ+MOxEATN1dQ3UlheS2FFLbt8PwvKavlkbQHLcksJDwli6sg+XDk2gxP7JiBd+Y96y5fwz0sgIROue8eFSOFq2PQZbP4Mtnzhmm7aE9kTsi6E4y+Bfqe50DgUhWvh2XNdzQBc2EQlu36FqGSISnGjrgafA4nHtb+/sh2uI2/Dh+594/7LBV9XUrLV1fK+fgHqKlwtKvtGdyLywg2Jq950TZ/j73bfINieeX92V93n/h7G3gSbP4cXL3NhcN3bbqr5zqivcff+fPGw+//f/1uw9j03Y0BchrtnaOS3IWUobJkHr1zpBl1MfwUyTmp9n8Wb4JWr3N/S5AfglFv8Eu4WBF3Uiu2lvLxgK28u2UFFbQODe0Uz/eS+XDgqjZ5RYe3vIBByPoWXLncn2vqqfSfh+H6QeYb749zTtNNQ60Y8NdS6pp+GOgiLgr7jIPgI1YJKc2HzF1BZABUFrhq+53lFAVTsdNslDoIhU1wnesZYCPaNwCrb4U4uK/8N2+a7ZQn93Y2BQaEw6ko47Tb/Vdubmly/SFTSwQMxbyl88airTYm4GlTGWFjyEmxf5KY8GXEZnHSj6+zfQxXKtrvmvbwl7md0Cpzz684Fx7oPXPnaaj70t7I8mPVTWPOOq2UicOnfYdgFbb9n2wJ3oTDkPLj8hX3lPtQw2DrfjVrbtc7NBnDOg26usLpKWDPLtflv/NgN/+41AnatdX8XV/1f+wMtaivgzf9y/xczxrmbJ7MucjX3I8SCoIurrG3gnWU7eOmrrSzNLSVIYEy/BM4a1ouzsnpxXHJ0oIu4vw2z3T0PSYMh83T38Ecn8pFQvMn1Rax7z50AmurddB8DJ0Hp9n0n/5ThrtqedREkD4aije6ejiUvuean4ZfA6T/evz+ittw1g+X7msOKN7r7P1Ky3BVhSpZr+mp+4qwth+2L3Ulq2wLXblxT6k5uCZnQ8zgXOokD3PPGOtfXs+lT12E/5noY932IS9+3zx3fwMK/w/LXoKHaNbH1O82Va8c37n4TcFemyUPciSwuA6540dV8Dqa+2t3Ut/g597rvKW423n6nHuY/TAepwtfPwwe/chcT4++GUVfBK9PdcZz6MIy57sD3VRXDk98CCYKbP4OIuP3XdyYMasvdQIYFf3PbX/AnN0y7NeX5rt9rxWvu/8K0P3d8YklV9zsWPOlGCoZEwrCpcMJ0GDD+0GvOPhYEx5BVO8p4b0UeH60uYFVeGQD9k6I4a1gKk4b1IrtfAiHBNnv4Iakpg5xPXDBs+Mg1HQ2/0J38kwa1/p7yne5EvOgZ1xRz3CR30i5Y6Zpp9giLdld9ZXn7TrzgQiclC+Iz3AisgpW+KUgEkoe6UU8pw1wbctFGF1zFOe6EvkdMqpsGfcz1B7+Kry6BpS+7UCjOcfvvMwpSR7laWq/h7k73rfNhxrXuBDftz+7eldYUroX/u959Z/fpP3bh8+nvXS1r4Fkw8Rftd+wfjqKNrh1+82eutnnBI/ua+Ooq3WfY8JELptNv3xe4qq5zdsNHcOMHkHZi6/vf/IVrv98TBlHJroZbtt3VFMt3uJ9LX3XLxv4nTPyluwfAn1RdyC15yYVKTYn7PzDychh1tbtQOQQWBMeo7SXVfLw6n49WF/DlxiLqGptIjApjyvG9mTqyDyf370lwUBfuU+hOqnfDgqfdlXF4tDu598pyNYleWRDXd19HYUWha+8taPYo2equxjPGurbitOy2T+pNTS4YinPcCe+4iQfOU3Uwqu7+k4O9p3wnzLjO1YhOuQXOun9fc5mqOwnN+qlrcrrkyX1XwHVV7oa/z//ojknWhTDhHvfZDoeqG85c5jv55i6Ez//k+n3OfgBOvO7AJqnGevj3f8HyGa4/5+wH3b/BvL/AB/fAub9zJ++D2RMGEuSaMJvq918vwa5D/rzfB2aockOtG1ix9GU3uu7UW1w/wiGwIOgGKmob+GxdITOX5zF7dQHV9Y0kx4Rz/ohUzh+Zypi+CQR1IhQam5QNBRXkFFZw+qCkwNzzYAKroc6dMBc85a64L3sOQiJg5k9ce3fmGXDJ3yA29cD31pTCl4/Dl39xfUUZ49zNi71HQO+RrjbSMoga6919K7vWudpG0QYo3eZqUWU7XIdrc0Onwnn/2/rv36OpyX1z4Fd/hRGXQ/YN8PwFbmr3y//Rsf6MbQtd81NUEsT0ccOjY1MhNs3VEg6zSeaIqSgE9JBHO1kQdDNVdQ18vKaAd5bm8cnaAmobmkiNi+CE9HgyekbSt2cP0nv2ICOhB+kJkUSEBrO7so4l20r4eutuvtlawpJtJXuHtmb0jOThb49iTD/7khxPWvISvPNj6JHo7lAvzoEz74Jv/bT9k2BlEcx/zI0cy1/hQgFcR3vKUDdPVm25mzOqOGf/K+7o3q5vac9JNybVdxJOc81Q8RkdK7+qq6HMfsBd2cdlwH/O9cYoqk6wIOjGKmob+GhVPu+t2Mm6gnJyd1dT19C03zY9o8IornTzFQUJDO0dy4n94jmxbwJxkaHc9/ZKtu+u5paJg7ht4kDrg/CiHUvg1Wtc5/R/PN2xoZktNTW6k33eUjfOfucy12EdHusGFiQP9s3MO8Td99GyA/dwLX7ezZN1+XOHNtdWN2dB4CFNTUphRS3biqvYtruKbcXV5JVWk9GzB6MzEhiZHkdU+P5zDZbX1HPvWyv519fbGZURz8PfHkVmUlSbv6OytoFNuyoZmBJNRGgXqTabw1df4zqyOzJ1ujnmWBCYDnln2Q5+/q/lNDQp910wnMuy0xERKmsbWLxlN1/mFDE/p4hluaU0NikhQcLwtDjG9E1gTL8ETuwXT2pcZKA/hjGmFRYEpsN2lFRz+4wlzM8p5vSBSVTVNbAst90ncG0AABEhSURBVJQG34n/hIx4xg3oyeBeMazdWc7iLbtZmltCTb1rjuoTF8G4AYl871sDGJZ65G6GMcYcHgsC0ymNTcrfPsvhiU830j8pilMGJDJuQCLZmQn0CDvwKyzqG5tYnVfG4i27WbxlN5+uLaS8toHzR6Tyo7MGMajXkZlZsaquge27q8ndXU3u7ir3s6SajIQe3HzmAOJ7dNG7sY3pAiwIzFFVWlXP05/n8Mznm6iqb+TCE/pw26RBDGjjDunahkY276pia3EVxZW17Kqoo6iijqLKWt/POvLLavZ2eO8RFhxEanwE24qriIkI5daJA7n2lMxDntm1pr6RDQUVbCys4IT0+IP2kxhzrLEgMAFRXFnHU3NzeH7eZuoam7h4dBqXZ2eQV1rN+vwK1heUs76ggi1FVTQ27f//MDo8hJ5RYSRGu2m7k2PCSU+I9D3csNjk6HCCgoTVeWX8etZqPlu/i36JPbhrylCmHN+7zQn8VJWdZTWszitjdV45a3aWsyavjJxdlXvLERwkXDw6jdsmDqJvYuc6Txsam9hV4cIrv6yG/PJaKmsbuHh0Gr1iIw7tYHZCbUMjJVX1pMSEd+1JDM1RZUFgAqqwvJYnP93IP+ZvodY3tDU4SMhM7MGglBgG9YpmYEo0mYlRJMWEkxgVdkijkT5dV8ivZ65mbX452f0SuOf8YRyfFkdOYSWr8kpZtaOMVXllrNpRxu6qfePZ0+IjGZYaw9DesQxNjaFfzyje+GY7L361hYYm5dIT07ll4kAyeh4YCLUNjSzevJu563fx1aYitu+uZldFLU2t/FlFhQXzw7MGccNp/Qn1wxDd4so6Xpy/hee/3MKuilpSYsIZ3dcNEz6xXwIj0uJslJeHWRCYLiG/rIal20rITIoiMzHKL1/O09DYxGuLc/nDh+soLK8lLCRo730VYSFBDO0dQ1ZqLMNSY8nqE8uQ3jHEtnFXdUFZDY/P2chLC7bS1KRclp3ODyYMpLqukbnrd/HZ+kK+yimmur6RkCDhxL4J9E+KoldcBL1iw+kVE0GvWPe8sq6R/3lnFbPXFDAwJZr7pw3ntIFJR+Qz5xRW8PfPN/H617nU1Dcxfkgypw9MYsX2Ur7eWsLWYneTV0iQkNUnloEp0USFhRAZFkxEaDA9woKJDA0mMiyYtPhITuybQGSYBUZ3Y0FgPKeytoHnv9xMSVU9Wb6T/oCkqEO6WW5naQ2Pz9nAKwu2Ude472a9AclRfGtQMmcMSmLsgESiww/sSG9p9up87n97FVuLqzh/RCr3nD+MPvFtD7ltalKaVGlSaFJFfT+bVFm1o4ynP9/ER6vzCQ0K4uLRaXz3jP4HdM4Xltfuvav86y272VZcRU1DE1V1DXtHezUXGiyMTHejw8b2T2RMv4QD7j3Z0/xVUF5DQVktdY1NBAkEiRAkQnCQIAIhQUEMTY0hKTq83WPTEQ2NTSzNLSErNc7CqpMsCIw5AnaUVPPqwm2kxkVw+qAk0hMO7carmvpGnpqbw2OfbCBIhOtPyyQyNJhdFbXuUV6393lZTevfcLdHQo9QrhnXj2tOySQ5pvMn26Ympaahkeq6RqrqGtlQWMFXOcXMzyli+fZ994uMSI8jPjKUgvJa8stqKaqspaOnjiCBsf0TOW9Eb845vjcpMYfWT7JyRyl3vr6MFdvLSIwK4zun9+fqcf2Iizy25slq8rUbdmZuMIB1+eVEhga32kTZERYExnRB24qr+J+Zq3h/ZT4AcZGhJEWHkRgd7r7nOjqM+B5hhAQJQb4r7CARgsU9T4wOY8rwVL9dGVf4biT8KqeIBZuKqaprdE1esRGkxEaQEuN7HhNORGgwTao0NrlaS6Ov1lJb38SXG3cxc3keGwsrEYGTMnty3vG9mXJ8Kr3j2g+FmvpGHp29nifn5pDQI5QfTBjIp+sKmbO2kOjwEK4a15cbT+9/yAHTGlVlR2kNa/LKKKqoI65HKAk9wugZFUp8jzDiI0M7XbvcWlTFiwu2MGPhNgAmDevF2Vm9OGNQcpv/huvzy3lnWR6zluexvqCC75zWn19dkHVIn8mCwJgurKSqjh5hIX7pM+kqVJX1BRXMWp7Hu8t3sja/HIBRGfFMznInxIEp0QeMclqwqZi7Xl9Gzq5KLh2Tzi/OH7b3fpGVO0p54tMcZi7bQUhwEJeOSefaU/rR0KjsLK0hr6yGnaXV5JXWsLO0htLqenpGhZESE0FyjBuJluL7GRosrN1ZwZqdZazJK2f1zjLK26mNxUSEkJHQg5P79+Tk/j05KbPnAbWyxiZlztoC/jl/C3PWFRIkwlnDUogMDWb2mgLKaxqICA3iW4OSOXt4byYNTWFXRS0zl+cxc5k7+YvAyZk9OX9kKlOG9yblEEeeWRAYY7qUDQUVvLcijw9W5bMstxSAfok9mDysF5OzejG0dyy//2AN/5y/lfSESH5zyQjOGJTc6r4276rkybk5vL44d78+HHCj03rFhNM7LoK4yFCKq+opLKuhsKKW+sYDz33R4SEM6R3D0N4xDE2NZVjvGHrFRlBaXc/uqjqKK+soqXLPd1fWsbGwksVbdlNd3wi4fqOxvlDYWVbDi/O3sr2kmuSYcKaf3JfpJ2fsnYalvrGJr3KK+WDVTj5Ymc/OshpE3GSqe2pOUw/z5N9cwIJARKYAjwDBwNOq+ts2tvsP4DXgJFU96FnegsCY7mVnaQ0frs7no1X5e7+AaU/F4Dun9ecnZw9u9Y72lgrKapi9poCEHqH0joskNS6CpOjwVr+8SVUpqaqnsKKWwvJaahsaGZQSQ1p8ZKfb7usbm1ixvZQFm4rdY3Px3trEKQMSuXpcP84e3uugQ4ZVleXbS/l4TQEJPcI49/gjc/JvLiBBICLBwDpgMpALLASmq+qqFtvFADOBMOAWCwJjvKu8pp6563bx9dbdTB2Zyui+CYEuUqc1Nilrd5YTGRZM/y50d/rBgqD9mD10JwMbVDXHV4hXgAuBVS22+2/gIeBnfiyLMeYYEBMRyvkj3bfuHauCffdrHEv82TuVBmxr9jrXt2wvETkRyFDVmQfbkYjcJCKLRGRRYWHhkS+pMcZ4WMCGKYhIEPBH4CftbauqT6lqtqpmJye33mFkjDHm0PgzCLYDzb90NN23bI8Y4HhgjohsBsYBb4lIq21Yxhhj/MOfQbAQGCQi/UUkDLgCeGvPSlUtVdUkVc1U1UxgPjCtvc5iY4wxR5bfgkBVG4BbgPeB1cAMVV0pIg+IyDR//V5jjDGd489RQ6jqLGBWi2W/amPb8f4sizHGmNZ133vajTHGdIgFgTHGeJwFgTHGeJwFgTHGeJwFgTHGeJwFgTHGeJwFgTHGeJwFgTHGeJwFgTHGeJwFgTHGeJwFgTHGeJwFgTHGeJwFgTHGeJwFgTHGeJwFgTHGeJwFgTHGeJwFgTHGeJwFgTHGeJwFgTHGeJwFgTHGeJxfg0BEpojIWhHZICJ3tbL+dhFZJSLLRGS2iPTzZ3mMMcYcyG9BICLBwGPAuUAWMF1Eslps9g2QraojgdeA3/mrPMYYY1rnzxrBycAGVc1R1TrgFeDC5huo6ieqWuV7OR9I92N5jDHGtMKfQZAGbGv2Ote3rC03Au+2tkJEbhKRRSKyqLCw8AgW0RhjTJfoLBaRq4Fs4PetrVfVp1Q1W1Wzk5OTj27hjDGmmwvx4763AxnNXqf7lu1HRM4C7gHOVNVaP5bHGGNMK/xZI1gIDBKR/iISBlwBvNV8AxEZDTwJTFPVAj+WxRhjTBv8FgSq2gDcArwPrAZmqOpKEXlARKb5Nvs9EA38n4gsEZG32tidMcYYP/Fn0xCqOguY1WLZr5o9P8ufv98YY0z7ukRnsTHGmMCxIDDGGI+zIDDGGI+zIDDGGI+zIDDGGI+zIDDGGI+zIDDGGI+zIDDGGI+zIDDGGI+zIDDGGI+zIDDGGI+zIDDGGI+zIDDGGI+zIDDGGI+zIDDGGI+zIDDGGI+zIDDGGI+zIDDGGI+zIDDGGI+zIDDGGI/zaxCIyBQRWSsiG0TkrlbWh4vIq771X4lIpj/LY4wx5kB+CwIRCQYeA84FsoDpIpLVYrMbgd2qOhD4E/CQv8pjjDGmdf6sEZwMbFDVHFWtA14BLmyxzYXA877nrwGTRET8WCZjjDEthPhx32nAtmavc4GxbW2jqg0iUgokAruabyQiNwE3+V5WiMjaQyxTUst9G8COS2vsmBzIjsmBjqVj0q+tFf4MgiNGVZ8Cnjrc/YjIIlXNPgJF6lbsuBzIjsmB7JgcqLscE382DW0HMpq9Tvcta3UbEQkB4oAiP5bJGGNMC/4MgoXAIBHpLyJhwBXAWy22eQu4zvf8UuBjVVU/lskYY0wLfmsa8rX53wK8DwQDz6jqShF5AFikqm8Bfwf+ISIbgGJcWPjTYTcvdVN2XA5kx+RAdkwO1C2OidgFuDHGeJvdWWyMMR5nQWCMMR7nmSBob7oLLxCRZ0SkQERWNFvWU0Q+FJH1vp8JgSzj0SYiGSLyiYisEpGVIvJD33LPHhcRiRCRBSKy1HdM7vct7++bCmaDb2qYsECX9WgTkWAR+UZE3vG97hbHxBNB0MHpLrzgOWBKi2V3AbNVdRAw2/faSxqAn6hqFjAO+IHv/4aXj0stMFFVTwBGAVNEZBxuCpg/+aaE2Y2bIsZrfgisbva6WxwTTwQBHZvuottT1bm40VnNNZ/m43ngoqNaqABT1TxV/dr3vBz3R56Gh4+LOhW+l6G+hwITcVPBgMeOCYCIpAPnA0/7Xgvd5Jh4JQham+4iLUBl6Wp6qWqe7/lOoFcgCxNIvtlvRwNf4fHj4msCWQIUAB8CG4ESVW3wbeLFv6GHgTuAJt/rRLrJMfFKEJgO8N3M58nxxCISDbwO/EhVy5qv8+JxUdVGVR2FmxHgZGBogIsUUCIyFShQ1cWBLos/HBNzDR0BHZnuwqvyRSRVVfNEJBV3BegpIhKKC4EXVfVfvsWePy4AqloiIp8ApwDxIhLiuwL22t/QacA0ETkPiABigUfoJsfEKzWCjkx34VXNp/m4DngzgGU56nztvH8HVqvqH5ut8uxxEZFkEYn3PY8EJuP6Tj7BTQUDHjsmqnq3qqaraibu/PGxql5FNzkmnrmz2JfkD7NvuosHA1yko05EXgbG46bOzQfuBf4NzAD6AluAy1W1ZYdytyUipwOfAcvZ1/b7c1w/gSePi4iMxHV8BuMuFmeo6gMiMgA30KIn8A1wtarWBq6kgSEi44GfqurU7nJMPBMExhhjWueVpiFjjDFtsCAwxhiPsyAwxhiPsyAwxhiPsyAwxhiPsyAwpgURaRSRJc0eR2zCORHJbD77qzFdgVfuLDamM6p90ysY4wlWIzCmg0Rks4j8TkSW++brH+hbnikiH4vIMhGZLSJ9fct7icgbvnn9l4rIqb5dBYvI33xz/X/gu3vXmICxIDDmQJEtmoa+3WxdqaqOAP6Cu1Md4M/A86o6EngReNS3/FHgU9+8/icCK33LBwGPqepwoAT4Dz9/HmMOyu4sNqYFEalQ1ehWlm/GfWFLjm+iup2qmigiu4BUVa33Lc9T1SQRKQTSm0854Jvq+kPfF94gIncCoar6P/7/ZMa0zmoExnSOtvG8M5rPRdOI9dWZALMgMKZzvt3s55e+5/NwM1ICXIWbxA7cV1x+H/Z+0Uvc0SqkMZ1hVyLGHCjS9+1ce7ynqnuGkCaIyDLcVf1037JbgWdF5GdAIXCDb/kPgadE5Ebclf/3gTyM6WKsj8CYDvL1EWSr6q5Al8WYI8mahowxxuOsRmCMMR5nNQJjjPE4CwJjjPE4CwJjjPE4CwJjjPE4CwJjjPG4/weQODmt3jK2HAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/1000\n",
            "36/36 [==============================] - 12s 95ms/step - loss: 0.7569 - auc: 0.5101 - val_loss: 0.8099 - val_auc: 0.7949\n",
            "Epoch 2/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.6834 - auc: 0.5672 - val_loss: 0.7501 - val_auc: 0.8013\n",
            "Epoch 3/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.6387 - auc: 0.6408 - val_loss: 0.7020 - val_auc: 0.8079\n",
            "Epoch 4/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.6086 - auc: 0.6821 - val_loss: 0.6596 - val_auc: 0.8104\n",
            "Epoch 5/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.5797 - auc: 0.7137 - val_loss: 0.6435 - val_auc: 0.8081\n",
            "Epoch 6/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.5282 - auc: 0.7773 - val_loss: 0.7098 - val_auc: 0.7728\n",
            "Epoch 7/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.5510 - auc: 0.7474 - val_loss: 0.6372 - val_auc: 0.8016\n",
            "Epoch 8/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.5250 - auc: 0.7605 - val_loss: 0.6439 - val_auc: 0.7991\n",
            "Epoch 9/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.5054 - auc: 0.7784 - val_loss: 0.6280 - val_auc: 0.8001\n",
            "Epoch 10/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.5043 - auc: 0.7746 - val_loss: 0.6288 - val_auc: 0.8062\n",
            "Epoch 11/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4805 - auc: 0.7897 - val_loss: 0.6287 - val_auc: 0.7949\n",
            "Epoch 12/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.5107 - auc: 0.7755 - val_loss: 0.6416 - val_auc: 0.7964\n",
            "Epoch 13/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.5066 - auc: 0.7661 - val_loss: 0.6346 - val_auc: 0.8013\n",
            "Epoch 14/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4971 - auc: 0.7671 - val_loss: 0.6090 - val_auc: 0.7976\n",
            "Epoch 15/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4736 - auc: 0.7986 - val_loss: 0.6953 - val_auc: 0.7688\n",
            "Epoch 16/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4943 - auc: 0.7700 - val_loss: 0.6080 - val_auc: 0.7943\n",
            "Epoch 17/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4641 - auc: 0.7924 - val_loss: 0.5904 - val_auc: 0.8040\n",
            "Epoch 18/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4738 - auc: 0.7833 - val_loss: 0.6212 - val_auc: 0.7971\n",
            "Epoch 19/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4591 - auc: 0.8055 - val_loss: 0.6574 - val_auc: 0.7871\n",
            "Epoch 20/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4602 - auc: 0.7976 - val_loss: 0.6039 - val_auc: 0.8025\n",
            "Epoch 21/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4693 - auc: 0.7825 - val_loss: 0.6408 - val_auc: 0.7810\n",
            "Epoch 22/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4438 - auc: 0.8193 - val_loss: 0.5921 - val_auc: 0.7955\n",
            "Epoch 23/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4401 - auc: 0.8133 - val_loss: 0.6141 - val_auc: 0.7904\n",
            "Epoch 24/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4654 - auc: 0.7798 - val_loss: 0.5781 - val_auc: 0.8017\n",
            "Epoch 25/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4400 - auc: 0.8178 - val_loss: 0.5788 - val_auc: 0.7961\n",
            "Epoch 26/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4341 - auc: 0.8174 - val_loss: 0.6353 - val_auc: 0.7895\n",
            "Epoch 27/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4488 - auc: 0.8048 - val_loss: 0.6642 - val_auc: 0.7904\n",
            "Epoch 28/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4681 - auc: 0.7760 - val_loss: 0.5853 - val_auc: 0.8005\n",
            "Epoch 29/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4268 - auc: 0.8222 - val_loss: 0.6261 - val_auc: 0.7885\n",
            "Epoch 30/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4396 - auc: 0.7960 - val_loss: 0.6022 - val_auc: 0.8000\n",
            "Epoch 31/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4312 - auc: 0.8195 - val_loss: 0.6260 - val_auc: 0.7886\n",
            "Epoch 32/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.4333 - auc: 0.8168 - val_loss: 0.5868 - val_auc: 0.7887\n",
            "Epoch 33/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4377 - auc: 0.8127 - val_loss: 0.5651 - val_auc: 0.7908\n",
            "Epoch 34/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4170 - auc: 0.8326 - val_loss: 0.6074 - val_auc: 0.7950\n",
            "Epoch 35/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4353 - auc: 0.7999 - val_loss: 0.6109 - val_auc: 0.8000\n",
            "Epoch 36/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4376 - auc: 0.8042 - val_loss: 0.5521 - val_auc: 0.7988\n",
            "Epoch 37/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4233 - auc: 0.8240 - val_loss: 0.5866 - val_auc: 0.8015\n",
            "Epoch 38/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4422 - auc: 0.8140 - val_loss: 0.5828 - val_auc: 0.8079\n",
            "Epoch 39/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4151 - auc: 0.8305 - val_loss: 0.5753 - val_auc: 0.8088\n",
            "Epoch 40/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4001 - auc: 0.8465 - val_loss: 0.5846 - val_auc: 0.8024\n",
            "Epoch 41/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4364 - auc: 0.8066 - val_loss: 0.5757 - val_auc: 0.8003\n",
            "Epoch 42/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4247 - auc: 0.8135 - val_loss: 0.6629 - val_auc: 0.7957\n",
            "Epoch 43/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4489 - auc: 0.7809 - val_loss: 0.5873 - val_auc: 0.7946\n",
            "Epoch 44/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4321 - auc: 0.8059 - val_loss: 0.5764 - val_auc: 0.7969\n",
            "Epoch 45/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4217 - auc: 0.8192 - val_loss: 0.6176 - val_auc: 0.8023\n",
            "Epoch 46/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4293 - auc: 0.8115 - val_loss: 0.5533 - val_auc: 0.8048\n",
            "Epoch 47/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4229 - auc: 0.8144 - val_loss: 0.5797 - val_auc: 0.7987\n",
            "Epoch 48/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4237 - auc: 0.8097 - val_loss: 0.5611 - val_auc: 0.8067\n",
            "Epoch 49/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4333 - auc: 0.8015 - val_loss: 0.5787 - val_auc: 0.7981\n",
            "Epoch 50/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4160 - auc: 0.8185 - val_loss: 0.5983 - val_auc: 0.8062\n",
            "Epoch 51/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4204 - auc: 0.8202 - val_loss: 0.5546 - val_auc: 0.8035\n",
            "35/35 - 0s - loss: 0.4230 - auc: 0.8539 - 256ms/epoch - 7ms/step\n",
            "4/4 - 0s - loss: 0.4098 - auc: 0.8562 - 47ms/epoch - 12ms/step\n",
            "Train loss: 0.4229743480682373\n",
            "Train accuracy: 0.853855311870575\n",
            "Test loss: 0.40978294610977173\n",
            "Test accuracy: 0.856203019618988\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/1000\n",
            "36/36 [==============================] - 12s 91ms/step - loss: 0.7339 - auc: 0.4884 - val_loss: 0.8839 - val_auc: 0.7263\n",
            "Epoch 2/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.6731 - auc: 0.5187 - val_loss: 0.8542 - val_auc: 0.6826\n",
            "Epoch 3/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.6524 - auc: 0.4954 - val_loss: 0.8147 - val_auc: 0.5703\n",
            "Epoch 4/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.6236 - auc: 0.5185 - val_loss: 0.7295 - val_auc: 0.8427\n",
            "Epoch 5/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.5464 - auc: 0.7155 - val_loss: 0.5642 - val_auc: 0.8565\n",
            "Epoch 6/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.5290 - auc: 0.7477 - val_loss: 0.6445 - val_auc: 0.8528\n",
            "Epoch 7/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.5537 - auc: 0.6656 - val_loss: 0.6931 - val_auc: 0.8513\n",
            "Epoch 8/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.5136 - auc: 0.7569 - val_loss: 0.5740 - val_auc: 0.8501\n",
            "Epoch 9/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4822 - auc: 0.7707 - val_loss: 0.5625 - val_auc: 0.8406\n",
            "Epoch 10/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4991 - auc: 0.7457 - val_loss: 0.5840 - val_auc: 0.8304\n",
            "Epoch 11/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4856 - auc: 0.7433 - val_loss: 0.5503 - val_auc: 0.8446\n",
            "Epoch 12/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4703 - auc: 0.8031 - val_loss: 0.5685 - val_auc: 0.8467\n",
            "Epoch 13/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4580 - auc: 0.7821 - val_loss: 0.5681 - val_auc: 0.8252\n",
            "Epoch 14/1000\n",
            "36/36 [==============================] - 2s 59ms/step - loss: 0.4798 - auc: 0.7615 - val_loss: 0.5398 - val_auc: 0.8494\n",
            "Epoch 15/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4585 - auc: 0.7723 - val_loss: 0.5512 - val_auc: 0.8440\n",
            "Epoch 16/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4515 - auc: 0.7868 - val_loss: 0.5299 - val_auc: 0.8408\n",
            "Epoch 17/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4406 - auc: 0.8052 - val_loss: 0.5442 - val_auc: 0.8469\n",
            "Epoch 18/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4543 - auc: 0.7829 - val_loss: 0.5754 - val_auc: 0.8381\n",
            "Epoch 19/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4369 - auc: 0.8001 - val_loss: 0.5623 - val_auc: 0.8441\n",
            "Epoch 20/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4377 - auc: 0.7979 - val_loss: 0.5288 - val_auc: 0.8487\n",
            "Epoch 21/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4291 - auc: 0.8109 - val_loss: 0.5703 - val_auc: 0.8471\n",
            "Epoch 22/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4336 - auc: 0.7904 - val_loss: 0.5738 - val_auc: 0.8429\n",
            "Epoch 23/1000\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 0.4396 - auc: 0.7818 - val_loss: 0.5645 - val_auc: 0.8374\n",
            "Epoch 24/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4281 - auc: 0.7905 - val_loss: 0.5625 - val_auc: 0.8391\n",
            "Epoch 25/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4452 - auc: 0.7779 - val_loss: 0.5289 - val_auc: 0.8393\n",
            "Epoch 26/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4445 - auc: 0.8027 - val_loss: 0.5530 - val_auc: 0.8447\n",
            "Epoch 27/1000\n",
            "36/36 [==============================] - 2s 67ms/step - loss: 0.4318 - auc: 0.7955 - val_loss: 0.5189 - val_auc: 0.8511\n",
            "Epoch 28/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4139 - auc: 0.8238 - val_loss: 0.5299 - val_auc: 0.8320\n",
            "Epoch 29/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4307 - auc: 0.7896 - val_loss: 0.5380 - val_auc: 0.8428\n",
            "Epoch 30/1000\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 0.4300 - auc: 0.8014 - val_loss: 0.5509 - val_auc: 0.8318\n",
            "Epoch 31/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4356 - auc: 0.7983 - val_loss: 0.5351 - val_auc: 0.8523\n",
            "Epoch 32/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4124 - auc: 0.8191 - val_loss: 0.5130 - val_auc: 0.8458\n",
            "Epoch 33/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4204 - auc: 0.8117 - val_loss: 0.5203 - val_auc: 0.8431\n",
            "Epoch 34/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4279 - auc: 0.8005 - val_loss: 0.5601 - val_auc: 0.8402\n",
            "Epoch 35/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4292 - auc: 0.8013 - val_loss: 0.5419 - val_auc: 0.8432\n",
            "Epoch 36/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4158 - auc: 0.8179 - val_loss: 0.5494 - val_auc: 0.8506\n",
            "Epoch 37/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4003 - auc: 0.8318 - val_loss: 0.5791 - val_auc: 0.8354\n",
            "Epoch 38/1000\n",
            "36/36 [==============================] - 2s 67ms/step - loss: 0.4120 - auc: 0.8164 - val_loss: 0.5164 - val_auc: 0.8523\n",
            "Epoch 39/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4050 - auc: 0.8243 - val_loss: 0.5365 - val_auc: 0.8490\n",
            "Epoch 40/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4058 - auc: 0.8220 - val_loss: 0.5214 - val_auc: 0.8508\n",
            "Epoch 41/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4113 - auc: 0.8165 - val_loss: 0.5531 - val_auc: 0.8406\n",
            "Epoch 42/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4103 - auc: 0.8120 - val_loss: 0.6218 - val_auc: 0.8370\n",
            "Epoch 43/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4184 - auc: 0.8031 - val_loss: 0.5092 - val_auc: 0.8484\n",
            "Epoch 44/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4028 - auc: 0.8288 - val_loss: 0.4978 - val_auc: 0.8498\n",
            "Epoch 45/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4021 - auc: 0.8237 - val_loss: 0.5650 - val_auc: 0.8419\n",
            "Epoch 46/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4185 - auc: 0.7969 - val_loss: 0.5039 - val_auc: 0.8418\n",
            "Epoch 47/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4055 - auc: 0.8264 - val_loss: 0.5138 - val_auc: 0.8472\n",
            "Epoch 48/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4123 - auc: 0.8096 - val_loss: 0.5151 - val_auc: 0.8461\n",
            "Epoch 49/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4108 - auc: 0.8120 - val_loss: 0.5408 - val_auc: 0.8418\n",
            "Epoch 50/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.3935 - auc: 0.8395 - val_loss: 0.5168 - val_auc: 0.8419\n",
            "Epoch 51/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4107 - auc: 0.8054 - val_loss: 0.5377 - val_auc: 0.8453\n",
            "Epoch 52/1000\n",
            "36/36 [==============================] - 2s 68ms/step - loss: 0.4164 - auc: 0.8034 - val_loss: 0.5450 - val_auc: 0.8449\n",
            "Epoch 53/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.3983 - auc: 0.8264 - val_loss: 0.5356 - val_auc: 0.8476\n",
            "Epoch 54/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.4024 - auc: 0.8299 - val_loss: 0.5295 - val_auc: 0.8445\n",
            "Epoch 55/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4062 - auc: 0.8134 - val_loss: 0.5028 - val_auc: 0.8523\n",
            "Epoch 56/1000\n",
            "36/36 [==============================] - 2s 63ms/step - loss: 0.3964 - auc: 0.8354 - val_loss: 0.5166 - val_auc: 0.8415\n",
            "Epoch 57/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4160 - auc: 0.8040 - val_loss: 0.4903 - val_auc: 0.8467\n",
            "Epoch 58/1000\n",
            "36/36 [==============================] - 2s 64ms/step - loss: 0.4032 - auc: 0.8275 - val_loss: 0.5476 - val_auc: 0.8442\n",
            "Epoch 59/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.3913 - auc: 0.8397 - val_loss: 0.5266 - val_auc: 0.8460\n",
            "Epoch 60/1000\n",
            "36/36 [==============================] - 2s 62ms/step - loss: 0.4043 - auc: 0.8233 - val_loss: 0.4955 - val_auc: 0.8546\n",
            "Epoch 61/1000\n",
            "36/36 [==============================] - 2s 69ms/step - loss: 0.4063 - auc: 0.8248 - val_loss: 0.5122 - val_auc: 0.8449\n",
            "Epoch 62/1000\n",
            "36/36 [==============================] - 2s 67ms/step - loss: 0.3958 - auc: 0.8301 - val_loss: 0.5268 - val_auc: 0.8409\n",
            "Epoch 63/1000\n",
            "36/36 [==============================] - 2s 66ms/step - loss: 0.4046 - auc: 0.8247 - val_loss: 0.5497 - val_auc: 0.8478\n",
            "Epoch 64/1000\n",
            "36/36 [==============================] - 2s 67ms/step - loss: 0.4064 - auc: 0.8170 - val_loss: 0.5310 - val_auc: 0.8535\n",
            "Epoch 65/1000\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.4183 - auc: 0.7993 - val_loss: 0.5512 - val_auc: 0.8452\n",
            "Epoch 66/1000\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.4010 - auc: 0.8216 - val_loss: 0.5272 - val_auc: 0.8483\n",
            "Epoch 67/1000\n",
            "36/36 [==============================] - 2s 68ms/step - loss: 0.4009 - auc: 0.8164 - val_loss: 0.4884 - val_auc: 0.8534\n",
            "Epoch 68/1000\n",
            "36/36 [==============================] - 2s 68ms/step - loss: 0.3996 - auc: 0.8201 - val_loss: 0.5078 - val_auc: 0.8466\n",
            "Epoch 69/1000\n",
            "36/36 [==============================] - 2s 68ms/step - loss: 0.3966 - auc: 0.8243 - val_loss: 0.5042 - val_auc: 0.8460\n",
            "Epoch 70/1000\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.3878 - auc: 0.8290 - val_loss: 0.5296 - val_auc: 0.8502\n",
            "Epoch 71/1000\n",
            "36/36 [==============================] - 3s 74ms/step - loss: 0.4072 - auc: 0.8118 - val_loss: 0.5199 - val_auc: 0.8502\n",
            "Epoch 72/1000\n",
            "36/36 [==============================] - 2s 67ms/step - loss: 0.3901 - auc: 0.8370 - val_loss: 0.5326 - val_auc: 0.8448\n",
            "Epoch 73/1000\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.4033 - auc: 0.8192 - val_loss: 0.5109 - val_auc: 0.8444\n",
            "Epoch 74/1000\n",
            "36/36 [==============================] - 3s 70ms/step - loss: 0.4187 - auc: 0.8020 - val_loss: 0.5170 - val_auc: 0.8501\n",
            "Epoch 75/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.3940 - auc: 0.8241 - val_loss: 0.5330 - val_auc: 0.8534\n",
            "Epoch 76/1000\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 0.3915 - auc: 0.8372 - val_loss: 0.5117 - val_auc: 0.8501\n",
            "Epoch 77/1000\n",
            "36/36 [==============================] - 2s 65ms/step - loss: 0.4062 - auc: 0.8095 - val_loss: 0.5059 - val_auc: 0.8430\n",
            "Epoch 78/1000\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.3743 - auc: 0.8525 - val_loss: 0.5325 - val_auc: 0.8450\n",
            "Epoch 79/1000\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.4143 - auc: 0.8045 - val_loss: 0.5574 - val_auc: 0.8413\n",
            "Epoch 80/1000\n",
            "36/36 [==============================] - 2s 69ms/step - loss: 0.3976 - auc: 0.8221 - val_loss: 0.5203 - val_auc: 0.8443\n",
            "Epoch 81/1000\n",
            "36/36 [==============================] - 3s 71ms/step - loss: 0.3921 - auc: 0.8311 - val_loss: 0.5051 - val_auc: 0.8482\n",
            "Epoch 82/1000\n",
            "36/36 [==============================] - 2s 69ms/step - loss: 0.4128 - auc: 0.8101 - val_loss: 0.5253 - val_auc: 0.8472\n",
            "35/35 - 0s - loss: 0.4009 - auc: 0.8622 - 278ms/epoch - 8ms/step\n",
            "4/4 - 0s - loss: 0.5096 - auc: 0.7635 - 50ms/epoch - 12ms/step\n",
            "Train loss: 0.40090349316596985\n",
            "Train accuracy: 0.8622359037399292\n",
            "Test loss: 0.5095902681350708\n",
            "Test accuracy: 0.7635338306427002\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean acc:  0.8328912019729614\n",
            "Confidence interval:  0.06142487799979779\n",
            "All:  [0.9088541865348816, 0.8997395038604736, 0.8038194179534912, 0.7814428806304932, 0.7345678806304932, 0.8940972089767456, 0.8184210062026978, 0.8682330846786499, 0.856203019618988, 0.7635338306427002]\n"
          ]
        }
      ],
      "source": [
        "# define hyperparameters\n",
        "dropout_rate = 0.25#0.25 \n",
        "learning_rate = 0.0020\n",
        "recurrent_rate = 0.15 #0.15\n",
        "l1_rate = 1e-4#-4\n",
        "l2_rate = 1e-3#-4\n",
        "preds_stat= np.array([])\n",
        "\n",
        "n_split=10\n",
        "\n",
        "skf = StratifiedKFold(n_splits=n_split, shuffle=False)\n",
        "mean = 0\n",
        "accuracy = []\n",
        "i=1\n",
        "\n",
        "for train_index, test_index in skf.split(df_all, y):#_mult):\n",
        "    Xtrain = df_all[train_index]\n",
        "    ytrain = y[train_index]\n",
        "    Xtest = df_all[test_index]\n",
        "    ytest = y[test_index]\n",
        "\n",
        "    inputs = Input(shape=(Xtrain.shape[1], Xtrain.shape[2]))\n",
        "\n",
        "    model = Sequential()\n",
        "    x= layers.BatchNormalization(axis=-1,\n",
        "                                 momentum=0.99,\n",
        "                                 epsilon=0.0001)(inputs)\n",
        "\n",
        "    x = layers.Bidirectional(LSTM(10, \n",
        "                    activation =\"sigmoid\",\n",
        "                    recurrent_activation = \"sigmoid\",\n",
        "                    return_sequences=True, \n",
        "                    dropout=dropout_rate, \n",
        "                    recurrent_dropout=recurrent_rate,\n",
        "                    kernel_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                    bias_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                    activity_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                    input_shape=(None, Xtrain.shape[1], Xtrain.shape[2])))(x)\n",
        "    \n",
        "    x = layers.LSTM(20, \n",
        "                    activation =\"tanh\",\n",
        "                    recurrent_activation = \"softmax\",\n",
        "                    dropout=dropout_rate, \n",
        "                    recurrent_dropout=recurrent_rate,\n",
        "                    kernel_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                    bias_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                    activity_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                    return_sequences=False)(x)\n",
        "\n",
        "    x= layers.LayerNormalization(  axis=-1,  epsilon=0.001,   center=True,    scale=True)(x)\n",
        "\n",
        "    x = layers.Dense(20,activation = \"relu\",\n",
        "                     kernel_regularizer=regularizers.L1L2(l1_rate,l2_rate))(x)#softmax\n",
        "\n",
        "    x = layers.LayerNormalization(  axis=-1,  epsilon=0.001,   center=True,    scale=True)(x)\n",
        "    x = layers.Dropout(dropout_rate) (x)\n",
        "\n",
        "    x = layers.Dense(5, \n",
        "                     kernel_regularizer=regularizers.L1L2(l1_rate,l2_rate), \n",
        "                     activation = \"softmax\")(x)\n",
        "    \n",
        "    x = layers.LayerNormalization(  axis=-1,  epsilon=0.001,   center=True,    scale=True)(x)\n",
        "    x = layers.Dropout(dropout_rate) (x)\n",
        "\n",
        "    outputs = layers.Dense(1, activation = \"sigmoid\")(x)  \n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"LSTM_clinical_data\")\n",
        "\n",
        "    # define how to compile\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        metrics=[\"AUC\"], #,\"Recall\",\"accuracy\"\n",
        "        )\n",
        "    \n",
        "    history = model.fit(Xtrain, ytrain, epochs=1000, validation_split=0.20, verbose=1, callbacks=EarlyStopping(monitor='val_loss', patience=15), batch_size=25) #batch_size=75,monitor='val_loss'\n",
        "   \n",
        "    model.save(\"/content/gdrive/My Drive/Speciale/climodel\"+str(i)+\".h5\")\n",
        "    i+= 1 \n",
        "\n",
        "    train_scores = model.evaluate(Xtrain, ytrain, verbose=2)\n",
        "    test_scores = model.evaluate(Xtest, ytest, verbose=2)\n",
        "    print(\"Train loss:\", train_scores[0])\n",
        "    print(\"Train accuracy:\", train_scores[1])\n",
        "    print(\"Test loss:\", test_scores[0])\n",
        "    print(\"Test accuracy:\", test_scores[1])\n",
        "    accuracy.append(test_scores[1])\n",
        "    \n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title(\"Model Loss\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.legend([\"Train\",\"Test\"],loc=\"best\")\n",
        "    plt.ylim([0,1.5])\n",
        "    plt.show()\n",
        "\n",
        "    preds_stat = np.concatenate([preds_stat,np.squeeze(model.predict(Xtest))])\n",
        "\n",
        "\n",
        "print(\"Mean acc: \", np.mean(accuracy))\n",
        "#create 95% confidence interval for population mean weight\n",
        "print(\"Confidence interval: \", get_standard_diviation(accuracy) )\n",
        "print(\"All: \", accuracy )"
      ],
      "id": "961rPd8epLiT"
    },
    {
      "cell_type": "code",
      "source": [
        "#np.savetxt('LSTM_cli_3v.csv', preds_stat, delimiter=',')   \n",
        "preds_stat.tofile(\"/content/gdrive/My Drive/Speciale/LSTM_cli_3v.csv\")\n",
        "\n",
        "#np.savetxt('y_lstm_cli_3v.csv', y, delimiter=',')   \n",
        "#np.savetxt('LinR_cli_5v.csv', preds_stat, delimiter=',')  \n",
        "#np.savetxt('y_cli_5v.csv', y_01356, delimiter=',') \n",
        "#np.savetxt('LinR_cli_5v_3v.csv', preds_stat, delimiter=',')  "
      ],
      "metadata": {
        "id": "_ZLIIrpjND6R"
      },
      "id": "_ZLIIrpjND6R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1Yiphf434x5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79d177d5-821d-494d-c55f-c74227621661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"LSTM_clinical_data\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_11 (InputLayer)       [(None, 3, 18)]           0         \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 3, 18)            72        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " bidirectional_9 (Bidirectio  (None, 3, 20)            2320      \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " lstm_21 (LSTM)              (None, 20)                3280      \n",
            "                                                                 \n",
            " layer_normalization_27 (Lay  (None, 20)               40        \n",
            " erNormalization)                                                \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 20)                420       \n",
            "                                                                 \n",
            " layer_normalization_28 (Lay  (None, 20)               40        \n",
            " erNormalization)                                                \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 20)                0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 5)                 105       \n",
            "                                                                 \n",
            " layer_normalization_29 (Lay  (None, 5)                10        \n",
            " erNormalization)                                                \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 5)                 0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 1)                 6         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,293\n",
            "Trainable params: 6,257\n",
            "Non-trainable params: 36\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ],
      "id": "a1Yiphf434x5"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"allo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqYnkib0Y994",
        "outputId": "bf2d831b-682c-4510-fcfe-5568d79f6981"
      },
      "id": "QqYnkib0Y994",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "allo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1lcYD9_pMOQ"
      },
      "outputs": [],
      "source": [
        "ean acc:  0.8057533502578735\n",
        "Confidence interval:  0.04494522359899174\n",
        "All:  [0.8048769235610962, 0.840501070022583, 0.767729640007019, 0.755782961845398, 0.8598761558532715]"
      ],
      "id": "t1lcYD9_pMOQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation "
      ],
      "metadata": {
        "id": "2yJprTg20YcP"
      },
      "id": "2yJprTg20YcP"
    },
    {
      "cell_type": "code",
      "source": [
        "# define hyperparameters\n",
        "dropout_rate = 0.25#0.25 \n",
        "learning_rate = 0.0020\n",
        "recurrent_rate = 0.15 #0.15\n",
        "l1_rate = 1e-4#-4\n",
        "l2_rate = 1e-3#-4\n",
        "\n",
        "inputs = Input(shape=(df_all.shape[1], df_all.shape[2]))\n",
        "\n",
        "model = Sequential()\n",
        "x= layers.BatchNormalization(axis=-1,\n",
        "                              momentum=0.99,\n",
        "                              epsilon=0.0001)(inputs)\n",
        "\n",
        "x = layers.Bidirectional(LSTM(10, \n",
        "                activation =\"sigmoid\",\n",
        "                recurrent_activation = \"sigmoid\",\n",
        "                return_sequences=True, \n",
        "                dropout=dropout_rate, \n",
        "                recurrent_dropout=recurrent_rate,\n",
        "                kernel_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                bias_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                activity_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                input_shape=(None, df_all.shape[1], df_all.shape[2])))(x)\n",
        "\n",
        "x = layers.LSTM(20, \n",
        "                activation =\"tanh\",\n",
        "                recurrent_activation = \"softmax\",\n",
        "                dropout=dropout_rate, \n",
        "                recurrent_dropout=recurrent_rate,\n",
        "                kernel_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                bias_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                activity_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                return_sequences=False)(x)\n",
        "\n",
        "x= layers.LayerNormalization(  axis=-1,  epsilon=0.001,   center=True,    scale=True)(x)\n",
        "\n",
        "x = layers.Dense(20,activation = \"relu\",\n",
        "                  kernel_regularizer=regularizers.L1L2(l1_rate,l2_rate))(x)#softmax\n",
        "\n",
        "x = layers.LayerNormalization(  axis=-1,  epsilon=0.001,   center=True,    scale=True)(x)\n",
        "x = layers.Dropout(dropout_rate) (x)\n",
        "\n",
        "x = layers.Dense(5, \n",
        "                  kernel_regularizer=regularizers.L1L2(l1_rate,l2_rate), \n",
        "                  activation = \"softmax\")(x)\n",
        "\n",
        "x = layers.LayerNormalization(  axis=-1,  epsilon=0.001,   center=True,    scale=True)(x)\n",
        "x = layers.Dropout(dropout_rate) (x)\n",
        "\n",
        "outputs = layers.Dense(1, activation = \"sigmoid\")(x)  \n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name=\"LSTM_clinical_data\")\n",
        "\n",
        "# define how to compile\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "    metrics=[\"AUC\"], #,\"Recall\",\"accuracy\"\n",
        "    )\n",
        "\n",
        "history = model.fit(df_all, y, epochs=1000, validation_split=0.20, verbose=1, callbacks=EarlyStopping(monitor='val_loss', patience=15), batch_size=25) #batch_size=75,monitor='val_loss'\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title(\"Model Loss\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend([\"Train\",\"Test\"],loc=\"best\")\n",
        "plt.ylim([0,1.5])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0hAkpwbH0Yoc"
      },
      "id": "0hAkpwbH0Yoc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypred = model.predict(df_all_val)\n",
        "print(\"Accuracy Score: \", roc_auc_score(y_val.astype(float), ypred))"
      ],
      "metadata": {
        "id": "no3U1wKx0vRQ"
      },
      "id": "no3U1wKx0vRQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shallow LSTM"
      ],
      "metadata": {
        "id": "cWH8noKSe2TM"
      },
      "id": "cWH8noKSe2TM"
    },
    {
      "cell_type": "code",
      "source": [
        "# define hyperparameters\n",
        "dropout_rate = 0.30 \n",
        "learning_rate = 0.001 \n",
        "recurrent_rate = 0.30\n",
        "n_split=10\n",
        "#Best : 0.7716447856617028, using {'hidden1': 35, 'l1_rate': 0.0001, 'l2_rate': 0.001, 'learning_rate': 0.001}\n",
        "l1_rate = 0.0005\n",
        "l2_rate = 0.005\n",
        "mean = 0\n",
        "accuracy = []\n",
        "i=0 \n",
        "preds_stat=np.array([])\n",
        "\n",
        "for train_index, test_index in skf.split(df_all_mult, y_mult):\n",
        "    Xtrain = df_all_mult[train_index]\n",
        "    ytrain = y_mult[train_index]\n",
        "    Xtest = df_all_mult[test_index]\n",
        "    ytest = y_mult[test_index]\n",
        "    inputs = Input(shape=(Xtrain.shape[1], Xtrain.shape[2]))\n",
        "\n",
        "    model = Sequential()\n",
        "    x = layers.BatchNormalization(axis=-1,\n",
        "                                 momentum=0.99,\n",
        "                                 center =True,\n",
        "                                 scale = True, \n",
        "                                 epsilon=0.0001)(inputs)\n",
        "\n",
        "    x = layers.Bidirectional(LSTM(38,\n",
        "                    activation =\"sigmoid\",\n",
        "                    recurrent_activation = \"sigmoid\",\n",
        "                    return_sequences=False,\n",
        "                    kernel_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                    bias_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                    activity_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                    dropout=dropout_rate, \n",
        "                    recurrent_dropout=recurrent_rate,\n",
        "                    input_shape=(None, Xtrain.shape[1], Xtrain.shape[2])))(x)\n",
        "\n",
        "    outputs = layers.Dense(1, activation = \"sigmoid\")(x)  \n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"LSTM_clinical_data\")\n",
        "\n",
        "    # define how to compile\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        metrics=[\"AUC\"], #,\"Recall\",\"accuracy\"\n",
        "        )\n",
        "    \n",
        "    history = model.fit(Xtrain, ytrain, epochs=1000, validation_split=0.20, verbose=1, callbacks=EarlyStopping(monitor='val_loss', patience=15), batch_size=25) #batch_size=75,monitor='val_loss'\n",
        "\n",
        "    model.save(\"/content/gdrive/My Drive/Speciale/climodel_shallow_morevisits\"+str(i)+\".h5\")\n",
        "    i+= 1 \n",
        "\n",
        "    preds_stat = np.concatenate([preds_stat,np.squeeze(model.predict(Xtest))])\n",
        "\n",
        "    train_scores = model.evaluate(Xtrain, ytrain, verbose=2)\n",
        "    test_scores = model.evaluate(Xtest, ytest, verbose=2)\n",
        "    print(\"Train loss:\", train_scores[0])\n",
        "    print(\"Train accuracy:\", train_scores[1])\n",
        "    print(\"Test loss:\", test_scores[0])\n",
        "    print(\"Test accuracy:\", test_scores[1])\n",
        "    accuracy.append(test_scores[1])\n",
        "    \n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title(\"Model Loss\")\n",
        "    plt.ylim([0,1.5])\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.legend([\"Train\",\"Test\"],loc=\"best\")\n",
        "    plt.ylim([0,1])\n",
        "    plt.show()\n",
        "\n",
        "print(\"Mean acc: \", np.mean(accuracy))\n",
        "#create 95% confidence interval for population mean weight\n",
        "print(\"Confidence interval: \", get_standard_diviation(accuracy) )\n",
        "print(\"All: \", accuracy )"
      ],
      "metadata": {
        "id": "NiJr2xa4e17b"
      },
      "id": "NiJr2xa4e17b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(Xtrain).round()\n",
        "confusion_matrix(ytrain, y_pred)"
      ],
      "metadata": {
        "id": "F240Rar9fOZT"
      },
      "id": "F240Rar9fOZT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ygyts6Ztiyh4"
      },
      "id": "ygyts6Ztiyh4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preds_stat.tofile(\"/content/gdrive/My Drive/Speciale/LSTM_shallow_cli_5v.csv\")\n",
        "#preds_stat.tofile(\"/content/gdrive/My Drive/Speciale/LSTM_shallow_cli_5v3v.csv\")\n",
        "#np.array(y_mult).tofile(\"/content/gdrive/My Drive/Speciale/y_lastm_cli_5v.csv\")\n",
        "#preds_stat.tofile(\"/content/gdrive/My Drive/Speciale/LSTM_shallow_cli_3v.csv\")\n",
        "#np.array(y_mult).tofile(\"/content/gdrive/My Drive/Speciale/y_lastm_cli_3v.csv\")\n",
        "#np.savetxt(fname='y_lastm_cli_5v.csv', x=y_mult, delimiter=',')   "
      ],
      "metadata": {
        "id": "1HdMrOuQOV98"
      },
      "id": "1HdMrOuQOV98",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "D0cEBp7ClSkz"
      },
      "id": "D0cEBp7ClSkz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cR95i6PXlIVI"
      },
      "id": "cR95i6PXlIVI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sQXAg_VpnVul"
      },
      "id": "sQXAg_VpnVul",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validate"
      ],
      "metadata": {
        "id": "9m-ugkDI1UVo"
      },
      "id": "9m-ugkDI1UVo"
    },
    {
      "cell_type": "code",
      "source": [
        "# define hyperparameters\n",
        "dropout_rate = 0.30 \n",
        "learning_rate = 0.001 \n",
        "recurrent_rate = 0.30\n",
        "l1_rate = 0.0005\n",
        "l2_rate = 0.005\n",
        "\n",
        "inputs = Input(shape=(df_all_mult.shape[1], df_all_mult.shape[2]))\n",
        "\n",
        "model = Sequential()\n",
        "x = layers.BatchNormalization(axis=-1,\n",
        "                              momentum=0.99,\n",
        "                              center =True,\n",
        "                              scale = True, \n",
        "                              epsilon=0.0001)(inputs)\n",
        "\n",
        "x = layers.Bidirectional(LSTM(38,\n",
        "                activation =\"sigmoid\",\n",
        "                recurrent_activation = \"sigmoid\",\n",
        "                return_sequences=False,\n",
        "                kernel_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                bias_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                activity_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                dropout=dropout_rate, \n",
        "                recurrent_dropout=recurrent_rate,\n",
        "                input_shape=(None, df_all_mult.shape[1], df_all_mult.shape[2])))(x)\n",
        "\n",
        "outputs = layers.Dense(1, activation = \"sigmoid\")(x)  \n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name=\"LSTM_clinical_data\")\n",
        "\n",
        "# define how to compile\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "    metrics=[\"AUC\"], #,\"Recall\",\"accuracy\"\n",
        "    )\n",
        "\n",
        "history = model.fit(df_all_mult, y_mult, epochs=1000, validation_split=0.20, verbose=1, callbacks=EarlyStopping(monitor='val_loss', patience=15), batch_size=25) #batch_size=75,monitor='val_loss'\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title(\"Model Loss\")\n",
        "plt.ylim([0,1.5])\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend([\"Train\",\"Test\"],loc=\"best\")\n",
        "plt.ylim([0,1])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NAyQXgbk1UjD"
      },
      "id": "NAyQXgbk1UjD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypred = model.predict(df_all_mult_val)\n",
        "print(\"Accuracy Score: \", roc_auc_score(y_mult_val.astype(float), ypred))"
      ],
      "metadata": {
        "id": "nxLZRXvR1jDv"
      },
      "id": "nxLZRXvR1jDv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZTy2eMD6mA_"
      },
      "source": [
        " # Gidseatch LR + Dropout"
      ],
      "id": "lZTy2eMD6mA_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbqGcIpipXe5"
      },
      "outputs": [],
      "source": [
        "def create_model(learning_rate,dropout_rate,recurrent_rate,kr, kr2):\n",
        "    inputs = Input(shape=(Xtrain.shape[1], Xtrain.shape[2]))\n",
        "\n",
        "    model = Sequential()\n",
        "    x= layers.BatchNormalization(axis=-1,\n",
        "                                 momentum=0.99,\n",
        "                                 epsilon=0.0001)(inputs)\n",
        "\n",
        "    x = layers.Bidirectional(LSTM(10, \n",
        "                    activation =\"sigmoid\",\n",
        "                    recurrent_activation = \"sigmoid\",\n",
        "                    return_sequences=True, \n",
        "                    dropout=dropout_rate, \n",
        "                    recurrent_dropout=recurrent_rate,\n",
        "                    kernel_regularizer=regularizers.L2(kr2),\n",
        "                    input_shape=(None, Xtrain.shape[1], Xtrain.shape[2])))(x)\n",
        "    \n",
        "    x = layers.LSTM(20, \n",
        "                    activation =\"tanh\",\n",
        "                    recurrent_activation = \"softmax\",\n",
        "                    dropout=dropout_rate, \n",
        "                    recurrent_dropout=recurrent_rate,\n",
        "                    kernel_regularizer=regularizers.L2(kr2),\n",
        "                    return_sequences=False)(x)\n",
        "\n",
        "    x = layers.Dense(20,activation = \"relu\",\n",
        "                     kernel_regularizer=regularizers.L1(kr))(x)#softmax\n",
        "    x = layers.Dropout(dropout_rate) (x)\n",
        "\n",
        "    x = layers.Dense(5, \n",
        "                     kernel_regularizer=regularizers.L1(kr), \n",
        "                     activation = \"softmax\")(x)\n",
        "    x = layers.Dropout(dropout_rate) (x)\n",
        "\n",
        "    outputs = layers.Dense(1, activation = \"sigmoid\")(x)  \n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"LSTM_clinical_data\")\n",
        "\n",
        "    # define how to compile\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        metrics=[\"AUC\"], #,\"Recall\",\"accuracy\"\n",
        "        )\n",
        "\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn = create_model,verbose = 1, validation_split=0.20,callbacks=EarlyStopping(monitor='val_loss', patience=20))"
      ],
      "id": "kbqGcIpipXe5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwEkgiQVpXhx"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define the grid search parameters\n",
        "n_split = 3\n",
        "learning_rate = [0.001,0.0001]\n",
        "dropout_rate = [0.15, 0.25]\n",
        "recurrent_rate = [ 0.15]\n",
        "kr2 =[0.00001, 0.000005, 0.000001]\n",
        "kr =[0.0005, 0.0001]\n",
        "\n",
        "# Make a dictionary of the grid search parametersparam_grid = dict(batch_size = batch_size,epochs = epochs)\n",
        "param_grids = dict(learning_rate = learning_rate,dropout_rate = dropout_rate, recurrent_rate=recurrent_rate,kr=kr, kr2=kr2)\n",
        "\n",
        "# Build and fit the GridSearchCV\n",
        "grid = GridSearchCV(estimator = model,param_grid = param_grids,cv = StratifiedKFold(n_splits=n_split,random_state=True, shuffle=True),verbose = 1,scoring=\"roc_auc\")\n",
        "grid_result = grid.fit(df_all, y,callbacks=EarlyStopping(monitor='val_loss', patience=25),batch_size = 25, epochs= 1000)"
      ],
      "id": "hwEkgiQVpXhx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMyoCzYOpXlO"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "PMyoCzYOpXlO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6269658a"
      },
      "source": [
        "### Split into training and test set"
      ],
      "id": "6269658a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ecbe8b2"
      },
      "outputs": [],
      "source": [
        "Xtrain, Xtest, ytrain, ytest = train_test_split(df_all,y, test_size=0.20, random_state=42, shuffle=True)"
      ],
      "id": "1ecbe8b2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvdackRBDQ3O"
      },
      "outputs": [],
      "source": [
        "# normalize data\n",
        "# retrice mean and std\n",
        "mean = np.zeros(Xtrain.shape[2])\n",
        "std = np.zeros(Xtrain.shape[2])\n",
        "\n",
        "for i in range(Xtrain.shape[1]):\n",
        "  mean += np.mean(Xtrain[:,i,:],axis=0)\n",
        "  std += np.std(Xtrain[:,i,:],axis=0)\n",
        "\n",
        "def normalize(data,m,s):  \n",
        "  m = m/data.shape[1]\n",
        "  s = s/data.shape[1]\n",
        "\n",
        "  for i in range(data.shape[1]):\n",
        "    data[:,i,:] = (data[:,i,:]-m)/std\n",
        "  \n",
        "  return data\n",
        "\n",
        "Xtrain = normalize(Xtrain,mean,std)\n",
        "Xtest = normalize(Xtest,mean,std)"
      ],
      "id": "tvdackRBDQ3O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e707cc69"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "e707cc69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWXS5tTnguT1"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "BWXS5tTnguT1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC48GLn9BsZw"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(Xtrain).round()\n",
        "confusion_matrix(ytrain, y_pred)"
      ],
      "id": "zC48GLn9BsZw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8b9caa0"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title(\"Model Loss\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend([\"Train\",\"Test\"],loc=\"best\")\n",
        "plt.show()"
      ],
      "id": "f8b9caa0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43b8ec48"
      },
      "outputs": [],
      "source": [
        "#Plot as a graph\n",
        "#keras.utils.plot_model(model, \"LSTM_clinical_data.png\")\n",
        "keras.utils.plot_model(model, \"my_first_model_with_shape_info.png\", show_shapes=True)"
      ],
      "id": "43b8ec48"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TC5zcqGRcE-"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "model.save(\"LSTM_clinical_data\")"
      ],
      "id": "5TC5zcqGRcE-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDM8eHjhe0Yo"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "eDM8eHjhe0Yo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXUPqg6jQWpv"
      },
      "source": [
        "# Back to basics 5-Fold *CV* "
      ],
      "id": "TXUPqg6jQWpv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyNivrOFQVuJ"
      },
      "outputs": [],
      "source": [
        "dropout_rate = 0.30 #from grid\n",
        "learning_rate = 0.0001 #from grid\n",
        "recurrent_rate = 0.30\n",
        "kr1 = 0.0005\n",
        "kr2 = 0.05\n",
        "\n",
        "n_split = 10\n",
        "\n",
        "skf = StratifiedKFold(n_splits=n_split,random_state=42, shuffle=True)\n",
        "mean = 0\n",
        "accuracy = []\n",
        "\n",
        "\n",
        "for train_index, test_index in skf.split(df_all, y):\n",
        "    Xtrain = df_all[train_index]\n",
        "    ytrain = y[train_index]\n",
        "    Xtest = df_all[test_index]\n",
        "    ytest = y[test_index]\n",
        "\n",
        "    inputs = Input(shape=(Xtrain.shape[1], Xtrain.shape[2]))\n",
        "\n",
        "    model = Sequential()\n",
        "    x= layers.BatchNormalization(axis=-1,\n",
        "                                 momentum=0.99,\n",
        "                                 epsilon=0.0001)(inputs)\n",
        "\n",
        "    x = layers.LSTM(Xtrain.shape[2], #59 \n",
        "                  activation =\"sigmoid\",\n",
        "                  recurrent_activation = \"sigmoid\",\n",
        "                  recurrent_dropout =  recurrent_rate,\n",
        "                  bias_regularizer=regularizers.L2(kr2),\n",
        "                  recurrent_regularizer=regularizers.L2( kr2),\n",
        "                  kernel_regularizer=regularizers.L2( l2=kr2),\n",
        "                  return_sequences=False, \n",
        "                  input_shape=(None, Xtrain.shape[1], Xtrain.shape[2]))(x)\n",
        "\n",
        "    x = layers.LayerNormalization(  axis=-1,  epsilon=0.001,   center=True,    scale=True)(x)\n",
        "    x = layers.Dropout(dropout_rate) (x)\n",
        "    \n",
        "    #x = layers.Dense(5,\n",
        "     #             bias_regularizer=regularizers.L2(kr2),\n",
        "      #            kernel_regularizer=regularizers.L2( l2=kr2))(x)\n",
        "\n",
        "    outputs = layers.Dense(1, activation = \"sigmoid\")(x)  \n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"LSTM_clinical_data\")\n",
        "\n",
        "    # define how to compile\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),#BinaryCrossentropy(),\n",
        "#        loss=tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\"),\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        metrics=[\"AUC\"], #,\"Recall\",\"accuracy\"\n",
        "        )\n",
        "    \n",
        "    history = model.fit(Xtrain, ytrain, epochs=1000, validation_split=0.15, verbose=1, callbacks=EarlyStopping(monitor='val_loss', patience=20), batch_size=35) #batch_size=75,monitor='val_loss'\n",
        "\n",
        "    train_scores = model.evaluate(Xtrain, ytrain, verbose=2)\n",
        "    test_scores = model.evaluate(Xtest, ytest, verbose=2)\n",
        "  \n",
        "    print(\"Train loss:\", train_scores[0])\n",
        "    print(\"Train accuracy:\", train_scores[1])\n",
        "    print(\"Test loss:\", test_scores[0])\n",
        "\n",
        "    ypred = model.predict(Xtest)\n",
        "    print(\"Accuracy Score: \", roc_auc_score(ytest.astype(float), ypred))\n",
        "    print(\"Test accuracy:\", test_scores[1])\n",
        "    accuracy.append(test_scores[1])\n",
        "    \n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title(\"Model Loss\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.legend([\"Train\",\"Test\"],loc=\"best\")\n",
        "    plt.ylim([0,2])\n",
        "    plt.show()\n",
        "\n",
        "print(\"Mean acc: \", np.mean(accuracy))\n",
        "#create 95% confidence interval for population mean weight\n",
        "print(\"Confidence interval: \", get_standard_diviation(accuracy) )\n",
        "print(\"All: \", accuracy )"
      ],
      "id": "lyNivrOFQVuJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EppLlliQVx-"
      },
      "outputs": [],
      "source": [
        "#Mean acc:  0.8337960441907247 Confidence interval:  0.01624283381880808 All:  [0.8412328362464905, 0.8151662349700928, 0.8449890613555908] # 2 dense nothing else\n",
        "#Mean acc:  0.8344757258892059 Confidence interval:  0.035537448842402196 # LSTM added All:  [0.820870578289032, 0.81547611951828, 0.858796238899231, 0.8447145223617554, 0.8217592835426331, 0.83314049243927, 0.7830827236175537, 0.7996240854263306, 0.8584586381912231, 0.9088345766067505]\n",
        "#Mean acc:  0.8353470623493194 Confidence interval:  0.0348902731716404\n",
        "#Mean acc:  0.8430311262607575  Confidence interval:  0.03647084784177799\n",
        "#Mean acc:  0.8449315249919891 Confidence interval:  0.03488350092279596\n",
        "#Mean acc:  0.8402410319873265 Confidence interval:  0.03220505914231502"
      ],
      "id": "6EppLlliQVx-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca6dd73d"
      },
      "outputs": [],
      "source": [
        "np.squeeze(model.predict(Xtest)).shape, ytest.astype(float).shape"
      ],
      "id": "ca6dd73d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adb9eb9d"
      },
      "outputs": [],
      "source": [
        "    x = layers.LayerNormalization(  axis=-1,  epsilon=0.001,   center=True,    scale=True)(x)\n"
      ],
      "id": "adb9eb9d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de60e905"
      },
      "outputs": [],
      "source": [
        "train_scores"
      ],
      "id": "de60e905"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0910eb5f"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "0910eb5f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save for ensamble"
      ],
      "metadata": {
        "id": "p-sUWqkN7Uvk"
      },
      "id": "p-sUWqkN7Uvk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a84850b7"
      },
      "outputs": [],
      "source": [
        "# define hyperparameters\n",
        "dropout_rate = 0.25#0.25 \n",
        "learning_rate = 0.0020\n",
        "recurrent_rate = 0.15 #0.15\n",
        "l1_rate = 1e-4#-4\n",
        "l2_rate = 1e-3#-4\n",
        "\n",
        "inputs = Input(shape=(Xtrain.shape[1], Xtrain.shape[2]))\n",
        "\n",
        "model = Sequential()\n",
        "x= layers.BatchNormalization(axis=-1,\n",
        "                              momentum=0.99,\n",
        "                              epsilon=0.0001)(inputs)\n",
        "\n",
        "x = layers.Bidirectional(LSTM(10, \n",
        "                activation =\"sigmoid\",\n",
        "                recurrent_activation = \"sigmoid\",\n",
        "                return_sequences=True, \n",
        "                dropout=dropout_rate, \n",
        "                recurrent_dropout=recurrent_rate,\n",
        "                kernel_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                bias_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                activity_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                input_shape=(None, Xtrain.shape[1], Xtrain.shape[2])))(x)\n",
        "\n",
        "x = layers.LSTM(20, \n",
        "                activation =\"tanh\",\n",
        "                recurrent_activation = \"softmax\",\n",
        "                dropout=dropout_rate, \n",
        "                recurrent_dropout=recurrent_rate,\n",
        "                kernel_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                bias_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                activity_regularizer=regularizers.L1L2(l1_rate,l2_rate),\n",
        "                return_sequences=False)(x)\n",
        "\n",
        "x= layers.LayerNormalization(  axis=-1,  epsilon=0.001,   center=True,    scale=True)(x)\n",
        "\n",
        "x = layers.Dense(20,activation = \"relu\",\n",
        "                  kernel_regularizer=regularizers.L1L2(l1_rate,l2_rate))(x)#softmax\n",
        "\n",
        "x = layers.LayerNormalization(  axis=-1,  epsilon=0.001,   center=True,    scale=True)(x)\n",
        "x = layers.Dropout(dropout_rate) (x)\n",
        "\n",
        "x = layers.Dense(5, \n",
        "                  kernel_regularizer=regularizers.L1L2(l1_rate,l2_rate), \n",
        "                  activation = \"softmax\")(x)\n",
        "\n",
        "x = layers.LayerNormalization(  axis=-1,  epsilon=0.001,   center=True,    scale=True)(x)\n",
        "x = layers.Dropout(dropout_rate) (x)\n",
        "\n",
        "outputs = layers.Dense(1, activation = \"sigmoid\")(x)  \n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name=\"LSTM_clinical_data\")\n",
        "model.save(\"/content/gdrive/My Drive/Speciale/climodel.h5\")"
      ],
      "id": "a84850b7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf9aab67"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "cf9aab67"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39efeaf5"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "39efeaf5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00380de6"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "00380de6"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "lZTy2eMD6mA_"
      ],
      "name": "Improved_LSTM_Clinical.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}